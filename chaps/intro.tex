\chapter{Introduction}

``News is the first rough draft of history,'' as the saying goes. Or is that the saying? Long attributed to \emph{Washington Post} president and publisher Philip L. Graham, it turns out that many might have said it before him, in a variety of iterations. His widow Katharine points to a speech he made in London 1963, but journalist Jack Shafer finds Graham using the phrase in 1948 and 1953, too.\autocite{shafer_who_2010} Even earlier, Barry Popik discovers its use in a 1943 book review in the \emph{New Republic}, not said by Graham at all but by Alan Barth. The \emph{Yale Book of Quotations} credits Douglass Cater, who wrote in 1959 ``The reporter [is] one who each twenty-four hours dictates a first draft of history.''\autocite[139]{shapiro_yale_2006} The variations don't stop here, as the phrase continuously changes as the staying disperses. Did he say ``first draft'' or ``first rough draft''? Was it ``journalism,'' ``the news,'' or just ``newspapers''? A quick search on Google finds thousands of results under all six of these variations (``newspapers are the first draft of history'' gets 36,000 hits, while ``news'' and ``journalism'' each break 10,000).

Attributing a famous phrase has never been easy, but the spread and proliferation of the digital sphere leads to an even tougher challenge. On one hand, attribution is facilitated through the foundational element of the web: the hyperlink. It has never been easier to point to your source. But on the other, the lack of standards and ease of copying make tracing any information to its definitive source nearly impossible. It requires following detailed digital traces, which are each owned and controlled by various actors and can be wiped out at a moment's notice.

But the inherent ephemerality of online media is paradoxically at odds with the perennial availability of this seemingly fleeting content. Entertainment journalist Andy Greenwald joked in a 2015 tweet, ``Remember: Newspapers are the first draft of history. But Twitter is a heavy stone tablet that you have to wear around your neck forever.''\autocite{greenwald_remember:_2015} When compared to newspaper articles, journalists' tweets could be seen as both even rougher drafts and as more permanently accessible artifacts. Newer platforms like Snapchat promise to erase these constant digital traces, and new methods of online presentation -- such as interactive pieces and short-form video -- dull text's easy searchability; but online content is usually here to stay, even while it is produced on tighter, hastier deadlines. By default, we store everything, and whether or not anything has a ``right to be forgotten'' is one of the major legal and ethical issues of our time.\autocite[See, e.g.,][]{arthur_explaining_2014, hakim_right_2014}

In following the genesis and the echoes of a famous or iconic quote, article, photograph, or recording, a researcher becomes a digital sleuth trawling through databases and following links in hopes of reaching some sort of origin or record of influence. This is not only how online researchers do their work (journalists and scholars alike)---it is also how search engine algorithms gain an understanding of influence and impact. The writers of these algorithms influence the results for the human researchers in turn, and the lack of transparency surrounding many of their methods is a substantial cause of debate. While journalism serves as a crucial case study in the problems of measuring attribution, impact, and influence online, the phenomenon runs much deeper. Debates over linking, forgetting, and storing have most recently been fought in the context of privacy and surveillance; this aspect is crucial, but it is only part of the story. These debates also consider what will and will not constitute our past, whether it is the memory of a single person or the history of a political or cultural revolution. To access our past on the web, we can do little more than trace the links, which are themselves the subject of much policing and debate.

\section{The Stakes}

In the spirit of examining the relationship between news and history, I will introduce a few examples from the recent news, all of which occurred in the year I wrote the bulk of this thesis. These events are merely the most recent in a string of similar smaller events and controversies that will quickly be forgotten, or at best brought in as passing evidence for larger phenomena. It's possible they will even be deleted from the web outright, saved only as phantom traces in the Internet Archive or Google. But at least here they will be baked into this scan of this printout of this copy of a Masters thesis, where a physical copy might reside in the MIT libraries for decades.

% Ezra Klein vs. Nate Silver
On April 13, 2015, as I put the finishing touches on this thesis, an argument broke out on Twitter. This would not typically be news, except that the argument was between two titans of a new wave of digital, data- and technology-oriented journalists: Nate Silver of FiveThirtyEight, and Ezra Klein of Vox.com. Silver cast the first stone, saying ``Yo, @voxdotcom: Y'all should probably stop stealing people's charts without proper attribution. You do this all the time, to 538 \& others.''\autocite{silver_yo_2015} When some respondents demanded proof of stealing, FiveThirtyEight writer Carl Bialik linked to two stories about the 2016 presidential election, each featuring a FiveThirtyEight chart. But readers were confused: ``Am I crazy or does that specifically say `Nate Silver and his team\ldots' plus credit 538 on the grafic?'' Bialik's answer was telling: ``Yes. Those links were added after our tweets.''\autocite{bialik_samslusher_2015} According to FiveThirtyEight, it took Vox under 10 minutes to add the attribution, but they first had to be called out on Twitter. More debate ensued about whether Vox had updated the timestamp on the article to reflect the attribution that was made.

This debate and the ensuing confusion bring up a series of issues that stem from the structural limitations of the web; there is no default storage of attribution, and no built-in way to dive back in time and see what Vox's page looked like 10 minutes before. The web has these features grafted onto it, rather than built in; we rely on web browsers, Google, and the Internet Archive to save, store, and index for us, and there's no telling where the information originated. Even journalists are copying and repurposing; in response to the Twitter debacle, Vox's Ezra Klein wrote a guide to ``How Vox aggregates,'' where he suggests that aggregation and linking are at the core of modern digital news. Still, he insists that an aggregator needs to \emph{link} to the source, and not just reference it. Klein defends Vox's use of aggregation in the offending article, ``but the post didn't include a link. This was carelessness, not malice, but it's a violation of Vox's internal standards. Our policy requires attribution, and any time we fail that policy is inexcusable.''\autocite{klein_how_2015}

Aggregation is nothing new, as Klein points out; Time magazine began as a source that would go through ``every magazine and newspaper of note in the world'' and take notes about it for their readers. Even at the time this was considered questionable journalism, but unlike the web, traditional newspapers did not have a direct window to the source text. As Klein emphasizes, the hyperlink serves as a proxy for attributing a source, which is a core component of doing good journalism; because of this new ability to directly attribute, it can, arguably, make aggregation a more journalistically respectable practice. Links are also the basis of Google's PageRank algorithm, and play a substantial role in search engine optimization. Looking deeper, battles over when to link and when not to pervade modern journalism. One of the primary roles of editors is to help journalists stick to Jeff Jarvis' core tenet of online journalism: ``do what you do best, link to the rest.''\autocite{jarvis_new_2007} Other editors might realign the anchor text of a link, or reword a headline for Twitter, so that it becomes more or less enticing to click on.\autocite{thompson_upworthy:_2013} The seemingly innocuous hyperlink thus turns into a powerful instrument in every news outlet's struggle for attention and economic value.

\subsection{The new face of plagiarism}

In summer 2014, several similarly fleeting news items occurred, which each placed a mark of their own on this larger debate. It began with writer Benny Johnson of BuzzFeed accusing the Independent Journal Review of plagiarizing his work---once again, on Twitter.\autocite{johnson_repeat_2014} Two Twitter users known only as @blippoblappo and @crushingbort saw an irony in a BuzzFeed writer accusing another publisher of stealing; BuzzFeed has long been accused of aggregating, remixing, and appropriating other outlets' work without attribution.\autocite[See, e.g.,][]{manjoo_how_2012, chen_remix_2012} Perhaps because of this, the pair of detectives searched the web for examples of Johnson's own lifting.

They were likely not aware of how deep the copying went, though; the pair found three instances of unattributed sentences, with sources ranging from the Guardian to Yahoo! Answers.\autocite{blippoblappo_3_2014} When BuzzFeed editor Ben Smith replied to the plagiarism allegations by calling Johnson ``one of the web's deeply original writers,'' @blippoblappo and @crushingbort responded with six more offenses, with a similarly broad range of sources, including About.com, Wikipedia, and the New York Times.\autocite{blippoblappo_more_2014} This new set forced BuzzFeed to investigate, and a day later they fired Johnson and apologized to their readers; they had found an incredible 41 plagiarized phrases among 500 Johnson pieces.\autocite{smith_editors_2014} The rate and ease at which these offenses seem to have been found is startling. If two researchers found so much bad-faith copying in one day, and BuzzFeed's internal investigation had turned up dozens more, how could they---how could \emph{anyone} not have not discovered these during Johnson's two years as a BuzzFeed writer? The offenses were hiding in plain sight.

The Washington Post's Erik Wemple suggested that some of these transgressions could have come from the specific demands of BuzzFeed; Johnson's ``multi-topical viral beat'' might have left him with not enough time to fully process the material, and not enough patience to link to every single source.\autocite{wemple_ravages_2014} Ben Smith points out that BuzzFeed is certainly not the first major publisher to deal with plagiarism in its ranks; this is of course true, but there is something new at play here. It can also be found in a February 2015 plagiarism case involving Mic.com's Jared Keller, who was outed by Gawker for using unattributed text from \emph{The Atlantic} and others. Here the line between plagiarism and shoddy journalism is blurred: sometimes, ``Keller lifted text from sources that he credits and links to in the post itself, but without setting that text in quotation marks or block quotes, and without noting anywhere on the page that the text was not his own.''\autocite{trotter_plagiarist_2015} Where and how you link can be the line between good journalism, shoddy journalism, and outright plagiarism.

The problem that places like Vox, BuzzFeed, and Mic are facing is still fairly new; they are trying to ethically aggregate and reappropriate from other online sources, but the protocol for doing so is still unclear. While there's no doubt that Johnson and Keller stepped across this ethical line, where exactly is the line? Ben Smith's first reaction to Johnson's transgressions suggested that three offenses was not enough; he also implied that plagiarism on older articles or trite listicles would be more acceptable than on newer, investigative pieces. But it seems that Johnson's attitude towards online aggregation bled into even more ``original'' investigative works. While the legal and ethical implications of aggregation is a crucial topic for journalism and research in the 21st century, this is not so much my focus as the way in which the aggregational mentality changes the \emph{practice} of journalism and research. This is the case for both what can actually be found online, and what we perceive to be findable online. It is surprising that Johnson did not see himself as vulnerable; despite his obvious offenses, he accused others of plagiarism in turn. Keller even offered links to the sources that he was lifting from, perhaps assuming that no one would click on them anyway.

These incidents reflect a new paradigm of attribution and authorship that blurs the distinction between original and copy, between individual and collective, and between highbrow and lowbrow. Johnson pilfered language from everywhere between Yahoo! Answers to the New York Times, with little distinction between the two. His most frequent transgressions, however, did seem to come from anonymous sources. As Wemple put it, he ``viewed [Wikipedia] as an open-source document,'' and grabbed phrases from government reports as if his tax dollars allowed him to.\autocite{wemple_ravages_2014} His liberal use of Yahoo! Answers and About.com also leads to more speculation; did he somehow feel that it was more ethical (or just less risky) to take from anonymous sources than other professional writers? Who should get the original credit, and how should they be compensated? Moreover, why did Johnson feel the need to pass them off as original? Johnson's safest option would have been to simply \emph{link to} the sources. Linking would be safe, but it would also be tedious. It would interrupt the story if the reader decided to click on a link, possibly never to return to Johnson's article again. And of course, it would lay bare Johnson's curation of often dubious sources, not only to readers, but to machines.

BuzzFeed understands well this double power of the link. Tellingly, their apology post about the Benny Johnson incident likewise did not include links to the tainted articles. When internet users pushed back on this omission, BuzzFeed updated the post with plaintext URLs, without the anchor text.\autocite{smith_editors_2014} Why would they do this? While it might slightly increase the friction for an interested user to get to the article, it is more likely that it was to keep web crawlers and search engines from knowing about the connection. On the web, you are what you link to, and this post didn't want to link to---or be linked to---dozens of plagiarized articles. BuzzFeed has also deleted thousands of its older posts, which they claim did not adhere to their newer, more rigorous journalistic standards; it is telling that rather than offering contextual explanation, they deleted these posts outright. While CEO Jonah Peretti offers that they were deleted because they were ``not worth improving or saving because the content [wasn't] good,'' J.K. Trotter of Gawker also found Johnson-like moments of slippery near-plagiarism in the deleted posts.\autocite{trotter_over_2014, trotter_dont_2014}

% In short, this controversy and BuzzFeed's reaction to it encompass many of the problems with assigning attribution and measuring impact on the web. It also points to the difficulty of online research, and the lack of standards and technologies for ethical, creative, original remix and reuse. This is as true for a tweet from today as it is a photo from decades ago. As newsrooms increasingly play the role of aggregator and context provider, they have a newfound ability \emph{and} responsibility to leverage archives -- whether their own proprietary archives or the web-as-archive -- to create and appropriate old content into new stories, merging news and history, placing sensational events in the longer phenomena that surround them, and centering the daily news in broader contexts.

\subsection{The summer of archives}

% Meanwhile: archives got popular! summer of archives, etc.
% Right to be forgotten? What's the default? Should everything be stored?
Meanwhile, legacy news outlets have been aiming to revive rather than delete their past. 2014 was known as the ``summer of archives'' at the Digital Public Library of America, and the New Yorker took this saying to heart as they opened up their digital archive completely, while experimenting with a new paywall. Prior to this summer, legacy media's default move was to hide their archival content behind strict paywalls; archives were considered one of the first incentives for digital publishers to entice readers into novel subscription models. The New York Times' 2014 \emph{Innovation} report similarly featured archives as a trove of untapped potential, and outlets like Time, the Nation, and National Geographic focused on their archives with sleek redesigns and dedicated editors and curators.

The archive summer was accompanied by a historic event farther from the journalism industry; Theodor Nelson's Project Xanadu was finally released on the web. First conceived over 50 years prior, Project Xanadu was famously the first hypertext project, a distant ancestor of the web, under active development for decades; I discuss it at length in section 3.3.1. Belinda Barnet says Xanadu was supposed to be ``like the web, but much better;'' in hindsight, Xanadu lives as a realization of an alternate hypertext system, one in which many of the pitfalls of the web --- the problems of attribution, measurement, and research --- are laid bare to be scrutinized and reimagined.\autocite[``The Magical Place of Literary Memory: Xanadu'']{barnet_memory_2013} The 2014 version was made for the web, which seems like a sort of admission of defeat; it also still lacks many of the core features that comprise Nelson's Xanadu dream. But Xanadu maintains a set of devoted acolytes and followers, and the project's persistence and rebirth demonstrates a drive for understanding and incorporating some of its features.

Among these followers are the founders of NewsLynx, a research project and platform under development at Columbia University's Tow Center for Digital Journalism. In August 2014, NewsLynx wrote about the perils of online linking and tracking; specifically, they lamented the web's ability to only link in one direction, and praised Nelson's Xanadu for its foresight in recognizing this problem. They pointed out the ``hole at the center of the web'' that let Google ``step in and play librarian.''\autocite{abelson_hyper-compensation:_2014} Here they recognized how intensely the structure of the web has affected its content, whether by allowing for transgressions like Benny Johnson's, or leaving Google to determine how to sort everything out.

NewsLynx's goal was to develop a platform that would help small and nonprofit news outlets better measure the impact of their work. They hoped to give each story a sort of biography; how often was a story read, or shared, and where? Who were the catalysts for distributing the work around the web? How long is a typical story's lifespan? They found these problems to be incredibly difficult at a smaller, research-oriented or nonprofit scale. Measurement methods have to be grafted onto the web, rather than built into its core structure or proposed as a radical alternative.

Much like the steady stream of news on the web, these incidents from the summer of 2014 do not singularly cohere into an overarching narrative; but they do all point to a shifting understanding of original, quality journalism, and a fundamental difference in the ways that journalists do research, deliver information to readers, and understand the value and impact of their work. Where traditional plagiarism might involve faking a conversation with a source, the new face of plagiarism is simply lifting information from the web and failing to link to it. As Klein points out, sometimes this is oversight and other times it is malicious; in either case, it points to the foundational role that linking appears to play in the political economy of today's online news, and its flattening effect in producing attributable journalism.

% So in the summer of 2014, not only did Xanadu come to life, but its concept was validated. But in both cases (from Xanadu itself and its NewsLynx acolytes), the solutions were grafted onto the web, rather than proposed as a radical alternative. The web is only to be added and appended to, not replaced. In later sections, I will be looking closely at these two appendages to analyze their histories, strengths, and failures, and to suggest what they can teach us about structure of the web itself, and the ways that our thinking might have and might need to adapt to it.

%In the summer of 2014, I was closely following news about the news. Working for the Nieman Journalism Lab as a Google Journalism Fellow, I split my time between writing articles about innovation in journalism, monitoring social media for stories worth sharing, and building an app that tracked link-sharing and conversations on Twitter (so even when writing code, I was following the news). During this summer, several news events coalesced to form the backbone of the exploration of this thesis topic. While these incidents may seem unrelated, my goal will be to showcase what these news events have in common, and set the stakes for the exploration of the changing nature of online research and cultural production on the web.


% A couple months before BuzzFeed's plagiarism incident, a staffer at the New York Times leaked the company's internal Innovation Report, which my colleagues at the Nieman Lab called ``one of the key documents of this media age.'' The report looks closely and especially at the revitalization of its archives.

% Not only do the archives have the power to historicize current pieces, trends, and events, they can also have amazing financial value, giving new life to old content that is repurposed, repackaged, and recontextualized.

% The problem goes both ways; while not enough tools exist for Times staffers to resurface the past, it's also true that their new content is not properly prepared for the future. The Innovation Report likewise cites many problems that the company has with structured data and categorization.

% Journalists have traditionally called the archive ``the morgue,'' and the Times Innovation Report both explains why this is the case and challenges its issues.

% Finally, the Innovation Report confirmed that the role of repackaging and reappropriating old content was not just a problem for the BuzzFeeds and Huffington Posts of the world; old stalwarts with canonical archives are in the same business. This is, in effect, the new business of journalism: while citizens and activists increasingly serve as the newsbreakers, the journalists must take a step away from the epicenter of the event and report on everything that surrounds it instead. The web provides many new tools and affordances to do this creatively and engagingly; but the news industry has a long way to go and a lot to learn.

\section{Outline of chapters and methods}

% Media archaeologists are doing some of the same things as a) webometrics researchers and scholars; and b) search engines and platforms. As such it's extremely relevant to a variety of sectors. These are all part of the same practice. My aim is to explore this practice.

The subtle but pervasive politics and style of linking on one hand, and the changing focus of journalism towards context and aggregation on the other, are two sides of the same coin; they are two symptoms of a larger shift in the journalistic landscape towards documents, citations, and explanation. At the same time, they reflect divergent impulses; to link \emph{out} on one hand, and embrace the spirit of the web; and to link \emph{in} on the other, to showcase one's own rich history and provide customized, ``in-house'' explanatory context. The research and dynamic landscape of linking tend to assume that linking out and attributing to external sources is an inherently good impulse, while inlinking to one's own archive is ``nepotistic'' rather than altruistic.\autocite[213]{chakrabarti_mining_2003} In reality these lines are much more blurred; outlinking at will is unhelpful if it drives the reader down too many confused paths, or only leads the reader to the same type of information or source under the veneer of sharing. Meanwhile, linking inwards can be crucial for continuing an ongoing story and maintaining contextual control over an ongoing event. The trick might be to embrace the best of both; one's own archive is a place to draw out and contain a compelling narrative, but the archive itself can link out and integrate with other sources, reactions, and annotations, embracing the spirit of the web.

I hope to unpack considerations of the web as a source of free information and rich remix, as it competes with rising business interests and link economies. Striking a balance between these two competing interests requires examining the structure and dynamics of the web as a \emph{container} of content. This thesis therefore takes journalism as a crucial case study, but it aims to reach beyond journalism for inspirations and conclusions. The chapters of this thesis therefore follow a sort of funnel structure, increasingly honing in on journalism and its use of hyperlinks as an enabler of archives. The initial chapters will be necessarily broad in scope, pertaining to the multifaceted meanings and interpretations of archives as they are considered by librarians, information theorists, journalists, and critical theorists alike. Each chapter will then increasingly point towards its applications and implications in news and journalism.

Chapter two, ``The Size and Shape of Archives,'' adopts a theoretical framework to examine the role of the World Wide Web as a container of content. Considering an online object as ``content'' necessariliy frames it in the networked language of the web, and subsumes it into a series of semiotic frameworks that serve to \emph{contain} the content at each stage. From the URL (Uniform Resource Locator), to the hyperlink, to the feeds and indexes that aggregate content, and finally the databases that store them, one finds content forging new connections and diluting its original core at each turn. The web's inherent ephemerality also plays into the sizes and structures of the web's many archives.

Chapter three, ``An Intertwingled History of Linking,'' turns to the history of information structures and knowledge systems, especially looking for traces of the mechanics and functions of today's hyperlink in early classification schemes. It darts back and forth in history, considering at length the origin of hypertext, but also peppering the history with treatments of the encyclopedias, notecard systems, and taxonomies that predated the digital era. Rather than organizing chronologically, I consider this history from the lens of three themes and challenges that recur when considering hypertextual knowledge systems: spatialization, intersubjectivity, and encyclopedism.

The fourth chapter, ``Networking the News,'' hones in on the changes in contemporary journalism that result from the web's structure and the many uses of hyperlinks. It treats the changing practices of journalists as a reflection of many of the containing forces of the web, and the simultaneous rise of ``explainer journalism'' and rich paywalled digital archives as similar symptoms of these forces. I then turn to the information architecture of digital news stories and the common methods for classifying them, ultimately advocating for a link-oriented classification scheme to supplement current tagging and taxonomic processes in the newsroom.

Chapter five, ``Tracing the Links,'' examines how journalists are currently considering and using hyperlinks in their work. First I review the existing qualitative interviews and quantitative analyses that demonstrate journalists' understanding and practice of hyperlinking. I then turn to my own inquiries into quantitative link analysis, which hone in on the internal linking that publishers are currently doing within their archive. This approach begins to examine how a link-oriented classification scheme might work, and the cultural and journalistic changes that need to occur in order to enable such a scheme.

Finally, I will conclude by peering beyond the link, and examining contemporary applications and frameworks that offer new forms of hypertextuality and interactivity. These sometimes expand the function of the link, and other times completely bypass it, rendering traditional methods of tracking attribution, influence and impact obsolete. I will conclude by considering these new forms of journalistic output through two lenses: the frameworks and standards organizations that hope to develop a cohesive language around these new initiatives in order to better save them, and the artists and activists that complicate the existing picture of digital history and memory.

% In this same summer, Theodor Nelson's Project Xanadu was finally released on the web. First conceived 45 years prior, Project Xanadu was famously the first hypertext project, under development for decades. Xanadu was the realization of an alternate hypertext system, one in which many of the pitfalls of the web -- the problems of attribution, measurement, and research that I aim to highlight -- are laid bare to be scrutinized and reimagined. On one hand, the fact that the project was finally released on the web seems like a sort of admisison of defeat. On the other hand, the project's persistence and rebirth has potential to help researchers think of online archives and repositories in a new way. Indeed, Nelson is setting his sights on overtaking PDFs.

% As the coiner of the term ``hypertext'' and one of its pioneers, Nelson has a wide set of acolytes and followers. Among them are the founders of NewsLynx, a research project and platform under development at Columbia University's Tow Center for Digital Journalism. In August 2014, they wrote about the perils of online linking and tracking; specifically, they lamented the web's ability to only link in one direction, and praised Nelson's Xanadu for its foresight in recognizing this problem. They pointed out the ``hole at the center of the web'' that let Google ``step in and play librarian.'' Here they recognized how intensely the structure of the web has affected its content, whether by allowing for transgressions like Benny Johnson's, obscuring archives like the New York Times', and leaving Google to determine how to sort everything out.

% So in the summer of 2014, not only did Xanadu come to life, but its concept was validated. But in both cases (from Xanadu itself and its NewsLynx acolytes), the solutions were grafted onto the web, rather than proposed as a radical alternative. The web is only to be added and appended to, not replaced. In later sections, I will be looking closely at these two appendages to analyze their histories, strengths, and failures, and to suggest what they can teach us about structure of the web itself, and the ways that our thinking might have and might need to adapt to it.

