\section{The Stakes}

In the summer of 2014, I was closely following news about news. Working for the Nieman Journalism Lab as a Google Journalism Fellow, I split my time between writing articles about innovation in journalism, monitoring social media for stories worth sharing, and building an app that tracked link-sharing and conversations on Twitter (so even when writing code, I was following the news). During this summer, several news events coalesced to form the backbone of the exploration of this thesis topic. While these incidents may seem unrelated, my goal will be to showcase what these news events have in common, and set the stakes for the exploration of the changing nature of online research and cultural production on the web.

\subsection{BuzzFeed plagiarism incident}

In the summer of 2014, Benny Johnson, a BuzzFeed editor, was accused of plagiarism by two enterprising web-divers known only as @blippoblappo and @crushingbort. Publishing the article on a blog created just for the occasion called Our Bad Media, it was initiated when Johnson attempted to call out the Independent Journal Review for plagiarizing his own work. @blippoblappo and @crushingbort noticed the irony of a BuzzFeed writer accusing another publisher of stealing. BuzzFeed has long been accused of aggregating, remixing, and appropriating other outlets' work without payment. Perhaps because of this, they turned to web searches for examples of Johnson's own lifting.

The pair of detectives were likely not aware of how deep the copying went, though; they found three instances of unattributed sentences taken from everywhere from the Guardian to Wikipedia to Yahoo! Answers. When BuzzFeed editor Ben Smith replied by calling Johnson ``one of the web's deeply original writers,'' @blippoblappo and @crushingbort responded with six more offenses, here from the National Review, About.com, and the New York Times.

This set forced BuzzFeed to investigate, and a day later they fired Johnson and apologized to their readers; they had found a whopping 41 plagiarized phrases in 500 Johnson pieces. The rate and ease at which these seem to have been found is startling. If two researchers found so much bad-faith plagiarism in one day, and BuzzFeed's internal investigation had turned up dozens more, how could they -- how could \emph{anyone} not have not discovered this during any of Johnson's [HOW MANY?] years as a BuzzFeed writer? The offenses were hiding in plain sight.

The Washington Post's Erik Wemple suggested that some of these transgressions could have come from the specific demands of BuzzFeed; Johnson's ``multi-topical viral beat'' might have left him with not enough time to fully process the material, and not enough patience to link to every single source. Ben Smith points out that BuzzFeed is certainly not the first major publisher to deal with plagiarism in its ranks; this is of course true, but there is something new at play here. BuzzFeed's problem is still fairly new, in that it is trying to ethically aggregate and reappropriate from other online sources. While it's clear that Johnson stepped across this ethical line, it's still unclear where this line is. Smith's first reaction suggested that three offenses was not enough; he also implied that plagiarism on older articles or trite listicles would be more acceptable than newer, investigative pieces. But it seems that Johnson's attitude towards online aggregation bled into even more ``original'' investigative works.

While the legal and ethical implications of aggregating is a crucial topic for journalism and e-research in the 21st century, this is not so much my focus as the way in which the aggregational mentality changes the \emph{practice} of journalism and e-research. This is the case for both what can actually be found online, and what we perceive to be findable online. It is amazing that Johnson did not see himself as vulnerable; despite his obvious offenses, he assumed that no one would ever find them, and quickly accused others of plagiarism instead.

Moreover, the incident reflects a new paradigm of attribution and authorship. Johnson pilfered language from everywhere between Yahoo! Answers to the New York Times, with little distinction between the two. His most frequent transgressions, however, did seem to come from anonymous sources. As Wemple put it, he ``viewed [Wikipedia] as an open-source document,'' and grabbed phrases from government reports as if tax dollars allowed him to. His liberal use of Yahoo! Answers and About.com also points to interesting questions; did he somehow feel that it was more ethical to take from anonymous sources than other professional writers? Who should get the original credit, and how should they be compensated? Moreover, why did Johnson feel the need to treat them as original?

Johnson's safest option would have been to simply \emph{link to} the sources, and one wonders whether he now wishes he had. Linking would be safe; but it would also be tedious. It would interrupt the story if the reader decided to click on a link, possibly never to return to Johnson's article again. And of course, it would lay bare Johnson's bald pilfering of often dubious sources; not only to readers, but to machines.

BuzzFeed understands well this double power of the link. Tellingly, their apology post about the Benny Johnson incident likewise did not include links to the tainted articles. When internet users pushed back on this omission, BuzzFeed updated the post with plaintext URLs, without the anchor text. Why would they do this? While it might slightly increase the friction for an interested user to get to the article, it is more likely that it was to keep web crawlers and search engines from knowing about the connection. On the web, you are what you link to, and this post didn't want to link to, or be linked to, dozens of plagiarized articles. In more extreme cases, BuzzFeed has deleted older content outright that did not adhere to their journalistic standards.

In short, this controversy and BuzzFeed's reaction to it encompass many of the problems with assigning attribution and measuring impact on the web. It also points to the difficulty of online research, and the lack of standards and technologies for ethical, creative, original remix and reuse. This is as true for a tweet from today as it is a photo from decades ago. As newsrooms increasingly play the role of aggregator and context provider, they have a newfound ability \emph{and} responsibility to leverage archives -- whether their own proprietary archives or the web-as-archive -- to create and appropriate old content into new stories, merging news and history, placing sensational events in the longer phenomena that surround them, and centering the daily news in broader contexts.

\subsection{New York Times Innovation Report}

A couple months before BuzzFeed's plagiarism incident, a staffer at the New York Times leaked the company's internal Innovation Report, which my colleagues at the Nieman Lab called ``one of the key documents of this media age.'' The report looks closely and especially at the revitalization of its archives.

Not only do the archives have the power to historicize current pieces, trends, and events, they can also have amazing financial value, giving new life to old content that is repurposed, repackaged, and recontextualized.

The problem goes both ways; while not enough tools exist for Times staffers to resurface the past, it's also true that their new content is not properly prepared for the future. The Innovation Report likewise cites many problems that the company has with structured data and categorization.

Journalists have traditionally called the archive ``the morgue,'' and the Times Innovation Report both explains why this is the case and challenges its issues.

Finally, the Innovation Report confirmed that the role of repackaging and reappropriating old content was not just a problem for the BuzzFeeds and Huffington Posts of the world; old stalwarts with canonical archives are in the same business. This is, in effect, the new business of journalism: while citizens and activists increasingly serve as the newsbreakers, the journalists must take a step away from the epicenter of the event and report on everything that surrounds it instead. The web provides many new tools and affordances to do this creatively and engagingly; but the news industry has a long way to go and a lot to learn.

\subsection{Project Xanadu and Newslynx}

In this same summer, Theodor Nelson's Project Xanadu was finally released on the web. First conceived 45 years prior, Project Xanadu was famously the first hypertext project, under development for decades. Xanadu was the realization of an alternate hypertext system, one in which many of the pitfalls of the web -- the problems of attribution, measurement, and research that I aim to highlight -- are laid bare to be scrutinized and reimagined. On one hand, the fact that the project was finally released on the web seems like a sort of admisison of defeat. On the other hand, the project's persistence and rebirth has potential to help researchers think of online archives and repositories in a new way. Indeed, Nelson is setting his sights on overtaking PDFs.

As the coiner of the term ``hypertext'' and one of its pioneers, Nelson has a wide set of acolytes and followers. Among them are the founders of NewsLynx, a research project and platform under development at Columbia University's Tow Center for Digital Journalism. In August 2014, they wrote about the perils of online linking and tracking; specifically, they lamented the web's ability to only link in one direction, and praised Nelson's Xanadu for its foresight in recognizing this problem. They pointed out the ``hole at the center of the web'' that let Google ``step in and play librarian.'' Here they recognized how intensely the structure of the web has affected its content, whether by allowing for transgressions like Benny Johnson's, obscuring archives like the New York Times', and leaving Google to determine how to sort everything out.

So in the summer of 2014, not only did Xanadu come to life, but its concept was validated. But in both cases (from Xanadu itself and its NewsLynx acolytes), the solutions were grafted onto the web, rather than proposed as a radical alternative. The web is only to be added and appended to, not replaced. In later sections, I will be looking closely at these two appendages to analyze their histories, strengths, and failures, and to suggest what they can teach us about structure of the web itself, and the ways that our thinking might have and might need to adapt to it.

\section{Big Data???}

\section{Layers of containment}

We need generic, all-encompassing words—words that describe a broad swath of things in a very general manner (“things” being one such word). While reality can be sliced and diced in any number of ways, we sometimes need to talk about the undivided whole. A word like “thing” encompasses many words (and actual things) inside it, which can be envisioned as a hierarchy or set of concentric circles around an entity; for example, ordered by levels of abstraction, my tabby cat could be called a tabby, a cat, a mammal, a vertebrate, an organism, or a thing (roughly following Linnaeus’s biological taxonomy). This hierarchical structure of language both reflects and shapes the ways in which we have historically classified and organized knowledge, ever since Plato began searching for the “natural joints” in reality, and through the most canonical example: Dewey’s Decimal System.

Today’s methods of classifying – and possibly, organizing knowledge in general – have radically changed; this is a phenomenon that I will explore at length in this paper. However, we increasingly need such generic words to describe the increasingly digital, ephemeral world around us. The software world has brought us objects, data, documents, information, and a word I will return to: content. Its processes include products, services, applications and platforms. I am interested in terms like these, because they can expand and contract in meaning, and in the process they skirt debate and risk glossing over embedded biases and controversies. They are at the top of a linguistic hierarchy, and threaten to subsume the nuances and contingencies within the subcategories. At the risk of sounding trite, everything is a thing, which is logically impossible to argue (and in fact, the ontology language that underlies the Semantic Web uses “thing” as the base layer under which all other words go). But what is a document, or data? How does our use of these words carry contextual weight?

Tech terms like these are far removed from the realities they describe, and often just as far removed from their original meanings. Remediated words balance an inheritance and a distance from their original (premediated) contexts, and much interesting work has explored the long histories of current words. For instance, several scholars have historicized and questioned the use of the word “data.” Daniel Rosenberg charted the term’s use through shifting contexts since the 18th century, noting that it was initially used to describe an indisputable fact or “given” in an argument (from Latin dare).  Annette Markham likewise questions the use of the word “data” in its modern context, suggesting that, “through its ambiguity, the term can foster a self-perpetuating sensibility that ‘data’ is incontrovertible, something to question the meaning or veracity of, but not the existence of.”  Johanna Drucker suggests implementing its counterpart “capta,” which highlights the inherently plucked and pre-envisioned nature of all information. 

Other contemporary words have been similarly historicized and questioned. John Seely Brown and Paul Duguid trace the history of the word “information” in The Social Life of Information and forthcoming research, highlighting its long history as an “unanalyzed term.”  Likewise, Tarleton Gillespie draws attention to the word “platform” in the context of the software industry, focusing on the implications of the term’s historical meanings.  In each of these cases, the appropriation of abstract words informs and reshapes our own notions of these words and the objects and realities that they represent.

One such remediated word, foundational to the web, is the “document.” It was previously understood as a physical, printed record—usually an original. A signed mortgage might be a document, but a photocopy was not; the word “document” went hand in hand with the idea of an original. When digital word processing tools co-opted “document” as a digital artifact, this made an age-old word new and strange. In many ways, it also forged the foundation of the web, as Tim Berners-Lee used the architecture of the document and file system as the web’s basis.  Taken for granted today, this decision was not at all a given, and in fact stirred much controversy. Ironically, many of the web’s detractors pointed precisely to the web’s lack of an “original” document copy as its primary shortcoming, a critique that undoubtedly informs my own inquiry into its infrastructure, and which I will return to. 

Along with document, data, and information, I am especially interested in the word “content” to describe creative works or texts, primarily (or perhaps exclusively) residing on the web. It is a word that is bound to encounter derision, whether from “content creators” (never self-defined as such), information theorists or media scholars. In a recent talk at MIT, Henry Jenkins referenced the word’s derivation from the Latin contentum, meaning “a thing contained.”  Doc Searls frequently criticizes the term for its ties to marketing, implying a one-way web where content is a catchall term for anything that can be packaged, sold, and consumed online. 

Another, perhaps friendlier way to describe content is as a “link.” Where content implies a container (containment), a link implies a connection (expansion), promising to break free from the contained information. Looking at the link favorably, if a publisher adds a hyperlink to an article, it purports to show not only erudition (the publisher has read and vetted the content within), but also altruism (the publisher is helping the content creator, and you, the user, reach one another). But here, the link surrounds the content. In effect, it is the original container, adding the first layer of context to the content, but diluting its core in the process. In studying the origins of the link’s structure and the web’s infrastructural qualities, we find many ways in which the web’s very structure, as well as the creators, indexers, and archivists that work with content, acts as a containing and homogenizing force on original creative works.

In this essay, I trace the lifecycle of “content” on the web, from its inception to its eventual storage in archives and databases. Treating it as a sort of biography, I show how the web exerts varying layers of containment on its intrinsic data, rendering it content in the first place. The web’s scalability embeds original content, as if in concentric circles, in a series of wrappers or levels of abstraction around the original source. Therefore, the original photograph finds itself embedded under several layers of representation. The first such wrapper, the original converter into content, is the URL (Uniform Resource Locator), which serves to represent multimedia in a homogenous piece of text that renders everything “uniform.” From there, several layers of representation are placed on top of it, starting with the hyperlink (an HTML element that forges connections between documents). An HTML document is a hybrid object; links contain links, and content is consecutively embedded in secondary sources, social media platforms, search results and archives. At each layer of containment, the content acquires new metadata (or context), created by individuals and machines, that indelibly affects our understanding of the original source. These varying layers of context, representation and containment reflect new modes of information organization and storage, and ultimately affect the ways in which we organize and represent multimedia works, influencing our understanding of history and the canon.

\subsection{Theorizing containment}

In crafting a biography of content, I am nodding towards the “biography of things” introduced by Igor Kopytoff. Like Kopytoff, I am interested in the passage of objects from one state to another, and the transitional moments that mark events in a thing’s history. This framework complicates the idea of any single, indelible act of categorization on an object—instead, an object is “classified and reclassified into culturally constituted categories.”  But I also look to Kopytoff and the “social life of things” as they relate to commodities and exchange value. For Kopytoff, states of transition are equivalent to acts of exchange—in other words, the transitional is also the transactional. But if an object’s “saleability” indicates its commodity status, what is the saleability of a digital object? Is content a commodity? The word, used so often in marketing contexts, implies a transaction of sorts—whereas the “link” implies free exchange. How might the ways in which a piece of content is transferred, linked, and shared online reveal something about the web’s culture and economy, its notions of value? How does the language of free information and open source likewise affect or hide the transactions at play? Does the lack of a privileged “original” copy explode traditional notions of value and exchange? I am especially interested here in the act of replication, and the ease with which digital objects are copied. Any time you move a file, your computer actually copying it (and deleting the original); any time you watch a video online, your computer is actually reading a local copy. It is no accident that so much of modern computer architecture was developed at Xerox PARC, a research lab sponsored by the world’s foremost copying company; Xerox was openly nervous about any file systems that did not employ copying as a central act. 

In following a piece of content, I am looking to analyze the whole; a single online photo might turn up in far-reaching corners of the web, and imply many acts of exchange and use around it. Specific acts of classification play into a greater whole that is interlinked by societal understanding of what constitutes a category, and how an object should be categorized. Kopytoff recognizes the need for healthy and cohesive classification as well: “Both individuals and cultural collectivities must navigate somewhere between the polar extremes by classifying things into categories that are simultaneously neither too many nor too embracing. In brief, what we usually refer to as ‘structure’ lies between the heterogeneity of too much splitting and the homogeneity of too much lumping.” 

Here I am informed by the study of infrastructure, and especially Geoffrey Bowker and Susan Star in their book Sorting Things Out: Classification and its Consequences. Tracing the history of classification as it is used formally (in standards) and informally (in the words, framings and mental models we are perpetually forming), they argue that each act of classification affects the classification system itself, and future classifications in turn. At its most abstract level, classification is the application of language to reality; whether you are calling a kitten “cute” or a person “male,” you are framing the subject at hand and privileging certain discourses and interpretations over others. Taken at scale, these acts affect the entire structure of technology and culture. Bowker and Star see infrastructures and standards as intimately interlinked; each one inherits the values and inertias of the systems around it. They point to the more than 200 standards imposed and enacted when a person sends an email—standards that overlap and depend on one another in important ways.  Sorting Things Out, along with Star’s companion article “The Ethnography of Infrastructure,” point to the large-scale effects of small-scale sorting. They highlight the problems and limits with traditional classification, and suggest ways to render it more dynamic and responsive. But Star also points out, quoting Gregory Bateson, “What can be studied is always a relationship or an infinite regress of relationships. Never a ‘thing.’”  In other words, in studying a single thing, it is important to recognize its embeddedness; content is never just content, and to describe it is to also describe its containers.

I also draw in part from questions of ontology, agency, and context as brought out by actor-network theory and Science and Technology Studies.  The classification standards in place, as suggested by Bowker and Star, give varying levels of agency to the systems and the humans working within them. While the web itself does not impose any singular ontological framework (except, to an extent, in the case of the Semantic Web), the databases and platforms that draw from it use its structure to organize their own knowledge. Moreover, many of these databases – such as the search indexes run by Google – apply advanced algorithms that filter and divide content at massive scales, untouched by human hands. In other words, the interplay between nonhumans and humans is paramount in the production and distribution of content on the web (in fact, many of the web’s end users are also machines, understanding and accessing data through HTML markup or APIs rather than words and images). My framing of content/container may seem to imply a hierarchical or one-way relationship between the whole (the web or the archive) and the part (the piece of content), forgoing the “flat” network proposed by ANT. However, as Bowker and Star suggest, the part always influences the evolution of the whole, and the layering of the web turns the container back into content. Debates about the border between content and context (or data, or metadata) fall apart, as they often collapse into one another.

\subsection{Stage One: The URL}

Content has a rich backstory before it arrives on the web, but I am treating its birth – the moment when an object or artifact becomes a piece of content – as the moment in which it is uploaded to the web. It now has its first container, and its first piece of web-native metadata: the URL. Even before it is connected to other URLs (at which point it becomes a “link”) an end user can access it online via a string of text. As Tim Berners-Lee tells it, the Uniform Resource Locator was one of the most difficult concepts to develop and understand as he began to weave the web.  To this day, he sees it as the web’s most foundational element, and its importance is amplified even further in the Semantic Web. The URL itself is a remediation of old standards and practices. It mimics the file folders on our home computers (an intentional decision, so it could be understood and adopted quickly), implying a hierarchical, document-based structure. Interpreted hierarchically, the URL can be seen as an address, pointing us to increasingly specific locations until we arrive at the document in question. The virtual space of the web here seems to mimic physical space in the world, suggesting that one can find a document at a certain “path” under a certain “domain.” By turning all rich multimedia into “uniform resources,” the URL is a homogenizing force, encoding all content as text and turning it into a reference rather than an experience or narrative.

URLs are not created equal, however, and savvy web users can read a great deal of information in this set of text. A “.org” domain, for instance, might imply a nonprofit or philanthropic institution where a “.com” connotes a business. A long, seemingly obfuscated URL might contain spyware or viruses. A URL that ends with “.html” or “.jpg” will probably be a specific document (or piece of content), but one that ends with “/users/?friendrequest=true” is more likely to be telling a social media site to request friendship with another user. Indeed, at the current stage of the web’s evolution, a URL is not by definition a resource; it could yield no content and simply trigger a piece of code, allowing any arbitrary action. Moreover, even documents are subject to change, and the web has no built-in way to track content’s erasures and additions. In other words, the “Uniform Resource Locator” is not necessarily uniform, nor is it necessarily a resource. Even that vague, homogenizing definition does not hold up.

Eszter Hargittai points to the need for technical expertise in order to properly understand a URL and the implications behind it.  It is easier for a user with less experience with the Internet to be duped by a phony URL that installs spyware or viruses; it is also more difficult for such users to find the content that they need when navigating through links. For instance, some users do not fully understand the difference between the web and Google, or whether a link in an article or feed will take them to the same source (the same domain) or a different one entirely. The URL thus serves as a barrier to understanding and retrieving information from the web for those who have less familiarity; technical knowledge enables information retrieval, and a lack thereof leaves users in the dark and vulnerable.

URL “shorteners” such as those employed by the firm bit.ly likewise add additional layers between user and content, and further obfuscate the final destination. With a URL shortener, a small and innocuous domain (such as “bit.ly/a423e56”) can take a user to any corner of the web, whether at the highest level (think “google.com”) or its most specific (like “pbs.twimg.com/media/Bm6QZAGCQAADEOk.png”). Shortened URLs have the same final reference point, but they no longer mimic the spatial world or even most personal computer filesystems; we have replicated and obfuscated the URL to the extent that any sort of uniformity or direction is impossible. 

Perhaps the explosion of the URL was an inevitable byproduct of the web’s very structure. It is infinitely distributed and highly networked; it shuns hierarchical organization schemes, which seems to go against the “domains” and “paths” of the URL itself. Indeed, both Berners-Lee and Theodor Nelson (the original coiner of the term “hypertext” and its first champion) explicitly highlighted the power of the link to cut across tree structures and find new, unexpected associations.  Where knowledge was once shaped like a tree, on the web it looks more like Deleuze and Guattari’s rhizome: an infinitely “intertwingled” mass. One cannot make sense of it using URLs alone, but links offer a start.

\subsection{Stage Two: The Link}

The birth of the “link” occurs at a second level of containment, after an object becomes “content” with a URL. The link wraps the URL in an HTML element that allows it to be quickly accessed from another page. Without links, the web would just be a series of disconnected nodes; with links, the web becomes a network. Bowker and Star suggest that links have the power to classify without any human agency or intervention, which forms the basis of this section: “Every link in hypertext creates a category. That is, it reflects some judgment about two or more objects: they are the same, or alike, or functionally linked, or linked as part of an unfolding series.”  Bowker and Star are not the only ones to cede agency to the link, and many disputes and debates occur over links; even in 2002, Jill Walker asserted that “links have value and they give power.”  In many ways, the link is the battlefield for the political economy of the web, serving as a sort of digital currency and object of value exchange.

All the same, the link is a seemingly innocuous object. We usually consider it taking the form of a blue, underlined piece of text on a webpage (under the hood it is known as an anchor tag—the string “<a href>…</a>” and everything in between—in an HTML document). Clicking on the link turns the object into a mechanic, leading a user down a rabbit hole of subsequent destinations and redirects (all employing some dozens of standards) before landing on the target destination—back to the URL. The URL is only one attribute of the link, along with others that determine, for instance, whether to open the link in a new tab or window—so in a literal sense, the link contains the URL.

The link is forever associated with (and perhaps plagued by) the footnote. Nelson’s hypertext manifesto Computer Lib/Dream Machines praises the screen for permitting “footnotes on footnotes on footnotes,”  and Berners-Lee’s web takes the traditional citation as inspiration. Nelson belies himself by consistently contrasting hyperlinks with footnotes; in some senses, one cannot escape being a remediation of the other. But the link’s readable text – its manifestation in a browser, known as the anchor text – adds another layer of semiotic containment and enrichment to the original content. The “jumpable interconnections” that Nelson envisions are built into the fabric of the writing rather than set aside like a footnote.

The anchor text has no innate relationship to its target, and it is only pointing to the target’s address. As a result, the link can be seen as a sign. Analyzing the link’s anchor text through a semiotic frame reveals a number of interesting conventions and uses, each of which bears underlying motives. The many flexible uses of the link may follow something like Charles Sanders Peirce’s semiotic triad; when a link says “click here” as opposed simply linking the text as so, it may be forming an indexical rather than symbolic relationship to the target. When a link’s text is identical to its address, like “http://www.google.com,” it seems to be removing this layer entirely. However, there is nothing stopping someone from putting a completely different address into the anchor text, further emphasizing the lack of relation between anchor and target, or signifier and signified. This distance is what allows a scam artist to direct an unknowing user to a phony bank website, even if the stated URL is for their real bank. It is also used for more playful and innocuous ends, such as with “rickrolling,” a meme where someone provides a purportedly useful link, but it actually leads to a video of Rick Astley’s 1987 hit “Never Gonna Give You Up.” Whether playful or nefarious, both of these uses are enabled by the structure of the link, and the lack of relationship between the text and the target.

Many studies have attempted to glean insight from the link by assuming, like Bowker and Star, that links create categories. On one hand, it seems extremely liberating to sidestep the ontological dilemma of what that category is, and simply treat it as a raw signal. I see this ability as the basis for much of the revolutionary rhetoric of the web and the power of networks. On the other hand, the lack of relation between text and target seems to point to the problems with this approach; a sign is not the same thing as a signal. Studies and practices that analyze and aggregate links would do well to closely analyze the text of the link. There have been very few large-scale studies of the semiotics of linking, or the way in which the anchor text helps to gain insight into the target resource or the connection being made. One exception comes from a small 2006 study of automated blog classification, where the researchers determined that the anchor text was in fact the best signal for improving classification.  One of the researchers now studies the text of tweets to gain insight into the links they embed, once again treating the users’ descriptions of links as more important than what networks are sharing it.

But for now, most studies simply take an aggregate view of link sharing, treating each connection as equal regardless of context. This has vast implications for the news media and has undoubtedly affected content creation and discourse. Anyone who shares an article inevitably, and perhaps inadvertently, raises the article’s profile and algorithmic rank whether they liked it or not. Algorithms might therefore prefer controversial links rather than universally liked, substantial, or thought-provoking ones. This could create incentives for publishers to use unnecessarily inflammatory or partisan language, with the assumption that despite how users feel about the content, they will certainly click on it, and possibly share it. This is best exemplified by Rusty Foster’s “Today in Tabs” newsletter, which popularizes the idea of “hate-reading” and links to some of the most infuriating articles in the news. It is not clear to algorithms whether or not someone liked an article (let alone why they liked it)—it is only clear that they are talking about it. This may be because there is no straightforward way for an automated system to understand the many cultural nuances behind a link.

This limitation is apparent to Berners-Lee, who has in recent years championed the Semantic Web as a way to make the web more structured and machine-readable. The Semantic Web allows for links themselves to be annotated and queried, so that, for example, we could search for “users who disagreed with this article” and not just “users who linked to this article.” This carries great promise not only for a machine-readable web but a new order of linkage and network formation. The W3C (the standards organization for the web) maintains appropriately revolutionary rhetoric around the Semantic Web, and has tried out scores of marketing terms in its efforts. It alternately envisions a “web of data” (rather than documents), a “Giant Global Graph,” and “Web 3.0,” a particularly telling attempt to couch the Semantic Web as the inevitable next step of forward progress. However, while linked data has been useful in smaller-scale initiatives, the Semantic Web movement is progressing very slowly. It also brings its own problems; while a web of documents is one level removed from the data itself (and therefore more difficult for machines to read), at least it keeps the source context intact. The Semantic Web also imposes its own set of ontologies, hierarchies and categorization schemes, a problem that I will return to.

Another alternative to the web’s form of linkage comes from Ted Nelson, a longtime critic of the web’s architecture. As the original hypertext visionary, his scheme, called Project Xanadu, floundered for decades, and was never truly built in the way that he envisioned. When critics suggested that Xanadu was the first failed web, Nelson bristled: “HTML is precisely what we were trying to PREVENT—ever-breaking links, links going outward only, quotes you can’t follow to their origins, no version management, no rights management.”  Xanadu’s most important feature, absent from the web, is the two-way link; when one document referenced another, the target document referred back to the original in turn. The hyperlink on the web, for all its flexibility, does not escape the trappings of the footnote in this single, very important way. Links always move backward, and given the lack of a canonical URL on the web (another of its limitations, which the URL-shortening phenomenon compounds), finding all the citations for a single document is next to impossible. Jaron Lanier believes this simple omission has profoundly affected culture and economics, which forms a cornerstone of his recent book Who Owns the Future? 

But in the absence of replacing or reconfiguring the web’s current structure, the one-way, semantically meaningless link remains the web’s primary organizational scheme, and the “click” remains the proxy for attention and engagement. Clicking on a link is not only a navigational mechanic; it is a signal of intent and interest, which influences algorithmic decisions and other readers in turn. It is also often a financial transaction between unseen actors; each link clicked and page viewed is a new “impression,” causing money to change hands between content distributors and advertisers. This has in turn changed the aforementioned semiotics of the link, and the meaning of its anchor text.

There has been much recent controversy surrounding the restructuring of the news headline in the hyperlinked age. Where traditional headlines might read “The Global Fight Against Tuberculosis,” a more recent one is more apt to say, “It Kills 3 People a Minute, but That’s Not Stopping This Group of Superheroes.”  The headline is “click bait,” playing to a user’s innate curiosity (Atlantic writer Derek Thompson calls it the “curiosity gap”)  without telling them the substance of the article or the actors in play (tuberculosis, the victims affected, the Global Fund, the Gates Foundation, and others). These actors and the issues they are tackling are reduced to pronouns. Here even the content becomes glossed, and a click is just as likely to signify curiosity about what the content is, rather than any genuine interest in the content itself. Machines are not likely to recognize these nuances, which results in false identification of public interest and discourse. The website Upworthy is a canonical example of click-baiting headlines, and even its organizational structure is revealing; the company creates no original content, but instead employs people to trawl the web, find content, and put a new headline on it. The team is not creating new content, but new containers—and it is one of the most popular and successful media business efforts of recent years. Despite this, Upworthy has been mocked frequently, such as via the joke news site “Clickstrbait,” which leads users down a rabbit hole of curiosity-inducing headlines without guiding them to actual content.

Interestingly, Upworthy is one of the first websites to attempt to move beyond the simple “pageview” metric, heralding a new measure of success called “attention minutes.” These metrics will make privacy advocates cringe; by monitoring which browser tab is open, where the mouse is pointing, or how much of a video the user has watched, Upworthy hopes to understand user behavior more deeply. Upworthy’s blog claims that “this is a metric focused on real user satisfaction,” but it is still a measure of behavior as a proxy for emotion, and the end goal (a like? a share? a donation to a worthy cause?) remains unclear. 

In all of these cases, the layers of containment could be seen as layers of signification. I stated earlier that the birth of content (the transformation of an object into content) occurs at the moment it is uploaded to the web, and accessible via a URL. Here is where it moves from essential object to sign and message, in Jean Baudrillard’s terms. Looking at the web as a designed artifact with a specific, graspable structure, Baudrillard proves fruitful in emphasizing the web’s political and philosophical origins and ramifications:

The semiotic revolution…concerns virtually all possible practices. Arts and crafts, forms and techniques both plastic and graphic…which until then were singular and distinct, are synchronized, and homogenized according to the same model. Objects, forms, and materials that until then spoke their own group dialect, which only emerged from a dialectical practice or an original style, now begin to be thought of and written out in the same tongue, the rational esperanto of design. Once functionally liberated, they begin to make signs, in both sense of the phrase (and without a pun): that is, they simultaneously become  signs and communicate among themselves. Their unity is no longer that of a style or practice, it is that of a system. 

Replacing “design” with a word like “information” or “data” reveals the homogenizing force of the web and its ability to squash varieties of creative works (photos, videos, text, music) into data, which allows for easy exchange, commodification and reuse.

\subsection{Stage Three: The Feed, the Index}

Links rarely exist in isolation. For one, links contain links themselves, as I touched on in the last section. But another form that the link takes is as part of a list or sequence. Whether it is a digest (on email), a feed (on Facebook, Twitter, or RSS), a set of search results, or a list of “related articles,” users are almost always confronted with several choices for what to click on. In this section, I look at the ways in which links get aggregated, indexed, and fed to users, allowing for another layer of containment beyond the link. For instance, while an article might embed an image, the article itself is then embedded and contained as a search result or single item in a table. The table usually truncates the content into a headline, and perhaps an image or opening paragraph. This can allow for a higher-level view of a major topic, author, or other organizing factor, but at the expense of hiding the richness of the content within.

The aggregators, indexers, and summarizers of the web are its search engines and social media feeds—in other words, the most powerful and profitable tech companies in the world. While the content creator usually has to win the attention of the distributor, the distributor in turn must always play the aggregator’s game, completely powerless without it. This is evidenced by Upworthy itself, who recently found its content potentially demoted in Facebook’s algorithm with no meaningful explanation, shrinking its immense traffic to half of its previous size.  Another major content distributor, the lyrics annotation website Rap Genius, recently found its pages move from the top hit on Google to its seventh page, due to changes in Google’s algorithm.  These content aggregators can move around large swaths of content (millions upon millions of interlinked pieces) via slight changes in their codebases, with no obligation to inform anyone of the reasons or even that it is occurring. This is perhaps the highest level of containment, and few (if any) actors can claim to contain these sites in turn.

To be fair, Google did explain its reasoning for the Rap Genius demotion, and the dispute was telling. Rap Genius had launched a “Blog Affiliate” program, which clandestinely offered to tweet out any blog post in return for links back to the Rap Genius site. In other words, Rap Genius was engaging in SEO (Search Engine Optimization) spam, attempting to falsely boost its search rankings by asking bloggers to post unrelated links back to their site. This is one high-profile example of what many smaller players do every day in order to keep their businesses alive: game Google’s algorithm in order to bolster their search rankings. SEO is, in effect, an entire industry built on gaming links.

This works because Google’s PageRank algorithm is primarily derived from who is linking to whom. In effect, their link-based classification scheme is what made them the dominant information provider that they are today. Prior to PageRank, web crawlers and indexers like Yahoo, HotBot, and AltaVista provided a plethora of options for Internet search (even these, in all their heterogeneity, were seen at the time as a major threat to the open web). But each was based on a traditional, hierarchical classification scheme. In PageRank, Google found a way to embrace the web’s disorder; where Yahoo insisted on keeping an organized system, Google relied on links to sort everything out. Clay Shirky argues that this is what allowed Google to surpass Yahoo and become the first truly “Web 2.0” company, asserting that on the web, “ontology is overrated.” 

Google famously published their initial PageRank algorithm, and once the cat was out of the bag, advertisers and spammers began to exploit it, inserting links not for their usefulness or relation to the text, but to improve their pages’ search rankings. A large portion of website hacks and attacks are merely to insert hidden links on the targeted sites. In the process, Google has had to remain one step ahead of the advertisers, with the link as the battlefield, influencing the web and changing its structure in turn. But this battle has mostly been played out by machines, which are responsible for a substantial amount of the links created – as well as the links browsed and followed – on the web. Besides a generic, easily replaceable piece of metadata in a web request, it is in fact impossible to tell whether a website request is coming from a human or a machine. In Google’s published PageRank paper, Sergey Brin and Larry Page provide a curious “intuitive justification” for their algorithm that seems to conflate the two:

PageRank can be thought of as a model of user behavior. We assume there is a “random surfer” who is given a Web page at random and keeps clicking on links, never hitting “back” but eventually gets bored and starts on another random page. The probability that the random surfer visits a page is its PageRank. 

This is a very strange user indeed, assumed to be easily “bored,” distracted, and clicking on links at random. Moreover, this was an assumed user in 1999, and the “model of user behavior” must undoubtedly be changing as the web’s capabilities and browsing habits change. This bizarre mixture of human and nonhuman, as well as the substantial influence that links have on the information we encounter as everyday users, speaks to the usefulness of actor-network theory in framing the political economy of links and linking.

While links are shared for a variety of reasons – some of them more nefarious than others – the blogging and tweeting culture of “Web 2.0” holds to the principle of link sharing for mutual interest and benefit. If two bloggers like one another’s content, they will agree to link to each other on their respective blogs. This happens on the “blogroll,” a list of other blogs that a blogger might recommend, usually presented as links in the blog’s sidebar. Here the link functions as an act of exchange under the guise of free information sharing. Looking at it through the lens of Marcel Mauss’s writings on gift exchange, however, it seems to carry more weight than this: “Exchanges and contracts take place in the form of presents; in theory these are voluntary, in reality they are given and reciprocated obligatorily.”  This can be seen beyond the blogs of Web 2.0; users exchange links on Twitter and retweet, favorite, or like posts on various social media platforms. In each case, a link or like on a social media post is performative and transactional, with the implicit expectation of a future like in return.

Moreover, these link exchanges solidify existing networks of bloggers and content creators, perhaps galvanizing the network but at the risk of collapsing into “filter bubbles.” Many studies of links have traced political homophily, public debate, blogs and global flows of information; if we take these at face value and treat hyperlink usage as a proxy for importance, impact, and communication, then link-sharing can turn conversation inward, allowing searchers to see only blogs that have overtly linked to one another (blogs which, presumably, have similar views and opinions).  While the Internet may allow for a more heterogeneous group of voices to surface than in traditional media (and indeed, this is one of the ways in which the medium is widely celebrated), one must still take part in link sharing with a particular group in order to be found, leading bloggers into already-established and tightly wound networks. This phenomenon is most expertly outlined by Philip Napoli, who calls it “massification”: in the editorial and algorithmic decisions that determine where links are placed and directed, there is a distinctive replication of old “mass” media patterns. 

While content creators, distributors, and aggregators are locked in this battle over links, what happens to the actual user who visits a site, application or search engine? The user is presumably after “content,” and unless they were provided with a direct URL, they can only access it through a series of layered containers. Moreover, the information, story, or “piece of content” that they may be after is replicated and embedded in different contexts and myriad places around the web. The end result, when a user goes to Google to search, is often repetition. The same piece of content appears everywhere, such as a canonical image for a popular news story, meme, or theme.

Repetition plays a strong role in Freud’s definition of the uncanny. I wouldn’t suggest that a user is frightened by search results, but there is a sense of unease or anxiety in finding the same content repeated ad infinitum. Google’s “search by image” feature provides a list of “visually similar images” that reveal hundreds of nearly identical photos. For example, an image search for “office meeting” turns up the same stereotypical figures; businessmen and businesswomen in suits, seated around a table, poring over seemingly identical documents in a typical conference room. The photos are mere signifiers—it seems clear that the subjects are actors, and no business is actually being done. The emptiness of the content itself, and its endless repetition, is highly unsettling. Freud’s notion of the uncanny has also been applied to the context of online advertising. Often when a user visits a product page, the same product is then re-presented to them in the sidebar of a completely different site—often mere minutes later, but other times it takes days or months. This is a more direct application of the uncanny, as it does make the user feel as if they are being watched. 

\subsection{Stage Four: The Archive}

Content’s final resting place is in the database, or archive. But all the same, it is not fair to call it “final,” since the context and metadata surrounding it is always subject to change. Moreover, this lifecycle is vastly oversimplified; often the content reaches a database as soon as it is accessible via a URL (for instance, with a photo that is uploaded to Flickr or Instagram). Then as the content moves around the web, affected by other creators, distributors, aggregators and indexers, it is placed in an untold number of databases, with varying structures and associated metadata. So in a sense, the four stages that I have outlined here collapse on one another, and the framing that I have offered is far too neat, simple, and narrative-driven for the distributed, infinitely networked, rhizomic web.

The database is a different form of container than the others, as it is in fact not truly of the web; it merely talks to it, interacts with it, works with it. While users increasingly treat the web as a database – what we know familiarly as the “cloud” – it is less distributed and hypertextual than that metaphor seems. There is no single database, but rather very many, housed on servers around the world. Each of them faces the same challenge: how to flatten and store the infinite possible contexts, networks, and signals that the web has created around each piece of content, into a format that allows a user to find it efficiently using any number of contexts. Perhaps a user is looking for everything stored in a specific time frame, a certain format, a dedicated category, or any combination thereof; in each case, the archive serves the role of retrieving the information needed.

As a result, the archive must anticipate any possible need from any possible user, whether they request content today or far into the future. Any signal that is left out is lost potential knowledge. So an archivist, most often associated with storing the past, also plays a crucial role in predicting and affecting the future. Jacques Derrida traces this phenomenon in Archive Fever, where he calls the archive “a pledge, and like every pledge, a token of the future.” 

There is no reasonable way to store every possible route through a database that a user might take; this would require infinite storage and processing power. Given the highly networked, context-focused organization of the web, it is an impossible task. Derrida highlights this challenge as well: “the limits, the borders, and the distinctions have been shaken by an earthquake from which no classificational concept and no implementation of the archive can be sheltered. Order is no longer assured.”  Derrida thus relates the archive to a prosthesis, a built and artificial entity that mimics but does not replicate the infinitely rich sensoria of reality. Claire Waterton, citing Michael Taussig, also uses the border metaphor to describe the increasing diffusion of information: “the border zone of representation is currently expanding, proliferating, and blurring, becoming permeated by itself.” 

Seen in this way, the database is perhaps the only truly containing force; the previous stages are in fact expanding contexts and meanings for each piece of content, and it is only in retrospect (through the archive) that it becomes contained. But all the same, we cannot see the content except through the archive. And with the assumption that a border must be drawn through the expansive, innately borderless web, the question is where and how to draw it. Lisa Gitelman laments the way in which the archive reduces “ideas into character strings,” or in the case of rich multimedia, encoded, flattened and unsearchable bits.  Character strings and encoded bits are devoid of context and semantic meaning. They certainly do no justice to the richness of the original content, which points to a proliferation of associated narratives (for instance, each photograph has a photographer, a subject, a setting, a camera, and all of the processes that formed the “becoming” of these entities, and which we implicitly consume and consider as we look at that photograph).

My aim is not to suggest any overarching solution to the limitations of the archive; it is, in fact, this very impulse that has often set back the work of retaining knowledge and history. Bowker and Star point to the myriad efforts of “universal classification,” dating back to the Tower of Babel, all of which have essentially failed. Classification is an inherently epistemological, performative act that is always embedded in a certain community and always subject to change. In short, it is socially constructed. In order to fully recognize and remember this, Bowker and Star suggest the framework of “boundary infrastructures” to acknowledge and work with the limitations of traditional classification. Boundary infrastructures make use of boundary objects: “those objects that both inhabit several communities of practice and satisfy the informational requirements of each of them.”  In practice, these objects (and the infrastructures that work with them) will maintain slightly different meanings in each community, but they are common enough to be recognizable to multiples. While this approach is more of a framework than a solution, it rightly discourages the drive for an overarching schema for every object and community. By recognizing that no system will ever be perfect, it instead highlights the need for a loosely linked multiplicity of them.

Likewise, I intend to propose that the web itself should not be universally schematized, and its content will never be singly and correctly categorized. In a sense, the proliferation of databases and motives for classification that the web provides allows for more “ways in” to the content than if the web were stored at a single endpoint. The Semantic Web is an interesting hybrid of centralized and distributed; it aims to bridge traditional taxonomy and contemporary chaos through its use of user-generated ontologies. In order for machines to understand a network, everything must be definitively categorized, but the categorization scheme itself is subject to change. Certain standards arise, but each individual or community is free to create its own form of linked data. All the same, the slow adoption of the Semantic Web may have to do with its reliance on these “ontologies”; even if multiple ontologies can coexist, they are still trying to compromise the web’s disorder.

Derrida’s “archive fever” is both a personal and an institutional drive. Google and Facebook store user data (including user-created content) with abandon, inventing new contexts at each turn. Users bookmark, download, pin, and clip online resources, sometimes all at once. Built-in browser solutions like bookmarks and history haven’t changed their structure in years, and it shows—they store nothing but the URL. “Bookmark” is a misnomer of a remediated word, as books can’t change or disappear overnight, while “history” implies a time machine that the web doesn’t have. Personal note-taking and online “snapshot” tools aim to create a sort of personal, annotatable intranet for users that want to filter signal from the noise (see applications like Evernote, Pinterest and Zotero). However, each system is limited by the borders of the database, and aside from folders and tags, none provide a useful way to store meaningful associations between these documents.

In all cases, the problem of “information overload” is paramount, and the virtual piles of documents and content get increasingly difficult to wade through and make meaning of (unless one takes the high level, “big data” view—a perspective that has its own pitfalls, as danah boyd and Kate Crawford show).  But information overload is nothing fundamentally new. Ann Blair finds a complaint about a “confusing and harmful abundance of books” as early as 1545 (in Conrad Gesner’s attempt to catalog all known books), and many other scholars have historicized information overload and management strategies (such as commonplace books, scrapbooking, “stringing,” and the encyclopedia).  One of the most canonical methods of organizing too much information is the card catalog, in use by libraries for more than a century; early hypertext systems, such as Xerox PARC’s NoteCards and Berners-Lee’s ENQUIRE, are noteworthy in their remediation of the affordances of this old tool.

However, the associations, trails, and lists sparked by the web add to the possible avenues for research; the myriad interconnections between documents may be more responsible than anything else for the seemingly unprecedented amount of information. In response to this, users store everything, in hopes of using the archive’s power of containment to understand it. But containing is not understanding, and by turning rich multimedia into bits of text, containment in fact furthers the distance between the user and the real, lived experience that the content aims to capture and describe.

\subsection{Postscript: Erasure and Afterlife}

Content has an afterlife when it goes through a sort of reversal of the stages outlined above; it must be plucked from an archive by a search algorithm, which is in turn responding to a request by a user. Some content never does live again; for instance, nearly one-third of all reports on the World Bank’s website have never once been downloaded.  This does seem to run counter to the “pack-rat” mentality of users and institutions proposed earlier, but it also points to the vast amounts of knowledge we are creating that may require a new format to be rendered useful. This is not to say that the knowledge contained in the World Bank’s documents has been utterly forgotten (the document could be distributed by email, or presented at a conference—the inability to track it is the crux of the problem); only that it is (literally) not helping anyone in its current structure.

Other content may in fact be useless, or worse, detrimental. Knowledge, and even facts themselves, have been rephrased, rewritten, and reversed for as long as facts have held public influence.  Some content is outright false, misleading or slanderous, and other content is simply embarrassing. Certain regrettable pieces ought to be remembered (such as a racist comment made by a powerful public figure); others are more likely best forgotten (such as a teen’s suggestive selfie that gets distributed around the internet). But the question is not who determines what deserves a place in history and what should be erased; even if one deletes the content, it is not likely to disappear entirely.

The user’s experience of deleted content is the broken or dead link, the ubiquitous 404 “Not Found” error page. While in some cases a dead link signifies a necessary removal (such as the teen’s photo), in others it is a stand-in for lost knowledge. There’s no doubt that content does disappear; studies have found that 30-50 percent of citations in scholarly papers and legal opinions no longer work (a phenomenon known as “link rot”), and even when they work there’s no telling how much they have changed (this is “reference rot,” which the Hiberlink initiative is currently researching the extent of).  There is, of course, a substantial chance that the content still lives somewhere on the web, often in multiple places; but if it is no longer at the path specified by the link, it will be much more difficult to find. To combat this, archivists and cultural heritage institutions aim to preserve the web’s history for later retrieval. The Internet Archive crawls and stores as many websites as possible, while the Archive Team aims primarily to preserve discussion forums and old blogs. Unlike other groups, the Library of Congress saves websites worth saving by manually choosing them, often taking an “aggregate the aggregators” approach and storing large text databases. In each of these cases, groups are establishing an archive that is perhaps less financially motivated than a company’s database, aiming instead to preserve the knowledge and associations within for public benefit.

\subsection{Conclusion}

Hypertext is built on the premise of collapsing traditional, hierarchical categorization schemes, felling the tree and digging to find the rhizome. This information structure certainly has its historical precedents; a reference book, such a dictionary or encyclopedia, is a classic example. Organized alphabetically (which is to say arbitrarily), it is always referring to other words and terms, requiring the dedicated reader to jump from one page to another, following any thread at will. Michael Zimmer connects Diderot’s Encyclopédie and its use of renvois to the hyperlink, noting its ability to subvert hierarchical knowledge distribution and censorship in the process.  Much of this language is echoed by Nelson, Berners-Lee, Paul Otlet and the many early champions of the web, who saw it as a democratizing force leading towards social good.

However, the web’s highest-level platforms now encompass, embed, and contain all other media, a phenomenon that is difficult to see as we users browse one page at a time. While this structure affords certain advantages, there should not be a one-size-fits-all model for experiencing media, communication, and culture. The web provides no built-in way to “zoom out” and see overarching link structures; it does not allow curious users to trace content to its origins; and it is a disorganized mass that various actors have spent a massive amount of time (and often earned a great deal of money) sorting out. As Bowker and Star remind us, each act of sorting has consequences, and that we rely on sites like Google to do it for us, with no obligation of transparency, is a dangerous reality to live in.

The web and the archive’s acts of containment, on every level, likewise have real economic consequences. There has been much lamentation of the demise of the “creative class,” reducing rich and multifarious works to the act of “content creation.” Similarly, there is much trepidation about big data companies that ingest this content and our interactions with it, making billions of dollars in order to grant us access. I would suggest that these trends are (not necessarily caused, but) enabled by the structure of the web itself. Looking to new structures and forms of classification would do well to counteract the containing, homogenizing forces of computation, and the “big words” (data, information, document) that come with it.



% \section{Layers of containment}

% These news events have in common that they highlight the ways in which the web acts as a \emph{containing} force on its material, turning the result into what we call \emph{content}. ``Content'' is a word that is bound to encounter derision, whether from ``content creators,'' information theorists or media scholars. In a recent talk at MIT, Henry Jenkins referenced the word’s derivation from the Latin \emph{contentum}, meaning ``a thing contained.'' Doc Searls frequently criticizes the term for its ties to marketing, implying a one-way web where content is a catchall term for anything that can be packaged, sold, and consumed online.

% Another, perhaps friendlier way to describe content is as a ``link.'' Where content implies a container (containment), a link implies a connection (expansion), promising to break free from the contained information. Looking at the link favorably, if a publisher adds a hyperlink to an article, it purports to show not only erudition (the publisher has read and vetted the content within), but also altruism (the publisher is helping the content creator, and you, the user, reach one another). But here, the link surrounds the content. In effect, it is the original container, adding the first layer of \emph{context} to the content, but diluting its core in the process. In studying the
% origins of the link's structure and the web's infrastructural qualities, we find many ways in which the web's very structure, as well as the creators, indexers, and archivists that work with content,
% acts as a containing and homogenizing force on original creative works.

% \subsection{The URL}

% \subsection{The Link}

% \subsection{The Feed/The Index}

% \section{The Fields}

% The hyperlink on the web consists essentially of three a URL (the ``target'') The URL is treated as a unique identifier and as an atomic unit of information, when it's truly neither.

% \begin{description}
% \item[The URL is not unique] The same article or event listing reappears under dozens of URLs, and any attempts to find a ``canonical URL'' are expensive and inconsistent. Sometimes -- like in the case of a wire service that gets aggregated by several publishers -- there's no singular home for it.

% \item[The URL is not atomic] URLs point to multitudes of resources, or none at all. Links have text, pictures, videos, audio, other links, and annotations on all of the above. They might change depending on who's asking for them and when they're asking. They might give your computer a virus. They can (and often do) cause money to change hands between unseen actors. They can open in a new tab or window, or open your email client.
% \end{description}

% In other words, the \emph{Uniform Resource Locator} is not uniform, nor is it necessarily a resource. The rest of the link (the <a href="..."\> and everything in between) doesn't give many clues as to what's behind it, or what the significance of the connection is. As a software developer, this leaves me scrambling for any clues as to how to define the content and its relationship to other content; aside from clicking the link and visiting the resource, which is slow and expensive at scale, you can't really know what's behind a link. 
