\chapter{Tracing the Links}

In the previous chapters I have increasingly honed in on the publishing archive and the conceptions of journalists and newsmakers of the archival value of their work. Here I will shift more from prescriptive to descriptive, tracing the links themselves and determine what publishers are \emph{actually doing} with their archives already. I will do this by tracing the inlinking that journalists already do within their publications' stories. Hyperlinks are curated windows into archival material, and a possible sign of publishers' historical research and institutional memory at work. By determine which articles journalists are pointing to, we can start to understand how stories become canonized within a media institution, as well as how the links themselves can begin to form new categories of their own.

After a review of journalists' current views of the role of hyperlinking in their practice---as conducted through both qualitative interviews and quantitative link analysis---I will conduct a hyperlink analysis of nine publications via two paradigms. The first is to infer how the institution categorizes its content through the publication's URL structure, and to see if hyperlinking practices differ across categories as well as publications. The second question is to determine whether the hyperlinks that journalists create can begin to form a new paradigm of categorization on its own; are journalists linking across traditional desks and categories, or are their hyperlinks reinforcing existing institutional structures?

\section{Links and journalism}

According to a variety of scholars, hypertextuality is one of the core new affordances of online journalism.\autocite{deuze etc.} It seems to naturally lend itself to journalistic use; Juliette De Maeyer identifies several aspects of traditional journalistic practice that are amplified by the hyperlink, such as providing context and credibility, allowing ``multi-perspectival'' journalism by linking to contradicting sources, initiating connection with other news outlets, and strengthening the process of gatekeeping.

Journalists seem to agree that hyperlinking lends itself to core tenets of journalism. Foust (2009: 161) says ``choosing links to include in your story gets to the very essence of what it means to be a journalist.''\autocite{de_maeyer_} In 1995(?), Poynter's Nora Paul coined the term ``annotative journalism'' to describe judicious use of linking by journalists; she suggested that this might even be a whole new category of newsroom employee.\autocite{paul_1995} Hyperlinks also have a hand in allowing journalists to take a step back from a news event and add context and interpretation, rather than constantly providing facts they've provided before. Instead of rehashing yesterday's news, why not link to it?

While editors will explain that their role often involves changing the syntax or function of hyperlinks in a reporter's work, there is little about the journalistic practice of hyperlinking that has been standardized or codified. Some publications have written standards or internal memos for hyperlinking, but even these have changed in a short amount of time; for instance, major publications like \emph{The Dallas Morning News} and even \emph{National Public Radio}, initially prohibited ``deep linking'' to their content (e.g. linking directly to a story, rather than a website homepage). Such a practice is obsolete in today's web, where homepages are increasingly seen as an afterthought in favor of social media and recommendations in sidebars.

Some publishers have codified linking policies and best practices. In a 2013 article, Mark Coddington summarizes a series of in-depth interviews that track the ``normalization'' of the hyperlink across different publishers and practices. Coddington finds that institutional influences from professional journalism melded with the rules-free political blogosphere to change the standards and practices of linking in recent years, as publishers and blogs ``mutually adapt towards new norms.''\autocite{coddington_normalizing_2013}. He finds that reporters and editors ``overwhelmingly expressed philosophies of openness'' towards link sources, which is a marked change from publishers' original hesitation.\autocite{coddington_building_2012} This can be considered as a product of blogging culture and the changing habits of web users; even down to individuals' web research skills and the design of browsers (for instance, many users are more comfortable navigating simultaneous browser tabs than in the previous decades of the web). It would seem that in embracing links, traditional publishers are borrowing a page from the blogosphere, recognizing that linking is a two-way street, and a form of ``paying it forward'' for longitudinal gain.

However, Coddington also finds that this spirit is not always borne out in the actual links that publishers forge. Traditional publishers still tend to link internally, which he suggests is ``almost anti-conflict, devoid of nearly all of the perspectival distinctiveness and boldness that characterizes the Web's discourse, but that might be perceived as a threat to the norm of journalistic objectivity.''\autocite{coddington_building_2012} Bloggers are more social with their linking, and frame their links in a way that he describes as more ``episodic'' (linking to recent events in an ongoing story) rather than ``thematic'' (linking to provide deeper background, context, or resources). Traditional publishers were limited in their linking by institutional oversight, technology (such as content management systems that make it cumbersome to link), and style guidelines.

Coddington notes that organizations like the New York Times have linking style guides that included ``who should add links and how,'' whereas bloggers' linking practices are free from oversight; however, these style guides and policies are not always strongly enforced, and some journalists found out that their organizations had link policies they were not aware of. Public media like the BBC and PBS use linking to further their own distinct institutional goals as well; the BBC requires at least one outlink per story, while PBS arranges ``a careful balance of opinion through links'' so as to maintain a sense of neutrality.\autocite{coddington_normalizing_2013, bbc_putting_2010}

These examples point to the varied goals for linking, and the divergent practices around it as a result. While digital news media is increasingly embracing the link as a powerful journalistic and storytelling tool, the varied and slow-moving institutional forces keep some of these organizations from fully adopting them. As Karlsson (2014) writes, ``The general impression is that a plateau of hyperlink use has been reached -- a plateau rather lower than the potential.''\autocite{karlsson_hyperlinking_2014}

\subsection{Qualitative or quantitative?}

Researchers seem divided on whether to opt for quantitative or qualitative research about hyperlinking, though when pushed, most would argue for a healthy combination. Generally speaking, the quantitative studies are more frequent; it is easier to mine the links in a publication (seeing what they're doing) than to gain access and interviews with the journalists who are doing the linking (seeing what they're thinking). Many such quantitative analyses thus conclude with the idea that their findings would be well served by supplementing with interviews and newsroom observation.

For Fragoso et al., the success of network analysis, webometrics, and Google's PageRank algorithm have ``favoured macroscopic approaches focusing on the structure and topology of the Web,'' which ``overshadow the fragile conceptualisation of individual hyperlinks they are based on.''\autocite{fragoso_understanding} The success that network analysis has found with hyperlinks makes it difficult to progress from description to interpretation. For Fragoso, the crucial step is to consider the web ``in terms of cascading categories'': considering the web as a medium allows Fragoso to see websites, webpages, and hyperlinks as successive media texts that encompass sub-types of media artifacts themselves.\autocite[193]{fragoso_understanding} This approach treats each layer as a distinct but overlapping media text, closely mapping to the layers of containment that I outline in chapter 2.

Their results also find that in-line links are more likely to function as traditional citations than other types of links. These links are also usually framed in a different way, as their anchor text flows as part of the diegesis of the text rather than existing outside of it as a URL or headline. Few studies have examined hyperlinking from a close-reading perspective---such a study is overdue, as a fruitful avenue for correlating the meaning or function of the hyperlink with its semantics and style. Luzon's ``Scholarly Hyperwriting'' analyzes academic weblogs and finds patterns in the stylistic use of anchor text, some of which could be incorporated as signals into link analyses. A verb-oriented linked phrase like ``X claims that'' or ``X responds'' functions differently from a noun-oriented one, like linking to a ``story,'' ``editorial,'' or ``letter.''\autocite{luzon_scholarly_2009}

But generally speaking, links have too many possible meanings and too little structure to automatically infer a motive. Luzon enumerates eight major types of links on blog pages even before diving into subcategories within them; links to comments pages, permalinks, trackbacks, archival categories, blog reactions, social shares, and so on.\autocite{luzon_scholarly_2009} The anchor text is a limited signal for each of these possibilities, and the many uses of the hyperlink clearly vary across institutional boundaries, whether small blogs, major publications, or public and nonprofit outlets. It seems that quantitative approaches to link analysis carry great promise because they are relatively easy to do at scale, but they do not have enough information to predict with great accuracy.

Juliette De Maeyer's ``overview of link studies'' suggests that while studies of hyperlink networks employ a hodgepodge of methods, ``a unified framework exists. It combines quantitative link counts, qualitative inquiries and valuation of field expertise to support link interpretation.''\autocite[737]{de_maeyer_towards_2013} Such an approach would make logical sense, taking advantage of the scale of link crawling while always approaching this data with a critical eye. But De Maeyer does note a conceptual divide between two major approaches to link analysis, however. The first stems from the network science world, and aims to describe the structure and properties of the network being created, while the second originates from social science, and looks at the ``information side-effect'' of the link as an indicator of other phenomena.

The former, network theory-oriented approach is led by researchers like Albert-L\'{a}zl\'{o} Barab\'{a}si; it relies on complex mathematical methods and models to predict the size and shape of networks. Such an approach takes a high-level view of the web and examines its properties in the context of other networks in the world (such as transportation and power grids, or epidemics). This risks glossing over the idiosyncratic linking practices of a particular community, but brings several helpful frameworks and ideas that journalists and publishers should keep in mind when considering the linking properties of the web. First, network theory finds the web to be a \emph{scale-free} network, and one that follows a \emph{power-law} distribution in terms of connecting pages. In other words, 20\% of the webpages on the internet have 80\% of the links.\autocite{find the ACTUAL number for this} Second, it brings the idea of \emph{preferential attachment}, which suggests that pages that already have links are more likely to acquire more links. The second leads to the first. While network theory addresses ``softer'' influences like modern trends and overall content quality, such a system generally favors older websites, which have had more time to spend acquiring new links.

The latter, social-scientific approach treats hyperlink formation as a proxy for other influence, such as economic and political ties, international communication flow, or prestige and presence. As Park et al. say, ``hyperlinks are not merely connectives between texts but mediators of a wide range of associative relations between producers of Web materials.''\autocite{park_sociology_????} Much research has found a correlation between virtual hyperlink status and real-world network influence, and such research is backed up by Google's PageRank algorithm, which similarly uses links as such a proxy.

PageRank is just one of hundreds of algorithms that Google simultaneously employs to determine credibility and influence on the web (others include the professionalism of the design and markup of the page, or indeed the anchor text of the hyperlinks that link to the page). It is a relatively simple concept; a page's rank is a function of the sum of the rank of all the pages that link to it. But PageRank is not the only paradigm for credibility and influence on the web; other citation-based search rankings also offer alternative methods for graphing the web. One compelling example is Jon Kleinberg's HITS (Hyperlink-Induced Topic Search) algorithm. HITS is a sort of byproduct of the shape of the web as network theory conceives of it; generally speaking, webpages can be divided into ``hubs'' (whose purpose is to link out to other resources) and ``authorities'' (whose purpose is to be linked to as a reference for information). The HITS algorithm computes a separate hub and authority score for each webpage, one which determines the value of its content, and the other which determines the value of its links. Two pages that are linked to by the same hub are said to have ``cocitation,'' a common proxy for topical relevancy; the fewer the links from the hub, or the closer they are to each other on the page, the greater the confidence that the pages are related. In their treatment of the news ecosystem, Weber and Monge (2011) add a third type of information actor to this flow; that of the ``source,'' such as wire services like Reuters and the AP that supply many authorities with the original information. This information is in turn directed from authorities to hubs, as publications like the New York Times feed to aggregators and indexes like Google News and the Huffington Post.\autocite{weber_flow_2011}

Finally, Jeffrey Dean and Monika Henzinger adopted similar methods to create a hyperlink-oriented related page suggestion algorithm. They turned to two algorithms; a cocitation algorithm as outlined above, and a ``companion'' algorithm, which extends Kleinberg's HITS to account for their order on a given page.\autocite{dean_finding_1999} This example, along with the qualitative studies and anchor text analyses considered above, show that determining influence and relevancy on the web is more than a matter of how-many-links, or even who-links-to-whom. It can incorporate the location, text, and styling of the link on the page, and the typical shape of the web, in order to determine what makes a particular link network unique.

Other research considers the ability to automatically add hyperlinks to news stories, either pre-digital ones that are in the process of being digitized, or born-digital stories that could simply use some additional context.\autocite{automatically_embedding_newsworthy} Such systems can recognize entities in text and link them to other entities in the archive; for instance, a system might find the phrase ``The Pittsburgh Steelers defeated the New England Patriots'' and automatically hyperlink the text, pointing to the original article that announced the game result. Outlets like the Washington Post and the New York Times have used such automated hyperlink creation methods before, but Coddington finds that they are phasing them out in most cases.\autocite{coddington_building_2012} This is not to suggest that automated linking carries no promise, but the simplistic and fully automated methods do not seem as effective. Others have proposed ``link apprentices'' that work with journalists to find the relevant links, combining automated search and human curation.\autocite{link apprentice} This approach seems to follow the ``prosthetics'' approach that Stijn Debrouwere advocates (See Chapter 3.2.3).

Still other research has examined readers' responses to hyperlink navigation. Such studies tend to rely on tech-savvy students as test subjects, tempering the conclusions we can draw; but the authors point to an interesting discrepancy in the link's function. Readers tended to prefer links in sidebars of news articles rather than in the anchor text of the article itself; they also preferred seeing additional metadata, like a lede or image, along with it. In this sense, sometimes the hyperlink exists to entice you to click and dive into more information, but other times, the link's metadata is all we need to understand the reference. This points to a need to consider the content being embedded when a link is made; if you embed a title, description, and image, this may be more user-friendly; on the other hand, they may not click on the link.

\subsection{Staying in or going out?}

Most research tends to divide the ``inlink'' and the ``outlink,'' or those links that stay in the same website versus pointing out to others around the web.

Mark Tremayne uses network theory to map external hyperlinks. While he observes that ``news stories on the Web are more heavily linked every year,'' he also notes that the percentage of external links has steadily decreased.\autocite{tremayne_applying_network_theory} This, he suggests, is a result of publishers building up an increasingly robust archive of their own, as they no longer need to link out to provide contextual information. In his words, ``As news sites' digital archives grow\ldots there are more opportunities for linking.''\autocite{tremayne_web_of_context} He calls the siloed publications that result ``gated cybercommunities.'' Tremayne also posits that stories about international affairs might be more heavily linked than stories about less complex topics that readers might already be more familiar with. This idea informs my consideration of links by category in the following section.

However, Tremayne seems to assume that external linking is inherently a good thing, and internal linking is bad. External linking is altruistic and fosters global communication; internal linking is nepotistic, narcissistic, and stifles information flow. Such a conclusion is understandable given the current state of publishing archives; but linking in does not have to be inherently bad. Editors, of course, want to boost their own clicks and time-on-site, but it also allows for \emph{better} context that is hand-curated by one's own institution and brand. The content has no risk of changing, and it adds an additional wrinkle of insight to your publishing archive. Linking to your archive can be beneficial to readers, as long as the archive altruistically links out when needed in turn.

Some links stay on the same \emph{webpage} as well as the same website; such links are operating at a still different level of containment and connection.

In a 2013 paper titled ``Staying in or Going Out?,'' Larsson examines how a handful of Swedish newspapers use internal and external hyperlinks. It finds that the vast majority of external links are found in the text of the article itself, while most internal hyperlinking occurs in the navigational sidebars (think ``Recommended for you'' or ``Most emailed''). He concludes that ``we are mostly seeing what could be labeled an automated approach to employing hyperlinks.''\autocite{larsson_staying_2013}

\section{Link analysis}

With these potentials and limitations in mind, I began considering approaches for conducting a link analysis of my own. As a software developer advocating for reuse of work, I was also interested in building a tool or framework that would facilitate nuanced quantitative and qualitative analyses of linking within and between news websites. Such a tool would allow others to conduct their own link studies, and account for many of the signals and pitfalls of existing studies by storing and indexing detailed data about the links.

% Start with considerations of link analysis

Many hyperlink studies discard internal links, treating them as ``nepotistic'' or otherwise focused on ulterior motives, such as search engine optimization. But I was interested specifically in these links as windows into the archive. How often are publishers linking to articles from weeks ago, or even years ago? How many of them make use of topic pages, and how heavily do they link to them? Where on the page do these links tend to occur, and what sort of anchor text is used on them? What sort of additional metadata or markup travels with the archival link?

Most crucially, are large legacy publishers linking within their own categories, or do they also link across newsroom desks and institutional structures? Addressing this question would begin to suggest whether a link-oriented classification scheme could enhance or replace the traditional categories, topics, and beats that structure news websites. I 

RQ1. How are news publishers using internal, archival hyperlinks within their own website, and how does it vary from the linking practices of bloggers and aggregators?
RQ2. Can these internal, archival links lead to alternative modes of classification and relevancy?

I framed RQ2 in this way because I suspected that the existing models, institutional sturctures, and norms of hyperlink use would reinforce many of the existing categories and perhaps lead to some glaring omissions. Unless reporters and editors are thinking of hyperlinks as units of organization and classification themselves, it is likely that they will be unsatisfactory in providing a complete or nuanced contextual picture of an ongoing event. However, I likewise was curious about any promising glimpses in existing practices; some specific categories of articles, or specific publications, might already be creating useful networks inadvertently simply through judicious use of internal links.

As such, I planned to begin with a larger data set in RQ1, in order to hone in on promising potential cases for a smaller data set for RQ2. While I expected that most articles and publications would not have sufficient internal linking to generate networks, certain categories of news articles or certain types of publications might show more promise. This would roughly correspond to quantitative and more qualitative research approaches, as well as hierarchical versus linked classificational schemes.

Because several studies have noted an increased use of internal hyperlinks over time, as well as a qualitative shift in the approaches and mentalities of publishers towards hyperlinking, I was also interested in gathering longitudinal data about these links, in order to determine whether the number and characteristics of internal linking has changed in recent years. This would allow me to determine whether a change was already happening.

\subsection{Methodology}

I aimed to gather three data sets, which traversed from larger to smaller data and from high-level to low-level analysis frameworks:

- A large sampling of major news websites and blogs, for an aggregate view of linking practices across the field.
- A comprehensive sample of certain publishers, for specific comparative analyses of those publishers' linking practices.
- A few specific, crawler-generated networks of links surrounding a particular article, topic, or event.

To obtain the comprehensive data sets, I used Media Cloud, a project jointly run by Harvard's Berkman Center for Internet \& Society and MIT's Center for Civic Media.\footnote{See mediacloud.org} Media Cloud offers a near-comprehensive repository of news articles from recent years, which can be downloaded and processed programmatically for research use; it also contains a series of ``media sets,'' which broadly define and classify types of media.

For the first set, large sample set, I honed in on two of Media Cloud's media sets: publishers in the Top 25 Mainstream Media, and publishers in the Top 1000 Popular Blogs. Comparing these two sets would allow an understanding of the quantitative differences in linking practices between major publishers on the one hand, and smaller blogs and aggregators on the other (although there is some overlap; Top 25 Mainstream Media includes some digital aggregators, such as the Huffington Post).

For the second set,

For the crawler,

% # April 2012
% # March 2013
% # February 2014
% # January 2015
% 	# Top 25 media (sample week?)
% 	# Top 1000 blogs (sample week?)

% # NYT
% # Guardian
% # Washington Post?

% # Time.com?
% # New Yorker?

% # Vox
% # Huffington Post
% # BuzzFeed? Gawker? FiveThirtyEight?

% # BBC
% # NPR
% # PBS

% # AP
% # Reuters

% Do stories with more links tend to be shorter? e.g. blogs?


\subsection{Link breakdown by category}

\subsection{Link breakdown by graph/network}

% The last chapter dealt with how publishers think about their archives. This section focuses instead on how they're \emph{linking}, through research and quantitative link analysis. It will combine historic news network analyses and a study that I performed with the help of Media Cloud.

% This section will have quantitative analysis of inlinking vs. outlinking, both historical and my own research.
% Maybe call out 1-2 particular cases (NYT? Guardian? etc.) to see how they're linking


%Even once we assume that linking is a fundamental good for journalists, the problem remains of standardizing and developing a language \emph{for} linking, for this new affordance in citation and context provision.

% Anthony Grafton %  % Google books / Google scholar % % This should all probably be in a different place though %

%The digital realm, unbound by physical juxtapositions, can not only add an arbitrary number of links, but it can link \emph{in both directions}. Where paper books can only take a reader back in time, pointing only to its sources, it cannot tell you who used it as a source in turn. This results in what network scientists call a ``directed'' network, rather than a bidirectional one. to adds to these links.

% Jaron Lanier: "You're throwing away all the best parts of a network" (or Weinberger? Barabasi?). "Could not have greater implications"

% Adamic and others have tracked links across blogospheres and social networks. They rarely turn inward to a particular organization and see what they have to say.
% Deep examination of inlinks vs. outlinks. Usually Hyperlink Network Analysis specifically throws out the inlinks. So do search engines; called "nepotistic" links. This on the other hand hones in on them.
% Emphasize that I'm just using the content within the text of the article itself; there is much more. Anne Helmond "Exploring the Boundaries of a Website"; "The News Reads Us"
% Hyperlinking as Gatekeeping -- 2003 study of Timothy McVeigh. "They are unlikely to offer external hyperlinks." So early on it was especially true. Go through that study as an example. 95% of all links were inlinks. Over 96% of these links were from a sidebar! 0.6% from the text itself. Though keep in mind: the web was very very different then. Tabs weren't really a thing so people were more worried about losing the reader or the reader losing their place. Websites were also less dynamic, which allowed for more consistent coding of related article sidebars. As this gets more customized/personalized, it's hard to use these as strong signals for analysis. In 2003 most papers were at the 2nd stage of digital news development.
% Related to sidebar discussion: there's a difference between blogroll and in-post links in blogs. De Maeyer 2013 p. 745
% More than this: Helmond MIT8 2013 -- New York Times pages have dozens of JavaScript trackers going, linked to other websites and services.
% Cyberbalkanization (Sunstein 2001) -- massification -- gatekeeping/walled garden of hyperlinks. "gated cybercommunities" (Tremayne 2005); "walled gardens" (Napoli 2008). Does this also apply in-domain? Can we see rifts among news desks, for instance? Places where they fail to categorize, read, or cite one another?
% Vobic Slovenian newspaper study: even in 2013 many of these orgs are not thinking forward about hypertext
% Does the hyperlink go against certain ad models? If your goal is to encourage clicks to advertisers, adding more hyperlinks to a page theoretically reduces the probability that the user will navigate to an ad.
% BUT: while networks have mostly been studied in terms of outlinks, there have also been many WIF studies that are internal to a given domain, e.g. the Adamic study that looked at communities at Stanford and MIT, or this "Macroscopic Information Web Links" paper that examines impact within a university (inlinks)


% Given these potentials, it might be surprising to find that many publishers are very reticent to link. Those who do have linking policies are often quite conservative.

% % OVERVIEW OF LINKING POLICIES: NYT, BBC, Guardian, etc. % % Reference to next section where I explain more what they're doing %

% Some of this is due to SEO; while no one knows exactly what Google's mercurial PageRank algorithm is doing, it's clear that links form a fundamental component (and as critics such as Clay Shirky have argued, relying on links rather than traditional categories and tags has been the crux of their success over competitors).\autocite{} Publishers are also wary of taking a reader away from their own website. But I'm also sure that much of the fear of links comes from inertia and tradition; since journalists never used to have a way to link, some don't see a need to start.

% While many have debated the potentials and pitfalls of hyperlinking the news, I am proposing an additional wrinkle to the conversation; a smart use of linking can be, to borrow Derrida's term, a \emph{pledge} to better structure the news and keep archives continuously animated and relevant.


% \section{Mapping Links}

% There is a lot of research on automatically mapping hyperlink networks; crawling, clustering, classifying utilizing a variety of machine learning techniques. However, most of these studies are interested in the ecosystem of content creators, and the ways in which links form across domains. My focus is instead on \emph{internal} links between stories. This is not only a more manageable set, but it is also allows opportunities for rich restructuring of the data schema. Chakrabarti says: "I believe that the Web will always remain adventurous enough that its most interesting and timely content can never be shoehorned into rigid schemata." This may be true, but internally, news publishers can use ankle-deep semantics to greatly improve their structures.

% Some webometrics endeavors have aimed to measure internal links within businesses, universities, etc. Others have done much work with internal clickstreams in order to gain insight about website usability. But nothing has looked at internal links of news stories.

% Some theory about how to do it. Talk to César? Reference De Maeyer's piece about it.
% Mapping, but also visualizing
% End by explaining my methodology

% De Maeyer "critical review of link studies". There are many limitations to webometrics. For instance, over-reliance on the information in the URL domain name and path, which we all agreed was not a uniform, consistent resource. This led to, e.g. Tuvalu being among the main hubs of developed countries in hyperlink network, because they sell their .tv domain.
% Maps of the blogosphere were popular in Web 2.0, they were "a link-driven genre" (De Maeyer 2013); but now we're in the era of platforms, how has that changed the link structure of the web?
% Also, a case against straight statistics and confidence intervals: "Links are not statistically independent observations, ‘because of factors such as imitation between web authors (…), the copying of pages or parts of pages, and automatic creation of web pages by server software’ (Thelwall, 2006: 7). According to Thelwall, this means that inferential statistics, such as confidence intervals, are not valid. As Harries et al. (2004) phrase it, web link creation is a ‘social activity that inspires imitation, the opposite of statistical independence’ (Harries et al., 2004: 439)."

% The goal, then: "semi-automatic methods". Most projects "rely on some manual coding, qualitative appreciations and human expertise." The main question is "when do we stop to purely describe the structure of links as technical objects and when do we start to relevantly exploit links to make sense of a social phenomenon?" But ultimately "mixed methods appropriately allow using hyperlinks as proxies for other phenomena." --  "combining quantitative link counts, qualitative inquiries and valuation of field expertise to support link interpretation."

% Fragoso 2011 tries to holistically combine the quantiative/qualitative, macro/micro study patterns that De Maeyer observes: "Web Science has favoured macroscopic approaches which have revealed much about the Web’s structural patterns. We argue that contextualised knowledge about hyperlinks on the Web has not advanced at the same rate and that complementary intermediate and micro-scale investigations are essential for a better understanding of the motivations, functions and meanings of these links."

% Could combine Media Cloud aggregate ``big data'' view with Times Newswire API (e.g. 1 month, ~9k stories) for deeper view into one publication. Deeper view could be a) anchor text, b) where the link is in relation to the top of the article, named entities, etc.

% \section{Automated NER and linked data potential}

% Can we get richer info automatically out of the entities referenced in newsrooms?

% Can we form networks out of the links that newspapers are making? Clusters, in network terms? How are they improvements on classifications, in a more traditional sense? a) they don't need to take as much account of the media type of the resource, right? Text, image, video, etc. are all treated semi-equally.

% Visualization of hyperlink networks: De Maeyer "Methods for Mapping Hyperlink Networks", Carriere/Kazman. De Maeyer "The web lacks geography," it's flat
% ``hyperlink maps are thus helpful in visualizing phenomena that would have been otherwise hard to detect: "the reason for performing such an analysis is to reveal latent structures that are not already obvious to an observer''


