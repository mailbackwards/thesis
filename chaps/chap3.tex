\chapter{Networking the News}

In the previous chapters, I have outlined the ways that archives, and critical readings of them, have expanded from a fixed and graspable entity to a suite of interconnected parts, constantly shifting and adapting to new information. The web, when seen as an archive of archives, is itself ``an active and evolving repository of knowledge,'' rather than a fixed, bordered entity or set of categories.\autocite[2]{chakrabarti_mining_2003} This chapter hones in specifically on the structure of news stories and publishing archives, and the ways online publishers and legacy news outlets are treating their digital and digitized archives in this new era of continuous reclassification.

In the publishing world, 2014 saw two simultaneous trends that point to a fundamental shift in the function of mainstream news on the web, and a corresponding reformulation of the roles and practices of journalists. In the digital-publishing sphere, some new publishers began to champion an ``explainer'' model of journalism; with headlines like ``Everything you need to know about the government shutdown'' or ``40 charts that explain money in politics,'' the explainer model suggests that newsrooms can be ``as good at explaining the world as it is at reporting on it.''\autocite{klein_vox_2014} At the same time, legacy publishers have led a new focus on renewing and reanimating their historical archives; whether they're cherry-picking old curiosities and republishing them, providing subscribers with an interface to dive in and leaf through, or leading crowdsourced projects aimed at organizing and structuring old content, legacy media has jumped at the chance to offer something that their born-digital rivals can't: a rich sense of their brand's history, and a new take providing context and resurfacing the past.

These two trends reflect a seismic shift in the online media landscape, one which has seen journalists adapt by amplifying their role as explainer, verifier, and context provider rather than news breaker or scooper. The newsrooms that employ these journalists must adapt in turn; as journalism on the web serves a different function than its pre-online counterpart, publishers need to recognize that their function as a commercial product \emph{and} public service has fundamentally changed. Publishers have a newfound opportunity to thrive in the information industry as much as the news or publishing industry, but in order to compete in an information landscape currently dominated by Silicon Valley and Wikipedia, this new mentality cannot be simply verbal or piecemeal. It requires updated technologies \emph{as well as} cultural change.

As such, this chapter is divided into two sections, one cultural and the other technical. The cultural section aims to outline the telling origins of archive-oriented and explainer journalism, emphasizing that the rapid proliferation of new content and connections have fundamentally altered journalism's roles and practices. The second section will consider the ways that publishers can adopt a technical infrastructure to support these new roles, first by analyzing in detail the architecture of a digtal news story, then offering frameworks, tools, and technologies that might help to link archives and structure stories for future archival value.

\section{Context in context}

In newsrooms, the archive is traditionally known as ``the morgue'': a place where stories go to die. But new technologies and conditions have led to many recent attempts to reanimate the news archive, and there seems to be an ``archive fever'' developing amongst news publishers. Nicole Levy wondered if 2014 is ``the year of the legacy media archive'' in a story about \emph{Time} magazine's archival ``Vault.''\autocite{levy_time.com_2014} She points to \emph{The Nation}'s ``back issues,'' \emph{The New Yorker}'s open archive collections, and the \emph{New York Times}' TimesMachine and @NYTArchives Twitter account as examples of old publishers endeavoring to use their rich histories to create something new. Back archives like \emph{Harper's} and \emph{National Geographic} are held up as examples of combining rich content with historical context, improving credibility and brand recognition in the process.

\emph{The Times} closely examined its own archives in their celebrated \emph{Innovation} report of 2014, suggesting that a clever use of archives could revitalize new content by seamlessly integrating with historical context, allowing \emph{The Times} to be ``both a daily newsletter and a library.''\autocite[28]{_innovation_2014} The report suggests that arts and culture content, more likely to be evergreen, could be organized ``more by relevance than by publication date,'' and that topic homepages should be more like guides than wires.\autocite[29-30]{_innovation_2014} The report goes on to enumerate successful experiments with repackaging old content in collections, organized by categories and themes. They suggest allowing users to create their own collections of stories---something that readers could also do without risk to the \emph{Times} brand. By creating ``no new articles, only new packaging,'' the \emph{Times} can easily give new life to old content.\autocite[34]{_innovation_2014}

In 2014 we also saw the rise of ``explainer journalism,'' and an intense focus on context provision for readers. Vox.com, the poster child for the explainer movement, wants ``to create a site that's as good at explaining the world as it is at reporting on it.''\autocite{klein_vox_2014} Explainer journalism aims to take a step back from the immediate news event and place it in a larger phenomenon. It reflects a deep shift in the roles and practices of online journalists: as news is increasingly broken and scooped on social media, journalists are increasingly becoming summarizers, filterers, and context providers. News has traditionally been delivered in a stream format, full of boilerplate text that is repeated across every story related to a given theme. In the archive and explainer movements, we see a pattern among some news outlets attempting to evade and reconsider the news cycle's obsession with speed and feeds, instead experimenting with new forms of what a news story can be, and how it can connect to other stories within the archive and around the web.

As many publishers emphasize the potential value of archives and context for the future of digital journalism, this moment is rich for closely examining this connection. By comparing the challenges of legacy media archives and newer forms of explainer journalism, we can gain a sense not only of how print and digital media differ as objects and media texts, but also of how journalistic practice has changed across both sectors.

\subsection{The archive drive}

Legacy media's intense focus on archives is a reflection of many uncertainties about the future of journalism. After all, it seems antithetical for news to focus on their history; news has forever been focused on the \emph{new} and \emph{now}. The pace of \emph{now} has only accelerated online, as newsrooms shift from daily or weekly print schedules to continuous publishing and posting. For a reporter, this creates a work environment of real-time frenzy; a reporter might have live statistics on her article's performance in one window, while she furiously scans a flurry of tweets looking for her next scoop in another. It's no wonder that newsrooms haven't traditionally focused on their archives; for today's reporter, the further back in the past, the lower the value.

% throw in "ambient journalism" somewhere?

But this everyday practice---manifested in real-time feeds, streams, and notifications on Twitter, Reddit, or RSS---is at odds with the plethora of old stories, images, facts, and quotes that are perpetually accessible on a publisher's website. The past and present collide in the digital news database, as seemingly fixed, petrified stories accumulate new metadata: new clicks from users, new links back from newer content, new topics and tags when the publication revamps its taxonomy. Some content is resurfaced, while other content is deleted forever. This is a sort of paradox, where the archive houses and contains the object but it is continuously forging and breaking connections, blending the past and present. This paradox emphasizes the continual relevance of the archive to the present, and the need to continuously integrate new news with the old; while the \emph{Times} asserts that ``our rich archive offers one of our clearest advantages over new competitors,'' they have done little to mine its value, ``largely because we are so focused on news and new features.''\autocite[28]{_innovation_2014}

The web's archival affordances are crucial for media scholar Peter Dahlgren, who sees archivality as one of the five components of the ``media logic'' of cyberspace. For Dahlgren, archivality forms a symbiosis with hypertextuality---another web logic---which enables new and more usable archives. The end result, he asserts, is that ``users of cyberspace for journalism are in principle no longer so bound to the present.''\autocite[66]{dahlgren_media_1996}\footnote{Mark Deuze only names three ``logics''---hypertextuality, multimediality, and interactivity---but Dahlgren adds ``figurational'' and ``archival.''} Even as reporters are focused on \emph{right now}, their users might be more interested in \emph{back then}.

Still, this is not necessarily borne out in publishers' insights about their readers. Their language around the value of archives is telling; most of them are sure that archives are valuable, but they have different answers---and sometimes no answer---for exactly \emph{why}. Moreover, the answer could be different for each publisher. While the \emph{Times} asserts a competitive advantage over born-digital upstarts like Vox and BuzzFeed, most do not go beyond suggesting that it is a unique and rights-free repository of content.

The University of Missouri's Reynolds Journalism Institute offers that ``It is difficult to calculate the full value of news archives given the countless hours of reporting and editing they distill, not to mention the treasure they represent in terms of their community's cultural heritage.''\autocite{mccain_saving_2014} They also suggest that archives ``serve as a form of institutional knowledge,'' allowing new journalists to get up to speed on the historical context of an ongoing story. This is especially crucial for new hires for local or specialized beats, where these newspapers often gather a repository of knowledge that is unmatched for a certain community or genre.

The Reynolds Institute led a 2014 survey of 476 news websites, finding that 88--93 percent of them highly value their archives. But the origins of this study are telling; the school's \emph{Columbia Missourian} paper lost 15 years of stories and seven years of images in a single server crash, and it was only after this permanent and devastating loss that they recognized its full value. Despite the nearly universal lip service paid to archives, about a quarter of news websites had lost significant portions of their archive due to technical failure. Missouri journalism professor Tom Warhover provides a series of examples to emphasize the devastation: ``You can't offer up a comprehensive product to sell---your archives---if they aren't complete. You can't be sure you've really vetted a candidate for school board or city council. You can't find those historical pieces from events that now are historic and worth reporting again on anniversaries.''\autocite{mccain_saving_2014} These examples showcase the latent and often hidden value of the archive, not only as a product but as a deep journalistic resource and form of institutional memory.

Due to the amorphous and ambiguous value of news archives, it seems to go without saying that archives should be saved, but it's harder to know exactly what to do with them. Saving them turns out to be the expensive part; the cost of digitizing, indexing, and hosting gigabytes of old content is no small investment. Some publishers stop here: they might offer a search interface for deep-dives by journalists, researchers, and obsessively curious readers; many of them also offer a PDF-style flipbook view that allows readers (usually paying subscribers) to go through the ``original'' paper or magazine. These interfaces will serve the power-users with a research interest or innate curiosity, but this only scratches the surface of the archive's potential as a serendipitous window into past insights for journalists and casual readers alike.

Imagine navigating to a topic page, such as one of \emph{The New York Times'} over 5000 pages ranging from ``A.C. Milan'' to ``Zimbabwe,'' and seeing a map or timeline of the most important stories in \emph{Times'} history. What about viewing a network of stories as a graph of influence and dialogue, or pulling out the most relevant quotes, images, or facts from the story to highlight for the user? If the \emph{Times} hasn't written about A.C. Milan in a while, they could suggest a story about a rival soccer team, or Italian football clubs in general. Like a good search interface, a good topic page should function like a research librarian, retrieving the information and context---not just the stories and items---that you want; and if the information is not at hand, they can point the reader in another direction.

Such a future requires smart use of indexing and metadata schemes, which I will consider in the following section; but it also requires a cultural shift in both the conception of the archive's audience and the ways that journalists think of their stories' value. For now, legacy publishers seem to know well that their archives are a competitive advantage, and an asset that sets them apart from born-digital upstarts. Indeed, it seems ironic that publishers with no history are the ones that emphasize context. For \emph{The Nation}'s editor and publisher Katrina vanden Heuvel, ``a clever use of archives is kind of an explainer 2.0,'' directly placing the two movements in comparison and competition.\autocite{levy_time.com_2014} In the following section, I will look at explainer journalism as a symptom and reflection of new forms of value. But I also might bring up another paradox of archival innovations, one that does not burden the explainer movement: in aiming to revitalize the archive and blend the present with the past, archive-oriented publishers might be inadvertently bringing past conventions, petrified by the archive, back into the present.

\subsection{Explaining the news}

The \emph{Times'} \emph{Innovation} report specifically names Vox.com as one of its direct competitors, one of the new digital upstarts that is offering new models of news production and threatening the reign of legacy media. This was an auspicious start; at the time of the report, Vox had not even launched. It did have a few big names behind it already, namely founder Ezra Klein, who left his position at the helm of the \emph{Washington Post}'s Wonkblog to start this digital outlet.

Vox has quickly become a darling of digital start-up news, with eight-figure investments and nine-figure valuations of its parent company, Vox Media, less than a year after launch. Vox.com has also become the poster child of the explainer movement, as it aims to infuse context and deep explanation into its journalism, technology, and business decisions alike. Its signature feature is its ``card stacks'': collections of reusable and editable snippets of facts, quotes, and other media, collected and synthesized by a journalist. With titles like ``Everything you need to know about marijuana legalization'' or ``9 facts about the Eurozone crisis,'' the stacks subdivide into question-driven ``cards,'' such as ``What is marijuana decriminalization?'' Readers can navigate sequentially, or dive from question to question. The final option on each card is the same: the ``Explore'' button takes the reader back to the top of the stacks.

The purpose of the card stacks is similar to many legacy institutions' reasons for protecting their archive: to avoid repeating work. For Klein, ``the biggest source of waste is everything the journalist has written before today.''\autocite{} Through the card stacks, we see a slow accumulation of a new kind of publishing archive, one oriented around questions and answers rather than topics and tags. While such background work is highly useful for journalists, leveraging the value of work done by Vox reporters before them, that the card stacks are seen as not only a public-facing but a \emph{core} feature of the Vox.com product is interesting. Rarely have news organizations considered a story \emph{structure} (a container) as part of their core product; their core product is usually the stories themselves (the content). This change in technology and business leads to a corresponding shift in the role of the journalists, who explain and contextualize as journalists always have, but with an eye towards updating the stacks with any new information. 

Klein has Wikipedia in his sights, suggesting in a \emph{New Yorker} interview that ``I think it's weird that the news cedes so much ground to Wikipedia. That isn't true in other informational sectors.''\autocite{} Wikipedia is the looming giant in context provision; publishers rightly ask how they could do any better without unlimited staff and budget. But Wikipedia is a generalist's resource, and its readings on more specialized and newsworthy topics can be dense and dry. Some topics, especially in science or medecine, assume a reader's technical knowledge from the outset, and require deep clicking on Wikipedia links to reach the source. Linking to Wikipedia also, of course, takes the reader away from the site. Vox's card stacks are a sort of journalistic alternative to Wikipedia, a wiki ``written by one person with a little attitude,'' as co-founder Melissa Bell puts it.\autocite{} In theory, the cards will only get better; better structured, clearer, better written.

Some are bearish on explainers, suggesting that newsreaders are looking for news, not information and context. Klein instead envisions a virtuous cycle of curiosity and information: ``the card stacks add value to the news coverage. And the news coverage creates curiosity that leads people to the card stacks.''\autocite{} The symbiosis between news coverage and cards is clear for journalists who need to quickly fact-check and contextualize, but the magical formula of news coverage that ``creates curiosity'' for a reader is a massive challenge.

Others think that Vox is diving too deep into the information space; given that even teams of trained librarians struggle to organize information, how can a team of journalists hope to keep up? They have ``a huge challenge, due to the rapid decay of facts,'' according to a \emph{Nieman Journalism Lab} article by Craig Silverman, who suggested that they could use a librarian.\autocite{} In order to be feasible, each card needs to be both independent and linked; the cards are adoptable as standalone pieces, but they also have to add up to a coherent Q\&A-style narrative for linear reading. Consider, for instance, Vox's card on marijuana legalization, which has already been updated 40 times between April 2014 and February 2015.\autocite{} Scientific research on marijuana's health effects is known to be limited; what happens to these preliminary studies when they are replaced by more rigorous ones? How will a journalist be alerted when an obscure research paper debunks a fact or figure? What happens to such a card if marijuana becomes legalized across all 50 states?

But Vox doesn't say their cards contain everything, just ``everything you need to know.'' This economical, glib approach to contextualizing might be the primary determinant of Vox's success. Some critics have lambasted the site for the hubris of its mission; it doesn't get much bigger than ``explaining the news,'' and it might be best left to a distributed group of subject-matter experts (like an encyclopedia) rather than a core of smart but time-strapped journalists, who are often distant from the topics that they cover. As a result, Vox does make mistakes; Deadspin gleefully and sometimes unfairly picked apart a full 46 Vox corrections from its first year.\autocite{} Others question the site's tone; music journalist David Holmes, suggested that Vox's model gives the reader ``just enough information\ldots to \emph{sound} smart,''likening their approach as appealing to ``the type of person who's afraid of sounding gauche at your next dinner party.''\autocite{} Indeed, ``everything you need to know'' sounds final, running counter to the very idea that knowledge is interlinked and flexible; everything \emph{who} needs to know? Vox's style of explainer journalism adopts a somewhat cold and distant veneer from the topic; it's more likely to explain why people are excited about a new movie than to get \emph{you} excited about it. This might appear to be context, but sometimes it's merely distance.

\subsection{Explaining the news part 2}

Vox is far from the first or only explainer, and the concept of explaining the news is, in many ways, a core journalistic principle. A 2001 Pew Center survey of newspaper editors concluded that they wanted to be ``news explainers'' first and foremost, ahead of ``news breakers'' or ``investigative watchdogs.''\autocite{} But in a 2008 article called ``National Explainer,'' Jay Rosen accused editors of not staying true to their mission: journalists are not good at explaining the news and providing context.\autocite{} Instead, they focus too much on incremental and episodic updates, many of which go over the head of readers who haven't been following. Rosen likens the process to pushing software updates to a computer that doesn't have the software installed.

Journalists ``don’t do a very good job of talking about the beginning and what got us to this point where it became news,'' according to Alex Blumburg of \emph{This American Life}. Even the occasional explainer that gets it right ends up in the flow of the same old information; Rosen argues that explainers like David Leonhardt's credit crisis piece in the New York Times ``should have been a tool in the sidebar of every news story the Times did about the mortgage mess.'' The little ``what's this?'' link that pops up on occasional news websites is ``not about web design. That's a whole new category in journalism that I fear we do not understand at all.''

Rosen went on to create explainthis.org, a site for people to admit what they don't know; journalists, the site promises, are ``standing by.''\autocite{} Now defunct, explainthis.org was like a library reference desk, staffed by the public and monitored by journalists. A peer of StackOverflow and ancestor to Quora, it is organized around questions rather than topics, discussed by the public and monitored by journalists. It requires someone to be curious enough to ask the question, however. Rosen touts the ability of explainers to generate interest in a topic, but here we're already expected to be interested.

At a 2010 South by Southwest conference panel called ``Future of Context,'' Rosen outlined the reasons explanation is needed and why it wasn't taking off. He cited both design and institutional problems; the prestige and real-time excitement of breaking (rather than explaining) news, as well as the explainer format getting lost in the shuffle of other news items.\autocite{rosen_2010} Metrics like clicking, watching, and even spending time on a site are not measuring the level of understanding or knowledge gained.

Panelists like NPR's Matt Thompson and Apture CEO Tristan Harris turned more towards the systems and technologies that compound the problem. Harris suggests an ``object-oriented journalism'' model that adopts ideas from computer science; ``think like an engineer,'' never do work that you can't reuse. Thompson considered the potentials of topic pages when contextually linked, and a sort of ``context-oriented'' website. Suggesting that publishers ``ghettoize'' topic pages and pay them no attention; any time a user encounters a random collection of links on a so-called topics page, ``the quest for context everywhere is set back.'' What would a site look like if it were structured around systems instead of stories?

Vox is not a complete shift into this territory, but it's a start. The card stacks might initiate a subtle shift in the ways that journalists think about the value of their content. A reporter who is writing about a new study on marijuana could do so by editing an existing card, or by writing a new card; this card could also be published to Vox as a standalone story. On one hand, this is a journalist writing a story as usual; but on the other, there is a subtle shift in the journalist's knowledge that the story is here to stay.

Vox has also experimented with another simpler but novel use of archives: republishing evergreen stories.


% Where do I talk about what JOURNALISTS ACTUALLY DO
% Coddington, Amy Gahran, stuff from class; journalists as context providers etc.






Some of this content is ``evergreen,'' with the potential to stay relevant to interested readers for years; consider a longform piece on a president's legacy, or a story that maps all of the bars or concert venues in a neighborhood. Other content is more ephemeral, like last night's sports scores. But in a database, each of these is simply called a ``story,'' and most publishers have not done a great job of differentiating the two. This has made it difficult to resurface the right stories for the right readers.

Even the sports scores can be useful to some. Old newspapers are rich archival documents for historians and scholars, because they store both ephemera and history. Journalists sometimes divide these types of news into ``stock'' and ``flow''; the constant stream of information built for \emph{right now}, versus the durable, evergreen stuff, built to stand the test of time.\autocite{sloan_stock_2010} Newspapers also have advertisements, classifieds, stock quotes, and weather diagrams. Many researchers rely on such ephemera---James Mussell calls it ``a key instrument of cultural memory''---so from the traditional archivist's perspective, everything needs to be considered ``stock,'' stored forever.\autocite{mussell_passing_2012} But historians might treat or navigate through ephemera differently, and each set of documents could have its own metadata or interface as a result. Casual browsers, rather than hardened researchers, could be after a still different history requiring a different interface.






\section{Stages of digital history}

% What I don't get into: what is the AUDIENCE for this archive? Most archival research has looked at its role for researchers; but what about casual interested browsers?

In 1997, John Pavlik suggested that there were three stages of development in online versions of newspapers. Keeping in mind this early date, Pavlik observed that newspapers' first stage online was to copy the print edition to a given website (a stage known and maligned as ``shovelware''), followed by supplementing the copy with interactive features (like hyperlinks or comments), then in the third and final stage, writing copy specifically for the online version of the story.\autocite{pavlik_future_1997} Twenty years later, it is safe to say that publishers have all reached the final stage. I aim to suggest a three-stage development of my own in the state of the digital legacy news archive, referring to each stage respectively as \emph{digitizing}, \emph{atomizing}, and \emph{linking}.

The first stage for any legacy publisher is to \textbf{\emph{digitize}} the archive. This tends to consist of scanning the pages of old publications, running OCR (optical character recognition) on each page, and exposing the results to a search interface for researchers and, perhaps, interested readers. It is a crucial first step for enlivening the archive, but a physical record can often limit the digital equivalent's potentials. Digital versions of physical articles often do not leverage links, images, and mixed media to the same effect. While a digital-native version of a print article might directly cite more sources or feature an intriguing interactive, these elements remain second-class citizens to the print article, which digital versions must remain faithful to. The Times' \emph{Innovation} Report argues that by modeling their website and apps on their print structure, the Times ``ask[s] too much of readers.''\autocite[26]{_innovation_2014} So it is crucial to remember at this stage that the digital archive has different potential from its physical counterpart. As many theorists and historians remind us, too, a paper's physical appearance and content are closely linked together, so simply ``digitizing'' and newspaper changes it massively, reshaping a great deal of context.\autocite[388-389]{mussell_elemental_2014,manoff_archive_2010} Richard Abel breaks down the promises and challenges in generating archival ``big data'' in research on 1910 US cinema. Using digital newspapers led to ``a wealth of unexpected documents,'' but he notes the unreliability of completeness and searchability, and the collapse of community.\autocite{abel_pleasures_2013}


Given the print newspaper's proto-hypertextual status, it presents a unique metadata challenge for archivists. Paul Gooding, a researcher at University College London, sees digitized newspapers as ripe for analysis due to their irregular size and their seriality.\autocite{gooding_exploring_2014} In order to learn more about how people use digitized newspaper archives, Gooding analyzed user web logs from Welsh Newspapers Online, a newspaper portal maintained by the National Library of Wales, hoping to gain insight from users' behavior. He found that most researchers were not closely reading the newspapers page by page, but instead searching and browsing at a high level before diving into particular pages. He sees this behavior as an accelerated version of the way people browse through physical archives---when faced with boxes of archived newspapers, most researchers do not flip through pages, but instead skip through reams of them before delving in. So while digital newspapers do not replace the physical archive, they do mostly mimic the physical experience of diving into an archive. Still, something is lost when the physical copy becomes digital; the grain of history---the old rip, annotation, or coffee stain---is reduced to information.

Some publishers have thrown up their hands altogether, relying on third-party services to organize and provide access to their own archives.\autocite{romenesko_u.s._2014} Reporters might suddenly see old stories disappeared, locked away behind services like LexisNexis. Such services provide fast and effective text search at low cost; but at what cost to an organization's brand, legal rights, and sense of history? A digital story is not just text, and increasingly, an archive includes images, videos, charts, maps, interactives, facts, statistics, quotations, comments, and annotations. Newer forms of classification can take a more holistic view of media, allowing a researcher to browse through text, image, sound, and video alike, and minimize the language limitations of search. This will become increasingly important as media evolves in a ``post-text'' web; the next generation of media companies cannot rely alone on text search to access their past.\autocite{salmon_why_2014}

The second stage is to \textbf{\emph{atomize}} the archive, or to break these scanned pages into their consituent parts. But what metadata is worth saving and breaking down: the text, the subtext, the pictures? The photo or pullquote on the side? Is the image in the center of the page associated with the article on the left, the right, or both?

A newspaper is a very complex design object with specific archival affordances; their irregular size, seriality, and great care in page placement make them ripe for unique forms of automated analysis. For some researchers, placement will be important (was an article's headline on the first page? Above or below the fold? Was there an image, or a counterpoint article next to it?). Others could be examining the newspaper itself over time, rather than the contents within (for instance, did a paper's writing style or ad placement change over the course of a decade?) Still others may be hoping to deep-dive into a particular story across various journals. In each case, we can glean information from where and when it was published on the page.

The project of atomizing the archive should take advantage of the signals built into newspapers in the first place, accumulating metadata from its size, shape, and context. An atomized archive should also provide a solid interface for viewing the original in its context. When legacy news publishers refer to a ``linked'' record in a database, they are referring to this ability to click on it and see the original, scanned source page, usually as PDF. Some publishers do not even have linked records for their entire archive, which makes context difficult to grasp for interested researchers.

It is telling that many news digitization projects, begun decades ago, focused exclusively on salvaging the text. This ignores substantial information in the archive, of course, and speaks to the shortsightedness of many projects aimed at digitizing the past. Images, advertisements, maps, formatting, and related metadata were all lost, and many of them are being re-scanned by publishers, at great expense, in order to properly atomize the archive and capture the details that they ignored years ago. Nicole Maurantonio criticizes old newspapers for ignoring the visual in favor of text, ``propelling scholars down a misguided path.''\autocite[90]{maurantonio_archiving_2014} Keith Greenwood finds that many newspapers diligently archived their photographs for daily newspaper use, but did not tag items with public historical value in mind, rendering many of them useless as historical records.\autocite{greenwood_digital_2011}

Historical images are one of the greatest potential sources of engagement and revenue for news archives, and it would be relatively easy for some news archives to sell old photographs with historic value.\footnote{someone's gotta be doing this already, find them} Some metadata projects in the publishing world are aiming specifically at images, like the New York Times' \emph{Madison} project, which hopes to crowdsource insight about 1950s \emph{Times} advertisements.\footnote{Madison can be found at http://madison.nytimes.com/.} Outside the publishing sphere, Kalev Leetaru took an image-centric approach to the Internet Archive. The Internet Archive's OCR software threw out images, and Leetaru's would save whatever it threw out as an image file. He has since put 2.6 million of these Internet Archive images onto Flickr for open use. ``They have been focusing on the books as a collection of words,'' he told the BBC. ``This inverts that.''\autocite{kelion_millions_2014} Newspaper and journal images provide a richer glimpse of history, and one that might prove more engaging to digital readers than dated text. A photograph reveals a contingent sense of the visual language and associations of the time; as any visual critic or cultural studies scholar can tell you, photos and advertisements provide a revealing window into culture and history.\autocite{sontag_photography_1977,barthes_rhetoric_1978}

The final stage is to \textbf{\emph{link}} the archive, which when considered on a massive scale, is a quixotic endeavor along the lines of the Radiated Library or Project Xanadu. We can't predict all possible links between every possible piece of content. But linking the archive requires learning from the explicit and implicit references that already reside in the stories. This combines manual and automatic means, supplementing dedicated search in the surfacing of archival content, and using a ``push'' rather than ``pull'' method for finding archival materials. A user doesn't always know exactly what he or she wants, and a linked archive can work with a user to surface it. If the archive doesn't have the resource a user needs, could it at least point the user in the right direction? Could it interface with other knowledge bases to retrieve the answer?

The linked archive borrows from, but is distinct from the notion of ``link journalism'' or ``networked journalism.'' As a term popularized by Jeff Jarvis to refer to the growing citizen journalism movement, link journalism has also led to Jarvis's succinct motto of ``Cover what you do best, link to the rest.''\autocite{jarvis_new_2007,jarvis_networked_2006} Building on this idea, Charlie Beckett posits that linking between sources leads to editorial diversity, connectivity and interactivity, and relevance.\autocite{beckett_editorial_2010} A linked archive turns the conversation inward---as Mark Deuze and others note, inlinks are vastly different from those that point out---but they can adhere to the same principles of diversity, connectivity, and relevance. While inlinking may seem nepotistic and selfish, this is not the case if the archive itself links out in turn.

It is unhelpful to have a massive, borderless archive, but linked archives can expand their borders strategically through clever use of APIs. As Anne Helmond's ``Boundaries of a website'' and the Open Knowledge Foundation's ``The News Reads Us'' project remind us, publishing websites rarely operate alone; they rely on a plethora of third-party platforms and services for analytics, sharing, commenting, and recommendations.\autocite{helmond_exploring_2013,wehrmeyer_news_????} One could similarly integrate with APIs that offer archival resources from around the web. If a user is searching a publisher's website instead of Google's, it's because she wants more context than a mere list or index of items. She wants less containment and more connection. A user should be able to see response articles, comments, tweets, timelines, images and videos, from around the web (as long as these are visually separate from the main content to avoid confusion). Otherwise, users will continue to go to Google and Wikipedia for information.

A publisher's archival search interface could include results from Google, Wikipedia, Creative Commons images from Flickr, or resources from digital libraries like Europaeana and the Digital Public Library of America---not to mention partnering with other organizations to merge archives or indices. These could be different, and differently useful, for reporters, researchers, and readers alike. The linked archive is therefore intricately indexed on a small scale, but also effectively connected on a large scale, seamlessly interfacing with other archives and collections around the web.

\section{The structure of stories}

% Hypertext as semi-structured; APIs fully structured

% Also look at Skye Doherty's foray into design of hypernarratives in journalism

Newspaper and magazine publishers prove an ideal study for examining the potentials of hypertext archives. If we treat a newspaper as a proto-hypertextual document, it becomes apparent that online news might be a natural extension of reading the newspaper. Few readers go through a newspaper sequentially, paying equal attention to every article; instead the reader jumps around from page to page, skimming some sections for its raw information while reading longer pieces more deeply. A website homepage reads like a newspaper's front page, with snippets and teasers that aim to draw the reader deeper. A given page can hold several articles, and an interested reader might be distracted or intrigued by a ``related article'' next to the one he or she came to read. Some works are categorized into sections---arts, sports, letters to the editor---while others might be paired with a certain advertisement or reaction article. These examples point to the inherently interlinked nature of newspapers, and the endless potential for insightful metadata; newspapers might seem to naturally lend themselves to the digital world.

The pre-hypertextual newspaper started as a response to a sort of historical information overload; the newspaper frontpage and summary lead paragraph, both solidified in 1870, were part of a broader trend towards ``helping readers to economize their scarce time in scanning a paper.''\autocite[254]{starr_creation_2004} Larger type, illustrations, and bolder headlines drew criticism for trying to grab attention, but they also directed and focused attention to the major stories of the day, allowing for nonlinear readings of a newspaper as a fragmented collection of stories, headlines, or leads. A newspaper's layout and seriality therefore scaffold a pseudo-hypertextual structure, one that can be computationally mined for insights. Some libraries and cultural heritage institutions are leading these endeavors, such as the Library of Congress' National Digital Newspaper Program, Europeana, and Trove.\autocite{europeana,trove}

But traditional newspapers have a major limitation: they cannot \emph{explicitly} link to other work in a structured and idiomatic way. Scholars have long relied on the footnote and bibliography to systematically track influence and dialogue, and networks of citations can be created out of them. % QUESTION: ARE THERE ANY DH PROJECTS TO REFERENCE HERE? Mapping citation networks in older documents? %
This forms the basis for citation analysis, or bibliometry, a practice with a long history and strong conventions that I will dive into more closely in the following chapter. Its essential principle is that the more an item is cited, the more influential and credible it is. The online version is known as ``webometrics,'' and it applies certain new standards which online newspapers can take advantage of, both in measuring impact on the web and inside their own archives. But citation is ``as old as written language itself,'' and it is \emph{itself} a language, with its own idioms, syntaxes and exceptions.\autocite[1]{chakrabarti_mining_2003} The footnote has its limitations, only linking back to the past---but newspapers don't even get footnotes.

Links---and their siblings, linked tags---allow for a new standard of citation, reference, and context provision for news. The link can even go beyond the footnote by linking in both directions, allowing readers to see who referenced the story; an old article in \emph{The New York Times}, for instance, can link out to more recent related Times articles, other publishers or blogs that picked up on the story, or conversations in the Times forum or on Twitter. Linking offers great potential, not only for enlivening the reading experience, but for creating a traceable dialogue that can improve a story's discoverability in the future. A number of search algorithms, such as Google's PageRank and Jon Kleinberg's HITS system, create ``hyperlink-induced communities'' between websites, and the same principles can be adopted and expanded within news websites.\autocite[12]{chakrabarti_mining_2003}  

% Where to include these studies of hypertextuality in newspapers? Note that there is a large conceptual difference between inlinks and outlinks (Deuze), Tremayne, Coddington, de Maeyer, Interviews, etc.

A human editor who is tagging a story is equivalent to the archivist in the library, attempting to predict every possible way that a user might search for the story in the future, whether it's ``Sports'' or ``Breaking'' or ``Opinion''---and editors don't have the extensive training and professional expertise that comes with being a librarian or archivist. Journalists are trained to explain, contextualize, and curate rather than structure and tag. Given the impossibility of explicitly and expertly tagging in advance for every possible present and future use, as well as the arbitrariness of tagging \emph{the story} instead of its constituent parts, we can turn to entities and links as supplements to categories and tags in the newsroom archive. These play to a journalist's strengths, and augment rather than replace the human touch that comes with inline links and curated collections.

\subsection{Atoms of news}

In technical terms, stories are usually objects in a database that have associated text, images and tags. Stories contain multitudes, and a typical story might have a variety of metadata attached to it; authors, dates, versions, categories, images, events and collections it's a part of, tags, and so on.\footnote{Figure here with typical story objects; a sample database schema} While more metadata and structure requires more investment at the outset, smart use of such metadata prepares a story for archival reuse. Some of it will be useful for linking or embedding on the website, others for use in an API or application. Stories can include other stories as part of their metadata too, either related manually (by a hyperlink or a human editor) or automatically (via similarity algorithms that analyze the words or topics in the article, the communities it reaches, and so on).

% Also mention the IPTC Standard which is still in use TODAY.

The story has long been the basic unit of news, and so it tends to have a one-to-one relationship with the URL, the basic unit of the web. One section of the \emph{Times'} \emph{Innovation} report announces that they produce ``more than 300 URLs a day,'' using URL as a sort of ``thing'' word, their default unit of work.\autocite[27]{_innovation_2014} Most publishers will assign a ``canonical URL'' to a given story, which serves as its unique identifier, and often, practically speaking, it is the only information that a researcher or search engine can feasibly obtain about a particular document on the web.\footnote{While the researcher or bot crawler could, of course, request the webpage to get the information, each request can take several seconds and some processing power, so it becomes infeasible at a larger scale.} You can be sure to find the most canonical version at the canonical URL, but the article lives in various forms across the web.

But if stories contain multitudes, then why is the article the basic unit of information for news? An article can pull paragraphs from one source, photos and charts from another. It is an ecosystem of media itself, and it can contain other stories in turn. The news app Circa organizes its content around ``atoms'' of news: single facts, quotations, statistics, and images that can be reaggregated and remixed as needed. Systems like Circa's atoms or Vox's ``cards'' aim to create a baseline repository to build upon rather than recreate from scratch every time.

The sustainability and commercial viability of such approaches is still unclear, but the excitement around them speaks to a fundamental rethinking of how we organize news items and structure stories. A ``story'' can be a collection or dialogue of items; indeed, most stories already are. A journalist can still create, but also curate, collect, and contextualize, or allow users to do the same. All of these remixes and reuses can improve the classification and discoverability of the content in turn. Thinking of a story as a collection or mash-up offers a new framework of a story as a highly linked entity, one that can start to organize itself.

Organizing the web by link and tag has often proven more effective than trying to fit its contents into an overarching taxonomy or ontology. Google's PageRank algorithm was the lifeblood that made it the dominant search engine over rivals like Yahoo! and HotBot.\autocite{shirky_ontology_2005} When Yahoo! began in 1994 as a hierarchical directory of useful websites, it seemed like a natural step. Computer users were accustomed to the tree-like document and file structure of computer systems, and the web replicated this in turn. Taxonomy allowed Yahoo! to build relationships between categories into its structure---parents, children, and siblings---which readily enabled features like ``related categories'' and ``more like this.''

But Google succeeded by crawling in the weeds rather than commanding from on high. For Google, the links sort everything out. Berners-Lee proved that networks could work with many links---and in fact, if you had a lot of links, as Clay Shirky puts it, ``you don't need the hierarchy anymore. There is no shelf. There is no file system. The links alone are enough.''\autocite{shirky_ontology_2005}

Shirky and David Weinberger champion the tag as a hybrid hierarchical/networked organizational structure. On one hand, tagging relies on an individual singularly classifying an object under a certain discourse. On the other hand, users are generally free to tag as many times as they want, and using whatever scheme they desire. Tags could range from ``World War II'' to ``articles I want to read.'' Studies and businesses alike have proven that at web scale, even with users tagging items for personal and idiosyncratic reasons, distinct and simple patterns emerge that allow for collaborative classification.\autocite{cattuto_semiotic_2007} These systems, sometimes called ``folksonomies,'' emerge as manifestations of the ``boundary infrastructures'' proposed by Bowker and Star.\autocite{bowker_sorting_2000}

Tags have their limitations; if another user tags an item ``World War 2,'' the system needs to recognize that it means the same thing as ``World War II,'' and publishers employ controlled vocabularies to avoid such ambiguities. Some research has shown that the first tags on an item are likely to influence future tags in turn, resulting in a sort of ontological groupthink.\autocite{cattuto_semiotic_2007} Still, whether a Flickr tag, a Delicious bookmark, or a Twitter hashtag, these crowdsourced approaches to tagging function as links between content; it is not about the tag itself, but the \emph{connection} being made to other content. Shirky even considers the possibility that syonymous terms aren't desirable; perhaps the people who are searching for ``films'' would be better served by just seeing results tagged as ``films,'' and not ``movies'' or ``cinema.''\autocite{shirky_ontology_2005} This radical suggestion uses minor semantic differences as signals, but sometimes hierarchies are crucial; a user searching for movies set in Massachusetts would also want movies tagged ``Boston.''

\emph{The New York Times} sees tagging as core to its business, and the reason it has remained the ``paper of record'' for so long.\autocite[41]{_innovation_2014} This title was bestowed to them largely on the back of the legacy Times Index, which has offered an annual reference version of Times stories since 1913, still published in hard copy. There is no doubt that the Times' tagging practices have helped them remain a library, information hub, and general authority on contextual information. But the \emph{Innovation} report sees them falling behind, adhering too much to the needs of the hard copy Index. They also note crucial limitations with identifying, splitting, and lumping categories; it took seven years for the Times to start tagging stories ``September 11.'' Their team of librarians is required to shepherd about 300 new articles a day into the archive, making sure to keep them discoverable under any context. Tags can concern more than just the contents or event of a story---the Report suggests tagging stories by timeliness, story tone, and larger ``story threads''---but they admit that many of the more exciting tagging potentials would require them to ``better organize our archives.''\autocite[41-42]{_innovation_2014}

So the linking of the archive can occur not only through explicit hyperlinks, but implicit tags and entities that reside within the stories themselves. Most news articles rely on tagging to be connected to other media; if \emph{The Boston Globe} writes a story about New England Patriots coach Bill Belichick, an editor might tag the story ``football'' in order to place it in dialogue with other football stories, landing on the \emph{Globe}'s football topic page, and so on. But this belies a more nuanced dialogue between the words, images, and hyperlinks used within the story itself; a short story that has ``Bill Belichick'' and ``Cincinnati Bengals'' in the text is likely to be referencing a recent game or a trade, while a longer story that brings up his family members or his hometown is likely to be a biographical piece about his life and upbringing. By combining natural language processing and entity linking tools, a story can be automatically and dynamically tagged according to the myriad possible contexts that a user might search for, whether she is looking for stories about football, the Patriots, or Bill Belichick himself.\footnote{explainer on NLP and linking?}

\subsection{From tags to links}

While tagging practices can be enhanced in a variety of ways, Stijn Debrouwere thinks that even ``tags don't cut it.''\autocite{debrouwere_tags_2010} As an expert in news analytics and a co-creator of link-tracking platform NewsLynx, he knows well the limitations of the web and newsrooms' content management systems. His blog series ``Information architecture for news websites'' dives into the headaches that result when journalists think of stories as blobs of text in feeds and streams, rather than structured systems of facts and opnions that carry value in their own right.\autocite{debrouwere_information_2010}

Debrouwere cites a blog post by Adrian Holovaty, co-creator of the Django web framework and its now-retired ``benevolent dictator for life.'' Holovaty's essay revolves around one idea: ``newspapers need to stop the story-centric worldview.''\autocite{holovaty_fundamental_2006} Each story, he notes, contains a vast amount of structure that is being thrown away with every click of the ``publish'' button. He leads through several examples, such as: \begin{itemize}
\item An obituary is about a \emph{person}, involves \emph{dates} and \emph{funeral homes}.
\item A \emph{birth} has parents, a child (or children) and a date.
\item A \emph{college graduate} has a \emph{home state}, a \emph{home town}, a \emph{degree}, a \emph{major} and \emph{graduation year}.
\item A drink special has a \emph{day of the week} and is offered at a \emph{bar}.
\item A \emph{political advertisement} has a \emph{candidate}, a \emph{state}, a \emph{political party}, multiple \emph{issues}, \emph{characters}, \emph{cues}, \emph{music} and more.\end{itemize}

\noindent Holovaty links to context everywhere above, using hyperlinks to literally highlight the information that's otherwise locked away behind stories. Of course we don't need all of this context all the time, but we may really need \emph{some} of the context \emph{sometime}, and it's easier to structure it now than to unlock it later. The better structured this information, Holovaty argues, the more serendipity can foster new features and applications. Proper story scaffolding can lead to more happy accidents of ``wouldn't it be cool if\ldots'' later. Want to map the births and deaths of a famous family, or the happy hours in a neighborhood? You might already have that information buried in your stories.

Debrouwere expands on Holovaty, summarizing his frustration with tags: ``each story could function as part of a web of knowledge around a certain topic, but it doesn't.'' Tags are our only window into content at the level of a story's metadata (which, too often, is all we have). For all their weblike strengths, they are still often inconsistent, outdated, and stale. ``The whole purpose of tags is to relate one piece of content to another,'' and given the dozens of ways that one can type ``George Bush,'' they can't even do that.

Debrouwere concludes that we need ``a way of indicating how content relates to other content on our website and on other websites that is more powerful and more expressive than tags.'' He suggests using vocabularies: set people, places, organizations, events and themes. Knowledge bases like DBpedia, OpenCalais, OpenNLP, AlchemyAPI, or the Getty Vocabularies allow for deep context at low cost, basing its tagging on ``entities, not labels.'' He also advocates for indexing relationships rather than contents, which borrows from Semantic Web principles to add detail to a link. ``A tag on an article says `this article has something to do with this concept or thing.' But what exactly?'' Rather than tagging an article ``Rupert Murdoch,'' a tag has more value if it can say ``criticizes Rupert Murdoch.'' For Debrouwere, ``we don't need the arbitrary distinction between a label and the thing it labels on a website. Let's unlock the full potential of our relationships by making them relationships between things.''

Such a scheme could benefit an end user in many ways. Topic pages, such as New York Times' over 5000 pages ranging from ``A.C. Milan'' to ``Zimbabwe,'' could be smarter, reflecting the most popular articles or most related topics. Entity- and link-oriented schemes can create cascades of relationships, synonyms, and homonyms. Journalists as well as readers would gain improved access to their organization's history, improving the research, context, and tagging of future stories. Debrouwere is suggesting a return of structure to the open web; he envisions a tagging system where a tag can double as a card or widget, linked in turn to other cards and widgets in a network of knowledge. This could be extended to events and phenomena as well as proper names and entities; some emerging systems can recognize phrases like ``Barack Obama announced his candidacy for president'' and ground it as as a unique, unambiguous newsworthy event.\autocite{nothman_grounding_2013} While this research has a long way to go, it is a promising start towards extending the ``ankle-deep semantics'' that Chakrabarti advocates.\autocite[289]{chakrabarti_mining_2003}

This return of structure does not have to be a step backwards to broad ontologies and topics; instead of manually and unilaterally structuring from on high, we can focus on the structure built into the stories already. Rather than replacing stories entirely, or requiring editors to exhaustively tag every component of a piece, a system could automatically supplement a story with new metadata that gives its inherent information an afterlife. Every story---whether a blog post, a map, a listicle, or an interview---has its own structures and patterns.

One example comes from the Boston Globe's March 2015 coverage of the Boston Marathon bombing trial. Data editor Laura Amico knew well that trials are far from linear stories; since they are told by lawyers, they unfold as a series of conflicting arguments. Trials present a proliferation of stories: the chronological narrative of the trial, the pieced-together narrative of the original event, and fragments of tangential narratives from court witnesses, documents, and so on. The key players---witnesses, evidence, victims and lawyers---would have a long history of Globe coverage from the bombing itself. Amico and her team knew there was more than one way to tell this story, so they decided to focus on the facts; the witnesses, exhibits, and arguments are all entered into a spreadsheet, which can generate snippets of facts and entities---designed as cards---for later review.\autocite{mullin_how_2015} Each of these cards can embed and contain other cards, or be combined to form a story. Such a framework recognizes the interlinked nature events, and the stories that summarize and filter them. This project also highlights that ``structured journalism'' does not need to be adopted wholesale; it can work for one-off stories and experiments as well.

One of the most obvious and underexplored structures is the hyperlink. A year after publishing his ``Information architecture'' series, Debrouwere followed up by questioning many of his own allegiances; tags still don't cut it, but maybe taxonomies don't either. He realized: ``The best content recommendations on news websites are inside of the body copy: inline links. With recommendations, you never know what you're getting. It's mystery meat. With links, a writer tells you why she's pointing at something the moment she's pointing at it.''\autocite{debrouwere_taxonomies_2011} It is better to draw on the connections from these links than to rely on automated recommendation engines to organize content. Journalists are better at explaining and contextualizing than they are at tagging and structuring, which are a librarian's craft. Debrouwere knows that newsroom developers are building for journalists, and he ends by asserting that he wants to build ``prosthetics, not machines.''

Another under-mined source of insight lies in the plethora of ``human-generated lists'' (as Google calls them in one patent) around the web.\autocite{franks_discovering_2012} Whether collecting articles, photos, books, songs, or tweets, people obsessively collect and curate, and some are known experts at doing so. These range from Amazon wish lists to mixed-media stories on Storify. Thinking of lists as links between contents, weighted by expertise, leads to interesting potentials. The title of the list, or its other metadata, could tell us more about the context behind the link; a list of local Mexican restaurants is linked by country and location, while a list of my favorite hip-hop albums of 2014 is linked by year, quality, and musical genre. The Times' \emph{Innovation} report suggests allowing users to create lists, since it could allow for deep interactivity without risk to their brand; such a system could leverage readers' collective wisdom by asking users to specify the context behind their lists.

These web-native and polyhierarchical approaches to classification reflect the growing need for newsrooms to find weblike ways to organize their stories. Shirky is a champion of the tag, but he recognizes that organizing by taxonomy and ontology is sometimes preferable; namely, with a small corpus and expert catalogers.\autocite{shirky_ontology_2005} This could have described a pre-web newsroom, but no longer; the publisher's corpus has expanded and linked beyond measure, and its ranks of expert catalogers are rapidly dwindling. This suggests a need to adopt new schemes, which leverage automatic and dynamic tagging, linked entites, image recognition, and the knowledge of experts and crowds.

Still, despite the news story's rigid structure and the limitations of indexing, there is no reason to believe that the article is going away as the core unit of news. Link-based platforms and services like RSS and social media feeds still rely on stable and consistent resources at given URLs. Innovations in story structure could even be at odds with the very notion of revitalizing the archive. In aiming to blend the present with the past, archive-oriented publishers might be bringing past conventions back into the present. Still, some new forays into interactive, multimedia, and app-driven journalism enhance or bypass the URL and hyperlink---I will touch on these at greater length in the conclusion. My aim is not to suggest that we restructure the news story; only that we rethink how they work under the hood. Stories are not uniform resources, and they should not be uniformly tagged and categorized.

% \section{Context in context}

% \subsection{Reporters, writers, librarians, and news geeks}

% \subsection{Explaining the news}

\section{Context in context}

Structuring and linking the news cannot happen through technology alone. One reason that newspapers lacked a citation convention before the web is because journalistic practice didn't require it; while scholars read and cite articles, journalists traditionally go into the field and gather quotes. These are not so easily cited; a well-organized journalist could keep recordings of sources and, for instance, link to a snippet of audio when adding a quote, but this isn't always possible. Journalists may also be reticent to link because they feel they don't need to; sometimes journalists need sources to remain anonymous, obfuscated, or off the record. Most journalists agree that linking manifests many core tenets of journalism, but forcing a journalist to obsessively link to every source would stifle journalistic practice.

Still, journalists are increasingly citing sources that are already published, whether documents on DocumentCloud, data points from an open government database, or aggregated information from other news articles. New journalistic practices thus lend themselves well to structuring and linking to sources to position a story; the challenge is, in part, in shifting journalistic focus. A journalist might be more apt to say that a court hearing occurred ``last week,'' but it is more archivally valuable to say it happened on ``December 12, 2014.'' This doesn't necessarily require changing the prose in the story itself, but a semi-automated process could occur when publishing a story that suggests these structures behind the scenes.

Named Entity Recognition and topic modeling tools can't do all of the work for us; there are too many missed signals and false hits. But a quick widget or plugin that works \emph{with} a reporter or editor to structure a story could prove fruitful. The challenge remains to convince editors of the value of this, and spend the extra minute verifying a story's structure.

% Much of the debate surrounding archives and explainers strikes at the heart of the role of journalism, both on the web and as an industry and institution. Michael Schudson frames the rise of popular newspapers in the 1890s as a pull between two journalisms, one telling ``stories'' and the other providing ``information.''\autocite{schudson} Which of these functions is the primary goal for a news outlet? This question remains critical today, with most news operating on a sort of continuum between these points. Information focuses on the facts, while stories serve an ``aesthetic'' function. But in general, a faster-paced breaking event (such as occurring over an AP wire or a Twitter feed) will result in information, while contextualized longer-form histories and interactive multimedia (like a \emph{New York Times} interactive or \emph{New Yorker} longread) skew towards stories.

% Interactive digital multimedia emerges somewhere between these scales, providing information that allows users themselves to create stories. Interactives carry the promise of deep contextual information along with narrative elements, but given the extremely time-consuming process of building an interactive piece, they tend to accompany events planned far in advance and guaranteed to drive traffic; you're more likely to see an Oscar's, World Cup, or presidential primary interactive feature than rich multimedia surrounding breaking news such as a plane crash or terrorist attack. While both breaking news and deep interactives seemingly exist to provide information, the interactives offer a  proliferation of stories to go with it.

% Such interactive elements are rich with the potential for re-use. Some of the pieces stay relatively ``evergreen'' in both content and structure, proving useful months or years later. They also incorporate collections of media that, taken together, form a network or ecosystem of resources that allow for a new view into digital archives, focused on what media you're utilizing and citing as much as what words are on the page, or what desk you're reporting from. As such, deeply linked multimedia stories can better structure an archive; but can the depth and immersion found in explainer sites scale to work with breaking news and ongoing scoops as well?

% \subsection{Paywalls}

% While many traditional media publications are recognizing the potential role of archives in their shift to digital, they have tended to silo away their archival resources behind paywalls. This is the most literal interpretation of gaining value from one's archive, and it is an understandable choice given the few avenues for them to make money on the web. But paywalls are just one of many possible ways to extract value from archives, and there's more than one way to make a paywall.

% One problem with the way that publishers deal with their paywall now is that they have few ways of measuring ``successful'' archive diving. Like many publishers, Time.com requires a paid subscription in order to access their online Vault. But say a registered user is clicking through the Time Vault. Did she sign up for the subscription just to see the old issues? Is she there on a dedicated research project, or is she just browsing?

% Archive paywalls tend to be very limiting; while publishers will occasionally lift the paywall on relevant archival stories, it is usually impossible to even preview an archived story. This prevents interested users from even testing out the interface or getting curious in the archive in the first place. As online publishers experiment with paywall methods, they should keep in mind their older stories.

% Some archives are more open than others. The New Yorker opened its archives for the summer of 2014, free of charge, as they built the paywall system. The archive is now back behind walls, but the summer experiment seemed interesting. How much were users diving into the archives? How were they browsing the archives--through search or serendipity? Did opening the archives encourage people to think and write more about the New Yorker's rich history?

% % Can I talk to the new yorker abt their experience and write about it here?

% A publisher's archive will often turn up in specific articles geared towards history. At Time.com's Vault, editor/curator Lily Rothman digs out stories and quotes from the history of Time, ranging from historical interests (``Read TIME's Original Review of \emph{The Catcher in the Rye}'') to ephemeral oddities (``13 Weirdly Morbid Vintage News Stories''). Time assistant managing editor Samuel Jacobs likened Rothman to a radio D.J., highlighting singles from the archive to entice readers to pay for the whole collection.\autocite{}

% Other historic deep dives might occur in weekly columns or ``long history'' forays by individual journalists. A Sunday Times article, for instance, might take a historic look at a particular person, neighborhood, or community. These projects have a chance to draw attention to the past through curation as well, by drawing out and resurfacing old stories, photographs and statistics.

% % Find a few examples here. Local interest stories etc. e.g. history of Whitey Bulgur, Boston's bids for the Olympics, or neighborhood deep-dives.

% These projects are a promising start, but they tend to be isolated endeavors, relegated to a single story. Sometimes willfully nostalgic, they can carry an air of ``eating your news vegetables.'' They do not bring the archive fully into dialogue with ongoing events, whether in the research or design process. Journalists don't have easy, seamless access to their publication's past knowledge and institutional memory. Topic pages suffer from lack of organization and explanation. Readers cannot easily dive into old content.

% More generally, news' perpetual focus on the \emph{new} and \emph{now} keeps news publishers in the mindset of serving as information providers rather than knowledge repositories. This results in news delivered in story format, through wires, feeds, and streams. These are one-dimensional channels and metaphors, delivering information in a straight line and single direction. But information is more understandable, sustainable and useful if it's in more than one dimension, structured as a tree, rhizome, or web. Internal news archives have the potential to provide richer, faster resources and connections than the web as a whole, or its indexers (like Google or LexisNexis). This can drive traffic back to the site in turn, and bolster the publisher's authority as an information source. Legacy news publishers are rightly proud of their long histories of stories, photos, and notes; they could do well to show it off rather than hide it away behind paywalls. For publishers, opening access to their past could contribute to solidifying their status in the future.

% \subsection{The Scoop Effect} % Dear lord find a better term for this

% The time is ripe for news and history -- content and context, feeds and archive -- to collide. News outlets have long obsessed over the ``scoop'', being the first to break a story, and indeed these breaking stories still drive a great deal of traffic. But publishers are increasingly scooped in turn; stories break immediately on social media, rather than the next morning in the newspaper. % Can I coin a neologism for this? The "break effect" or something? So that I can keep referring to it in the next couple pages without driving myself crazy
% Emily Bell argues that social media and ``these super platforms ARE the free press, taking over many of the functions of the mainstream media. Social networks are now attracting the same pressures and challenges at a much larger scale that journalism and civic media has wrestled with for years.''
% This is having a profound effect, of course, on publishers; where does the media fit in here? But it is especially affecting the research process, skills and news lifecycle for both journalists and editors/strategists.

% For journalists, it has increasingly destroyed the stereotypical image of the reporter with a notepad in city hall. The increasingly real-time nature of scooping has led to reporters scouring Twitter as much as being in the field; even communication with sources increasingly occurs via email or tweet. The increasing presence of ``explainer'' and ``data journalism'' likewise speaks to this need; reporters must to wade through massive amounts of information in fast, efficient ways in order to uncover possible news stories, which requires very strong digital research skills. These are skills that librarians have been practicing for centuries, and a well-organized and linked archive can help reporters immensely with this research process. The data journalists thus emerges as an amalgam of reporter and librarian.

% For editors and newsroom strategists, it has shifted the role of the journalist and the news publisher to explainer, data-gatherer, and context-provider. Picture a newsworthy event occurring as the epicenter, and the reporting that occurs around it as a set of concentric circles around the event. Towards the center, one might find tweets, wire reports, and quick announcements. At the edge, there are longform pieces, explainers, multimedia work and data-oriented stories that help draw immediate events into larger phenomena. While the scoop remains crucial and breaking news draws traffic, news outlets can no longer serve as raw information providers, with no context. For a publisher to stand out, it is crucial to bring ongoing stories into a larger dialogue and conversation.

% % Coddington 2014 WikiLeaks article. Also articles from Ethan's class. "Sense-making" rather than "information-gathering"
% % Also Boczkowski 2010 talks about recycling content ("News at Work" book)

% \subsection{Explainers}

% This focus is not limited to legacy media, as the rise of ``explainer journalism'' and context-based reporting emerges as the other side of this coin. \emph{The Nation}'s editor and publisher Katrina vanden Heuvel suggests that ``a clever use of archives is kind of an explainer 2.0.''\autocite{} The goal is to provide knowledge, not news.

% The concept of explaining the news is not new. A 2001 Pew Center survey of newspaper editors concluded that they wanted to be ``news explainers'' first and foremost, ahead of ``news breakers'' or ``investigative watchdogs.'' But in a 2008 article called ``National Explainer,'' Jay Rosen accused editors of not staying true to their mission: journalists are not good at explaining the news and providing context. Instead, they focus too much on incremental and episodic updates, many of which go over the head of readers who haven't been following. Rosen likens the process to pushing software updates to a computer that doesn't have the software installed.

% Rosen argues that while journalists are paid to report the news and not explain it, they should also be giving background and context to larger stories. Journalists ``don’t do a very good job of talking about the beginning and what got us to this point where it became news,'' according to Alex Blumburg of \emph{This American Life}. Even the occasional explainer that gets it right ends up in the flow of the same old information; Rosen argues that explainers like David Leonhardt's credit crisis piece in the New York Times ``should have been a tool in the sidebar of every news story the Times did about the mortgage mess.'' The little ``what's this?'' link is ``not about web design. That's a whole new category in journalism that I fear we do not understand at all.''

% Rosen also points out that such explainers are helpful for other reporters as well as the public, influencing news and information flow across the pipeline. A Times explainer, for instance, can reach a reporter who is informed by it as he or she interviews local officials. Calling it a ``scaffold of understanding,'' Rosen suggests that we ``start with clueless journalists'' in the path towards providing context, and went on to create explainthis.org, for people to admit what they don't know to journalists who are ``standing by.''\autocite{rosen_2008}

% Explainthis.org, now defunct, was like a library reference desk, staffed by the public and monitored by journalists. A peer of StackOverflow and ancestor to Quora, it is organized around questions rather than topics, discussed by the public and monitored by journalists. It requires someone to be curious enough to ask the question, however. Rosen touts the ability of explainers to generate interest in a topic, but here we're already expected to be interested.

% At a South by Southwest panel in 2010 called ``Future of Context,'' Rosen outlined the reasons explanation is needed and why it wasn't taking off. He cited both design and institutional problems; the prestige and real-time excitement of breaking (rather than explaining) news, as well as the explainer format getting lost in the shuffle of other news items.\autocite{rosen_2010} Metrics like clicking, watching, and even spending time on a site are not measuring the level of understanding or knowledge gained.

% The panel opened with NPR's Matt Thompson, owner of former contextual news blog newsless.org, arguing that we need more ``systemic information, not episodic info.'' Systemic information could include lists, charts, and maps that stay valuable well after the episodic news is irrelevant. Tristan Harris of Apture says, ``my background is computer science. You never do work that you can't re-use.'' He suggested an ``object-oriented'' approach to journalism with an eye towards sustainable, continuously updating tools and widgets that keep a reader informed. News is organized around stories rather than objects, resulting in streams rather than systems. A systemic, object-oriented approach to news places the context in the center.

% The panel concluded with Harris suggesting a wiki-like approach to journalism, which a big news organization like the New York Times would have the power to sustain. When Kramer asked ``how is this more than links?'' Thompson replied ``Links can be part of it.'' So can wikis, embeds, collections, and related articles.


% ``The context should be the foundation. The systemic stuff should be what you can access first. The episodic stuff is what should be the more info. We “ghettoize” topics pages on our sites, by creating a topics section. When the public just finds just a random collection of links on a so-called topics page, “the quest for context everywhere is set back,” Thompson argues. What would a site look like if it were structured around systems instead of stories?

% Journalists may think, we’re doing so much and now you want to provide context!? Think like an engineer. Make it an imperative to do work you can re-use to provide context. You can use that subduction plates info graphic again and again with every story you write about earthquakes. It’s redefining the notion of “today” value. You’re writing something TODAY that’s only appending something that’s already valuable. Engineers don’t do work they can’t re-use. Do work you can use next time.''

% Chuck Peters, CEO of the Cedar Rapids Gazette:

% ``I can’t see providing that context without changing how we create information in the first instance. Any factual element (photo, incident, quote, data, etc.) can be relevant to numerous contextual narratives. So each of those elements needs to both “stand on its own” and be tagged with as many potential relationships as possible\ldots We usually create information today in locked-down packaged articles, which block the easy flow of the elements between and among narratives.''

% Finally, in an article entitled ``Swimming lessons for journalists,'' PBS's Amy Gahran asserts, ``today’s journalists can---and probably should---consciously shift away from jobs that revolve around content creation (producing packaged `stories') and toward providing layers of journalistic insight and context on top of content created by others (including public information).''

% % Criticize explainers a bit. ``Everything you need to know about x'' sounds final; it runs counter to the very idea that knowledge is infinite, interlinked, and flexible/elastic/malleable. It might not be everything you need to know tomorrow. It might be more or less than you need to know depending on what you need. http://pando.com/2015/01/20/how-rap-genius-and-explainer-sites-are-killing-music-journalism/
% % It turns out that smart, easy-to-digest journalism doesn’t lend itself so easily to the fast production cycles of Internet media, which requires writers with little specialization to quickly educate themselves on a topic using Wikipedia or other secondary sources before regurgitating it back to the public. % "It keeps the reader at arm's length as an outsider looking in"

% % Problem with constantly-updating news: it might make it feel less consequential to publish something erroneous. How do you trust something without an official publish date?
% % Explaining is different from aggregating; and it can seem final and complete, the opposite of what's intended here

% \subsection{Vox}

% At the start of 2014, Ezra Klein left his position at the head of Washington Post's Wonkblog to start Vox, a news site that aimed to make context a first-class citizen of web journalism. Vox's mission: ``to create a site that's as good at explaining the world as it is at reporting on it.''\autocite{} Vox hopes to take a step back from the immediate news event and place it in a larger phenomenon. Taking the long view on stories also gives them an eye towards sustainability; Vox's topics are built around what they call ``card stacks.''\footnote{See http://www.vox.com/cardstacks.} Cards have titles like ``Everything you need to know about marijuana legalization,'' or ``9 facts about the Eurozone crisis,'' and each card is divided into question-driven subsections like ``What is marijuana decriminalization?'' Readers can navigate sequentially, or dive from question to question, going through Vox's explanations and photos. The final option is always the same: the ``Explore'' button takes the reader back to the top of the stacks.

% Vox's card stacks house a growing and morphing repository of knowledge. They are a public archive, like a Wiki but with more authorship intact. At the end of each card, Vox offers a link to email the author/curator of the card stack. For Vox reporters, starting a stack is also a pledge to maintain it. Vox also give a summary of changes made to the card (full versioning, they say, is coming soon).

% The goal is not to replicate Wikipedia, but more like a wiki ``written by one person with a little attitude,'' as Vox co-founder Melissa Bell put it. It's obeying the rules of journalism rather than ``no original research.''  Klein has Wikipedia in his sights, suggesting in the New Yorker that ``I think it's weird that the news cedes so much ground to Wikipedia. That isn't true in other informational sectors.'' By combining incremental news with an evolving repository, Klein hopes to gain the best of both worlds: ``the card stacks add value to the news coverage. And the news coverage creates curiosity that leads people to the card stacks.'' This follows Rosen's idea that explaining the news can generate future interest in incremental updates. For Klein, ``the biggest source of waste is everything the journalist has written before today.''\autocite{nyt_vox_melding} 

% For Klein, there is a distinct need, like Rosen saw, for a website that takes a step back and explains the news. In his words, ``The more folks in the media feel like it's beneath them to answer questions like ``What is marijuana?'' or ``What is Ukraine?'' the more we don't have to compete with them.''

% Vox has accompanied other ``explainer'' and data-focused websites, like Nate Silver's FiveThirtyEight, and The New York Times' The Upshot. Soon after Vox's launch, Craig Silverman wrote ``Why Vox (and other news orgs) could use a librarian,'' suggesting that Vox had ``a huge challenge, due to the rapid decay of facts.''.\autocite{}  Some of these facts may not even be obviously newsworthy, such as if an academic research paper changes a fact in an explainer on Alzheimer's care. Who is going to keep everything up to date? ``Someone at Vox is going to need to know which card stacks to update when,'' and how to keep the explainers updated with minimal maintenance.

% % Evergreen experiment at Vox http://www.vox.com/2015/1/15/7546877/evergreen-experiment

% % How to tie data journalism in with explainer journalism? With multimedia/interactives?

% % The ``tabloidization'' of news a la Emily Bell: "The demands of web scale economics have torpedoed the local news model". Can explainers and deep archive dives help bring local interest back to news reporting? How can news services provide local info too?

% % Herbert Simon "attention economy" in 1971 -- tied to Xeroxing. His concerns then are not new (information overload, attention economy)...but has the web's default state of copying exacerbated his concerns?

% \subsection{News Libraries}

% While someone needs to maintain all of these card stacks, it may not be a librarian. There are fewer and fewer librarians in newsrooms, which places their responsibilities increasingly on the reporter instead. Amy Disch, chair of the Special Libraries Association News Division, speaks to the traditional division of skills between reporter and librarian in the newsroom: ``We can find the information in a lot less time because we know how to drill down in a database. We know good sources to go to where you can quickly find information, so we can cut a lot of time for [reporters] and leave them to do what they do best, which is interviewing and writing. I have my specialty, and they have theirs.''\autocite{}

% Most legacy newsrooms have a library, but their librarians are ``a dying breed,'' with librarians getting laid off from a variety of institutions after the recession. Over 250 news librarians lost their jobs in the U.S. from 2007 to 2010, and membership in the Special Libraries Association News Division has steadily dwindled. Some news libraries and research centers have been completely shut down, outsourced to vendors like LexisNexis.\autocite{} Not only do their reporters' research abilities suffer, they cease to be a steady provider of useful and updated information for readers.

% At a 2001 summit on news libraries, futurist Arthur Harkins suggested that in order to stay relevant, news librarians should ``leave the information management functions to automation'' and instead focus on ``the ability to put knowledge into context and to synthesize information.'' The librarians focused on solutions like structuring incoming stories, helping merge mixed media operations and create new revenue opportunities from older assets. teaching journalists the necessary research and technical skills. Finally, the librarian's task is largely to tag; to structure stories for future discoverability and reuse, by both journalists and the public. Some news libraries, like the Boston Globe, used to manually tag their stories, but no longer do.

% Today, many of these skills are expected of new journalists at the outset. Such reporters arrive armed with years of internet research skills, though some of it with Google over specialized databases. Leslie Norman, former librarian at the Wall Street Journal, suggested, ``I see the news library as it once existed as probably dying, but in many newspapers, it's evolved into something else.''

% Although news libraries are a dying breed, some libraries and cultural heritage organizations are making promising digital inroads into news. Old newspapers provide a rich archive of both historic resources and incidental ephemera like sports scores, weather reports, advertisements and small human interest stories. This gives historians a glimpse of a day, with the major phenomena of the day mixed in with everyday events.

% Most of these projects are aimed towards the serious researcher, but they also point towards ways to engage casual browsers and fans of history. The National Library of Australia's Trove collection features 370 million resources; primarily, Australian newspapers ranging from 1803 to 1954.\footnote{http://help.nla.gov.au/trove/building-with-trove/api} Their API allows programmatic access, which in turn leads to the TroveNewsBot, an irreverent Twitter bot that can search the collection and yield a personalized result. Similarly, the Digital Public Library of America's DPLA Bot and British Library's Mechanical Curator both post random resources from their collection, aiming to inject a serendipitous sense of the past into the present.

% Newspapers would do well to merge increasingly with digital cultural heritage institutions and library APIs. 

% or they argue that ``the software newsrooms have adopted in the digital age has too often reinforced a workflow built around the old medium.''

% \subsection{Promising starts}

% Some initiatives and organizations are taking promising steps towards linking their archives. The New York Times R\&D lab recently released a tool called Madison that aims to crowdsource insight about the ads in old Times issues. Starting with the 1950s, the tool asks users questions about the ads that they see, with the aim of adding structured metadata to otherwise difficult-to-parse texts. The team even released the underlying crowdsource platform as an open-source project, allowing others to run their own crowdsourcing endeavors. This is a promising opportunity for publishers who have the scale and user base required for a crowdsourcing project.m

% % Could I ask the Times how that's going? If they have similar projects?
% % Add some data about how it's progressing anyway

% % Vox Chorus
% Vox's Chorus platform is another promising endeavor in structuring archives. Beyond the sustainable mind to data that is Vox's card stacks, Chorus helps reporters better structure their stories, from adding smart tags to media and widgets.

% % Can I find out more about Chorus? Can I even see how it works?...

% % Photos
% Some promising endeavors are coming from small starts. Take a collection of already-related stories and build nuanced links between them. Or hone in on a single aspect of a linked collection -- for instance, geographic data -- and aim to structure that first. Projects like MapCake -- which automatically creates maps out of structured location tags -- show how effective it can be to focus efforts on structuring one media type. Sites like National Geographic are likewise well primed for beautiful photo archives, which can be watermarked and travel with articles and social media posts. It makes sense for such sites to focus on structuring their photo archives first and foremost. Radio and podcast-oriented sites could do well to take advantage of services like PopUp Archive

% Finally, there are many digital libraries beginning to offer resources and services; quotes, images, and videos. These have the potential to expand the borders of a news archive. The Digital Public Library of America, JSTOR, Flickr, PopUp Archive; all of these sites have useful resources for both reporters and readers. Many also have APIs that would allow easy integration with existing content discovery systems.


% Trends happen in cycles, and the news follows; the news is often repeating itself. Even something seemingly new like data journalism is a holdover of ``precision journalism,'' something that few news stories explaining the trend will point a reader to. Sometimes old documents, whether leaked or declassified, can refuel an old story and paint it in a new light. Other times, new cultural events will conjure up the old; when \emph{12 Years a Slave} was released, the New York Times unearthed a story about its namesake, which then went viral on Gawker.
