\chapter{Networking the News}

In the previous chapters, I have outlined the ways in which the archive, and critical notions of it, has shifted from a fixed and graspable entity to a suite of interconnected parts, constantly shifting and morphing and adapting to new information. As Soumen Chakrabarti puts it, the web is ``an active and evolving repository of knowledge,'' rather than a fixed, bordered entity or set of categories. This chapter hones in specifically on the structure of news archives, and the ways online publishers and legacy news outlets are treating their digital and digitized archives.

The archive has historically been known as ``the morgue'' to newsrooms, but new technologies and conditions have lead to many recent attempts to reanimate the news archive. Nicole Levy wondered if 2014 is ``the year of the legacy media archive'' in a \emph{Capital New York} story about \emph{Time} magazine's new ``Vault''.\footnote{The Vault can be found at http://time.com/vault.} She points to \emph{The Nation}'s ``back issues'', \emph{The New Yorker}'s open archive collections, and the \emph{New York Times}' TimesMachine and @NYTArchives Twitter account as examples of old publishers endeavoring to use their rich histories to create something new.\autocite{}

The Times' celebrated \emph{Innovation Report}, an internal document leaked to the press in May 2014, emphasizes the archive's potential: ``Our rich archive offers one of our clearest advantages over new competitors\ldots[b]ut we rarely think to mine our archive, largely because we are so focused on news and new features,'' arguing that ``we can be both a daily newsletter and a library.'' The report suggests that arts and culture content, more likely to be evergreen, could be organized ``more by relevance than by publication date,'' and the topic homepages should be more like guides than wires. The report goes on to enumerate successful experiments with repackaging old content in collections, organized by categories and themes. They even suggest building a CMS widget to create collections-- something that readers could also do without risk to the Times brand. By creating ``no new articles, only new packaging,'' the Times can easily give new life to old stories.

What makes this moment rich for focusing on digital archives, and their potential value to publishers both old and new? What are these outfits' plans for measuring success? How can legacy media best engage their old archives, and how can digital media prepare itself for the archives of the future?

The archival focus is one of several related movements in today's online news industry. Coupled with trends towards ``explainer'' and ``data'' journalism, we see a pattern among some news outlets attempting to evade and reconsider the news cycle's obsession with speed and feeds. News is typically delivered in a stream format, full of boilerplate text that is repeated across every story related to a given theme. By experimenting with new forms of what a news story can be -- incorporating lists, photos, videos, graphs, quotes, interactives -- explainers and data journalists aim to find a new digital

The resulting media is rich with the potential for re-use. Many of these pieces stay relatively ``evergreen'', the news world's term for stories that are less directly tied to the news cycle. They also incorporate collections of media that, taken together, form a network or ecosystem of resources that allow for a new view into digital archives, focused on what you're citing as much as what words you're using.

\section{Publishers and the archive}

Trends happen in cycles, and the news follows; the news is often repeating itself. Even something seemingly new like data journalism is a holdover of ``precision journalism,'' something that few news stories explaining the trend will point a reader to. Sometimes old documents, whether leaked or declassified, can refuel an old story and paint it in a new light. Other times, new cultural events will conjure up the old; when \emph{12 Years a Slave} was released, the New York Times unearthed a story about its namesake, which then went viral on Gawker.

By looking at the challenges and methods in digitizing and structuring legacy media archives, we can gain a sense of how news stories are structured on a small scale, and how a collection of them creates context on a larger scale. This lets us think closely about the structure of digital content, and the ways that news publishers can continuously keep their archives relevant and context at hand.

\subsection{Paywalls}

While many traditional media publications are recognizing the potential role of archives in their shift to digital, they have tended to silo away their archival resources behind paywalls. This is the most literal interpretation of gaining value from one's archive, and it is an understandable choice given the few avenues for them to make money on the web. But paywalls are just one of many possible ways to extract value from archives, and there's more than one way to make a paywall.

One problem with the way that publishers deal with their paywall now is that they have few ways of measuring ``successful'' archive diving. Like many publishers, Time.com requires a paid subscription in order to access their online Vault. But say a registered user is clicking through the Time Vault. Did she sign up for the subscription just to see the old issues? Is she there on a dedicated research project, or is she just browsing?

Archive paywalls tend to be very limiting; while publishers will occasionally lift the paywall on relevant archival stories, it is usually impossible to even preview an archived story. This prevents interested users from even testing out the interface or getting curious in the archive in the first place. As online publishers experiment with paywall methods, they should keep in mind their older stories.

Some archives are more open than others. The New Yorker opened its archives for the summer of 2014, free of charge, as they built the paywall system. The archive is now back behind walls, but the summer experiment seemed interesting. How much were users diving into the archives? How were they browsing the archives--through search or serendipity? Did opening the archives encourage people to think and write more about the New Yorker's rich history?

% Can I talk to the new yorker abt their experience and write about it here?

A publisher's archive will often turn up in specific articles geared towards history. At Time.com's Vault, editor/curator Lily Rothman digs out stories and quotes from the history of Time, ranging from historical interests (``Read TIME's Original Review of \emph{The Catcher in the Rye}'') to ephemeral oddities (``13 Weirdly Morbid Vintage News Stories''). Time assistant managing editor Samuel Jacobs likened Rothman to a radio D.J., highlighting singles from the archive to entice readers to pay for the whole collection.\autocite{}

Other historic deep dives might occur in weekly columns or ``long history'' forays by individual journalists. A Sunday Times article, for instance, might take a historic look at a particular person, neighborhood, or community. These projects have a chance to draw attention to the past through curation as well, by drawing out and resurfacing old stories, photographs and statistics.

% Find a few examples here. Local interest stories etc. e.g. history of Whitey Bulgur, Boston's bids for the Olympics, or neighborhood deep-dives.

These projects are a promising start, but they tend to be isolated endeavors, relegated to a single story. Sometimes willfully nostalgic, they can carry an air of ``eating your news vegetables.'' They do not bring the archive fully into dialogue with ongoing events, whether in the research or design process. Journalists don't have easy, seamless access to their publication's past knowledge and institutional memory. Topic pages suffer from lack of organization and explanation. Readers cannot easily dive into old content.

More generally, news' perpetual focus on the \emph{new} and \emph{now} keeps news publishers in the mindset of serving as information providers rather than knowledge repositories. This results in news delivered in story format, through wires, feeds, and streams. These are one-dimensional channels and metaphors, delivering information in a straight line and single direction. But information is more understandable, sustainable and useful if it's in more than one dimension, structured as a tree, rhizome, or web. Internal news archives have the potential to provide richer, faster resources and connections than the web as a whole, or its indexers (like Google or LexisNexis). This can drive traffic back to the site in turn, and bolster the publisher's authority as an information source. Legacy news publishers are rightly proud of their long histories of stories, photos, and notes; they could do well to show it off rather than hide it away behind paywalls. For publishers, opening access to their past could contribute to solidifying their status in the future.

\subsection{The stages of news archives}

The development of a digital news archive goes through three stages. These mirror Pavlik's stages of development in online newspapers in general; in 1997, Pavlik observed that newspapers started by copying from the print edition, then supplemented the copy with interactive features, then in the third and last stage, started writing copy specifically for the online version of the story. The development of a digital news archive likewise runs through three stages, which I refer to as digitizing, atomizing, and networking.

The first stage for any legacy publisher -- anyone who creates physical newspapers or magazines -- is to \emph{digitize} the archive. This tends to consist of scanning the pages of old publications, running OCR (optical character recognition) on each page, and exposing the results to a search interface for researchers and, perhaps, interested readers.

Most publishers have reached this stage; it is a crucial first step for enlivening the archive, but a physical record can often limit the digital equivalent's potentials. Digital versions of physical articles often do not leverage links and mixed media to the same effect. While a digital-native version of a print article might directly cite more sources or feature an intriguing interactive, these elements remain second-class citizens to the print article, which digital versions must remain faithful to. The Times' Innovation Report argues that by modeling their website and apps on their print structure, the Times ``ask[s] too much of readers.'' So it is crucial to remember at this stage that the digital archive has different potential from its physical counterpart. The physical record should be the starting point of the digital archive, but as a source for linking, rethinking, and remixing, not as a stodgy artifact to be modeled after.

% Back up your archives! http://www.cjr.org/behind_the_news/minus_proper_archives_many_new.php. Many of these just rely on LexisNexis instead.

It is also telling that many of the digitization projects, begun decades ago, focused exclusively on salvaging the text. This ignores substantial information in the archive, of course, and speaks to the shortsightedness of many projects aimed at digitizing the past. Images, advertisements, maps and formatting were all lost, and many of them are being re-scanned by publishers, at great expense, in order to capture the details that they ignored years ago.

Historical images are one of the greatest potential sources of engagement and revenue for news archives. It could be trivially easy for some news archives to sell old photographs of historic local events. Some projects, like the New York Times' advertisement tagging project, are aiming specifically at images in attempts to gain more insight from them. It is therefore surprising that images struggle to be monetized. 

Researcher Kalev Leetaru took this approach to the Internet Archive. The Internet Archive's OCR software threw out images, and Leetaru's would save whatever it threw out as an image file. He has since put 2.6 million of these Internet Archive images onto Flickr for open use. ``They have been focusing on the books as a collection of words,'' he told the BBC. ``This inverts that.''\autocite{bbc_leetaru} Newspaper and journal images provide a richer glimpse of history, and one that might prove more engaging to digital readers than dated text. You also get a sense of the visual language and associations of the time; as any cultural studies scholar can tell you, advertisements provide a revealing window into culture and history.

Old images are an untapped resource for new stories as well; these millions of images on Flickr or in a news archive can be used freely to enrich a story. Networking an archive also involves making its images structured and discoverable.

The digital publishing space is increasingly moving toward a ``post-text'' world, to use a term Felix Salmon used when he announced joining Fusion. Digitization projects focus on text, not least because it is the easiest to computationally glean insight from, but future efforts to digitize and categorize need to take a holistic, post-text approach to their media assets and web resources as a whole. This will allow the signals and not the formats to determine the best way to tell stories and present information.

% Another complicating layer: digital archiving projects (e.g. Internet Archive) "take the website as the main unit of archiving (Brugger 2012) and privilege the content of a website over its socio-technical context. (Weltevrede 2009).  The website as an archived object is favored over other natively digital objects where the website is archived over “the references contained therein (hyperlinks), the systems that delivered them (engines), the ecology in which they may or may not thrive (the sphere) and the related pages, profiles and status updates on platforms” (Rogers 2013). Thus, in the archiving process the website is detached from the larger web context it resides in." (Helmond 2013 MIT8). This means even networking the digital archive has its limitations and challenges; where do you start/stop linking?
% Also see Brugger "When the Present Web is Later the Past" and "Website history and the website as an object of study"

The second stage is to \emph{atomize} the archive; to break these scanned pages into their consituent parts. Given the newspaper's inherently hypertextual nature (discussed later), this is a major challenge at any scale. What metadata is worth saving? The text, the subtext, the pictures? The photo or pullquote on the side? Is the image in the center of the page associated with the article on the left, the right, or both?

Newspapers are rich archival documents, because they store both ephemera and history. Journalists sometimes divide these types of news into ``stock'' and ``flow''; the constant stream of information built for \emph{right now}, versus the durable stuff, built to stand the test of time. Newspapers also have advertisements, classifieds, stock quotes, and weather diagrams. A newspaper is a very complex design object with specific archival affordances; their irregular size, seriality, and great care in page placement make them ripe for unique forms of automated analysis. For some researchers, placement will be important (was an article’s headline on the first page? Above or below the fold? Was there an image, or a counterpoint article next to it?). Others could be examining the newspaper itself over time, rather than the contents within (for instance, did a paper’s writing style or ad placement change over the course of a decade?) Still others may be hoping to deep-dive into a particular story across various journals. In each case, we can glean information from where and when it was published on the page.

The project of atomizing the archive should take advantage of the signals built into newspapers in the first place, accumulating metadata from its size, shape, and context. An atomized archive should also provide a solid interface for viewing the original in its context. When legacy news publishers refer to a ``linked'' record in a digital archive, they are refering to the ability to see the original source page; if a text-searchable index is connected to a scan of the original, it is considered ``linked.'' Some publishers do not even have linked records for their entire archive. This makes context difficult to grasp for interested researchers.

But legacy publishers with atomized archives also often have clunky ways to access them. The Boston Globe uses Methode, a content management system that does not allow mass printing of articles, does not carry the publication date with it as an article is printed. These small errors add up, and no one likes to use the archive as a result, instead relying on LexisNexis. Atomizing the archive is not helpful without a good interface.

The final stage is to \emph{network} the archive, which few publishers have fully done; indeed, as the drive towards encyclopedism implies, it is likely that this project could never be \emph{fully} done. A better question is, where is the best place to start in networking the archive? How can we balance value with manageability?

Networking the archive requires tagging, annotating, and connecting the items together. This includes both explicit and implicit references, and both manual and automatic means. Whether an editor makes a note on a specific article, or a bot tags an old image with ``Poughkeepsie, N.Y.,'' each act can help in the networking of the archive.

The end goal is to bypass the need for dedicated search to surface archival content. We want to use a ``push'' rather than ``pull'' method of archival materials. This is as true for reporters and dedicated researchers as it is for casual browsers and fans. A user doesn't always know exactly what he or she wants, and a networked archive can work with a user to surface it.

The ``networked archive'' borrows from, but is distinct from the notion of ``networked journalism.'' A term popularized by Jeff Jarvis to refer to the growing citizen journalism movement, networked journalism has also led to Jarvis's succinct motto of ``do what you do best, link to the rest.''\footnote{jarvis}. Building on this notion, Charlie Beckett posits that linking between sources leads to editorial diversity, connectivity and interactivity, and relevance.\footnote{}

A networked archive borrows from this notion but turns the conversation inward; links that point inward are vastly different from those that point out. Few news organizations have truly embraced networked journalism, and even fewer (if any) have considered a networked archive.

However, as Juliette De Maeyer points out, there is increased interest among news organizations in the power of linking.

\section{Context in context}

\subsection{The Scoop Effect} % Dear lord find a better term for this

The time is ripe for news and history -- content and context, feeds and archive -- to collide. News outlets have long obsessed over the ``scoop'', being the first to break a story, and indeed these breaking stories still drive a great deal of traffic. But publishers are increasingly scooped in turn; stories break immediately on social media, rather than the next morning in the newspaper. % Can I coin a neologism for this? The "break effect" or something? So that I can keep referring to it in the next couple pages without driving myself crazy
Emily Bell argues that social media and ``these super platforms ARE the free press, taking over many of the functions of the mainstream media. Social networks are now attracting the same pressures and challenges at a much larger scale that journalism and civic media has wrestled with for years.''
This is having a profound effect, of course, on publishers; where does the media fit in here? But it is especially affecting the research process, skills and news lifecycle for both journalists and editors/strategists.

For journalists, it has increasingly destroyed the stereotypical image of the reporter with a notepad in city hall. The increasingly real-time nature of scooping has led to reporters scouring Twitter as much as being in the field; even communication with sources increasingly occurs via email or tweet. The increasing presence of ``explainer'' and ``data journalism'' likewise speaks to this need; reporters must to wade through massive amounts of information in fast, efficient ways in order to uncover possible news stories, which requires very strong digital research skills. These are skills that librarians have been practicing for centuries, and a well-organized and linked archive can help reporters immensely with this research process. The data journalists thus emerges as an amalgam of reporter and librarian.

For editors and newsroom strategists, it has shifted the role of the journalist and the news publisher to explainer, data-gatherer, and context-provider. Picture a newsworthy event occurring as the epicenter, and the reporting that occurs around it as a set of concentric circles around the event. Towards the center, one might find tweets, wire reports, and quick announcements. At the edge, there are longform pieces, explainers, multimedia work and data-oriented stories that help draw immediate events into larger phenomena. While the scoop remains crucial and breaking news draws traffic, news outlets can no longer serve as raw information providers, with no context. For a publisher to stand out, it is crucial to bring ongoing stories into a larger dialogue and conversation.

% Coddington 2014 WikiLeaks article. Also articles from Ethan's class. "Sense-making" rather than "information-gathering"
% Also Boczkowski 2010 talks about recycling content ("News at Work" book)

\subsection{Explainers}

This focus is not limited to legacy media, as the rise of ``explainer journalism'' and context-based reporting emerges as the other side of this coin. \emph{The Nation}'s editor and publisher Katrina vanden Heuvel suggests that ``a clever use of archives is kind of an explainer 2.0.''\autocite{} The goal is to provide knowledge, not news.

The concept of explaining the news is not new. A 2001 Pew Center survey of newspaper editors concluded that they wanted to be ``news explainers'' first and foremost, ahead of ``news breakers'' or ``investigative watchdogs.'' But in a 2008 article called ``National Explainer,'' Jay Rosen accused editors of not staying true to their mission: journalists are not good at explaining the news and providing context. Instead, they focus too much on incremental and episodic updates, many of which go over the head of readers who haven't been following. Rosen likens the process to pushing software updates to a computer that doesn't have the software installed.

Rosen argues that while journalists are paid to report the news and not explain it, they should also be giving background and context to larger stories. Journalists ``don’t do a very good job of talking about the beginning and what got us to this point where it became news,'' according to Alex Blumburg of \emph{This American Life}. Even the occasional explainer that gets it right ends up in the flow of the same old information; Rosen argues that explainers like David Leonhardt's credit crisis piece in the New York Times ``should have been a tool in the sidebar of every news story the Times did about the mortgage mess.'' The little ``what's this?'' link is ``not about web design. That's a whole new category in journalism that I fear we do not understand at all.''

Rosen also points out that such explainers are helpful for other reporters as well as the public, influencing news and information flow across the pipeline. A Times explainer, for instance, can reach a reporter who is informed by it as he or she interviews local officials. Calling it a ``scaffold of understanding,'' Rosen suggests that we ``start with clueless journalists'' in the path towards providing context, and went on to create explainthis.org, for people to admit what they don't know to journalists who are ``standing by.''\autocite{rosen_2008}

Explainthis.org, now defunct, was like a library reference desk, staffed by the public and monitored by journalists. A peer of StackOverflow and ancestor to Quora, it is organized around questions rather than topics, discussed by the public and monitored by journalists. It requires someone to be curious enough to ask the question, however. Rosen touts the ability of explainers to generate interest in a topic, but here we're already expected to be interested.

At a South by Southwest panel in 2010 called ``Future of Context,'' Rosen outlined the reasons explanation is needed and why it wasn't taking off. He cited both design and institutional problems; the prestige and real-time excitement of breaking (rather than explaining) news, as well as the explainer format getting lost in the shuffle of other news items.\autocite{rosen_2010} Metrics like clicking, watching, and even spending time on a site are not measuring the level of understanding or knowledge gained.

The panel opened with NPR's Matt Thompson, owner of former contextual news blog newsless.org, arguing that we need more ``systemic information, not episodic info.'' Systemic information could include lists, charts, and maps that stay valuable well after the episodic news is irrelevant. Tristan Harris of Apture says, ``my background is computer science. You never do work that you can't re-use.'' He suggested an ``object-oriented'' approach to journalism with an eye towards sustainable, continuously updating tools and widgets that keep a reader informed. News is organized around stories rather than objects, resulting in streams rather than systems. A systemic, object-oriented approach to news places the context in the center.

The panel concluded with Harris suggesting a wiki-like approach to journalism, which a big news organization like the New York Times would have the power to sustain. When Kramer asked ``how is this more than links?'' Thompson replied ``Links can be part of it.'' So can wikis, embeds, collections, and related articles.


``The context should be the foundation. The systemic stuff should be what you can access first. The episodic stuff is what should be the more info. We “ghettoize” topics pages on our sites, by creating a topics section. When the public just finds just a random collection of links on a so-called topics page, “the quest for context everywhere is set back,” Thompson argues. What would a site look like if it were structured around systems instead of stories?

Journalists may think, we’re doing so much and now you want to provide context!? Think like an engineer. Make it an imperative to do work you can re-use to provide context. You can use that subduction plates info graphic again and again with every story you write about earthquakes. It’s redefining the notion of “today” value. You’re writing something TODAY that’s only appending something that’s already valuable. Engineers don’t do work they can’t re-use. Do work you can use next time.''

Chuck Peters, CEO of the Cedar Rapids Gazette:

``I can’t see providing that context without changing how we create information in the first instance. Any factual element (photo, incident, quote, data, etc.) can be relevant to numerous contextual narratives. So each of those elements needs to both “stand on its own” and be tagged with as many potential relationships as possible\ldots We usually create information today in locked-down packaged articles, which block the easy flow of the elements between and among narratives.''

Finally, in an article entitled ``Swimming lessons for journalists,'' PBS's Amy Gahran asserts, ``today’s journalists can — and probably should — consciously shift away from jobs that revolve around content creation (producing packaged “stories”) and toward providing layers of journalistic insight and context on top of content created by others (including public information).''

% Criticize explainers a bit. ``Everything you need to know about x'' sounds final; it runs counter to the very idea that knowledge is infinite, interlinked, and flexible/elastic/malleable. It might not be everything you need to know tomorrow. It might be more or less than you need to know depending on what you need. http://pando.com/2015/01/20/how-rap-genius-and-explainer-sites-are-killing-music-journalism/
% It turns out that smart, easy-to-digest journalism doesn’t lend itself so easily to the fast production cycles of Internet media, which requires writers with little specialization to quickly educate themselves on a topic using Wikipedia or other secondary sources before regurgitating it back to the public. % "It keeps the reader at arm's length as an outsider looking in"

% Problem with constantly-updating news: it might make it feel less consequential to publish something erroneous. How do you trust something without an official publish date?
% Explaining is different from aggregating; and it can seem final and complete, the opposite of what's intended here

\subsection{Vox}

At the start of 2014, Ezra Klein left his position at the head of Washington Post's Wonkblog to start Vox, a news site that aimed to make context a first-class citizen of web journalism. Vox's mission: ``to create a site that's as good at explaining the world as it is at reporting on it.''\autocite{} Vox hopes to take a step back from the immediate news event and place it in a larger phenomenon. Taking the long view on stories also gives them an eye towards sustainability; Vox's topics are built around what they call ``card stacks.''\footnote{See http://www.vox.com/cardstacks.} Cards have titles like ``Everything you need to know about marijuana legalization,'' or ``9 facts about the Eurozone crisis,'' and each card is divided into question-driven subsections like ``What is marijuana decriminalization?'' Readers can navigate sequentially, or dive from question to question, going through Vox's explanations and photos. The final option is always the same: the ``Explore'' button takes the reader back to the top of the stacks.

Vox's card stacks house a growing and morphing repository of knowledge. They are a public archive, like a Wiki but with more authorship intact. At the end of each card, Vox offers a link to email the author/curator of the card stack. For Vox reporters, starting a stack is also a pledge to maintain it. Vox also give a summary of changes made to the card (full versioning, they say, is coming soon).

The goal is not to replicate Wikipedia, but more like a wiki ``written by one person with a little attitude,'' as Vox co-founder Melissa Bell put it. It's obeying the rules of journalism rather than ``no original research.''  Klein has Wikipedia in his sights, suggesting in the New Yorker that ``I think it's weird that the news cedes so much ground to Wikipedia. That isn't true in other informational sectors.'' By combining incremental news with an evolving repository, Klein hopes to gain the best of both worlds: ``the card stacks add value to the news coverage. And the news coverage creates curiosity that leads people to the card stacks.'' This follows Rosen's idea that explaining the news can generate future interest in incremental updates. For Klein, ``the biggest source of waste is everything the journalist has written before today.''\autocite{nyt_vox_melding} 

For Klein, there is a distinct need, like Rosen saw, for a website that takes a step back and explains the news. In his words, ``The more folks in the media feel like it's beneath them to answer questions like ``What is marijuana?'' or ``What is Ukraine?'' the more we don't have to compete with them.''

Vox has accompanied other ``explainer'' and data-focused websites, like Nate Silver's FiveThirtyEight, and The New York Times' The Upshot. Soon after Vox's launch, Craig Silverman wrote ``Why Vox (and other news orgs) could use a librarian,'' suggesting that Vox had ``a huge challenge, due to the rapid decay of facts.''.\autocite{}  Some of these facts may not even be obviously newsworthy, such as if an academic research paper changes a fact in an explainer on Alzheimer's care. Who is going to keep everything up to date? ``Someone at Vox is going to need to know which card stacks to update when,'' and how to keep the explainers updated with minimal maintenance.

% Evergreen experiment at Vox http://www.vox.com/2015/1/15/7546877/evergreen-experiment

% How to tie data journalism in with explainer journalism? With multimedia/interactives?

% The ``tabloidization'' of news a la Emily Bell: "The demands of web scale economics have torpedoed the local news model". Can explainers and deep archive dives help bring local interest back to news reporting? How can news services provide local info too?

% Herbert Simon "attention economy" in 1971 -- tied to Xeroxing. His concerns then are not new (information overload, attention economy)...but has the web's default state of copying exacerbated his concerns?

\subsection{News Libraries}

While someone needs to maintain all of these card stacks, it may not be a librarian. There are fewer and fewer librarians in newsrooms, which places their responsibilities increasingly on the reporter instead. Amy Disch, chair of the Special Libraries Association News Division, speaks to the traditional division of skills between reporter and librarian in the newsroom: ``We can find the information in a lot less time because we know how to drill down in a database. We know good sources to go to where you can quickly find information, so we can cut a lot of time for [reporters] and leave them to do what they do best, which is interviewing and writing. I have my specialty, and they have theirs.''\autocite{}

Most legacy newsrooms have a library, but their librarians are ``a dying breed,'' with librarians getting laid off from a variety of institutions after the recession. Over 250 news librarians lost their jobs in the U.S. from 2007 to 2010, and membership in the Special Libraries Association News Division has steadily dwindled. Some news libraries and research centers have been completely shut down, outsourced to vendors like LexisNexis.\autocite{} Not only do their reporters' research abilities suffer, they cease to be a steady provider of useful and updated information for readers.

At a 2001 summit on news libraries, futurist Arthur Harkins suggested that in order to stay relevant, news librarians should ``leave the information management functions to automation'' and instead focus on ``the ability to put knowledge into context and to synthesize information.'' The librarians focused on solutions like structuring incoming stories, helping merge mixed media operations and create new revenue opportunities from older assets. teaching journalists the necessary research and technical skills. Finally, the librarian's task is largely to tag; to structure stories for future discoverability and reuse, by both journalists and the public. Some news libraries, like the Boston Globe, used to manually tag their stories, but no longer do.

Today, many of these skills are expected of new journalists at the outset. Such reporters arrive armed with years of internet research skills, though some of it with Google over specialized databases. Leslie Norman, former librarian at the Wall Street Journal, suggested, ``I see the news library as it once existed as probably dying, but in many newspapers, it's evolved into something else.''

Although news libraries are a dying breed, some libraries and cultural heritage organizations are making promising digital inroads into news. Old newspapers provide a rich archive of both historic resources and incidental ephemera like sports scores, weather reports, advertisements and small human interest stories. This gives historians a glimpse of a day, with the major phenomena of the day mixed in with everyday events.

Most of these projects are aimed towards the serious researcher, but they also point towards ways to engage casual browsers and fans of history. The National Library of Australia's Trove collection features 370 million resources; primarily, Australian newspapers ranging from 1803 to 1954.\footnote{http://help.nla.gov.au/trove/building-with-trove/api} Their API allows programmatic access, which in turn leads to the TroveNewsBot, an irreverent Twitter bot that can search the collection and yield a personalized result. Similarly, the Digital Public Library of America's DPLA Bot and British Library's Mechanical Curator both post random resources from their collection, aiming to inject a serendipitous sense of the past into the present.

Newspapers would do well to merge increasingly with digital cultural heritage institutions and library APIs. 

or they argue that ``the software newsrooms have adopted in the digital age has too often reinforced a workflow built around the old medium.''


\section{The structure of stories}

% Hypertext as semi-structured; APIs fully structured

% Also look at Skye Doherty's foray into design of hypernarratives in journalism

News publishers prove an ideal study for examining the potentials of hypertext archives. If we treat a newspaper as a proto-hypertextual document, it becomes apparent that online news might be a natural extension of reading the newspaper. Few readers go through a newspaper sequentially, paying equal attention to every article; instead the reader jumps around from page to page, skimming some sections for its raw information while reading longer pieces more deeply. A newspaper's front page reads like a website homepage, with snippets and teasers that aim to draw the reader deeper. A given page can hold several articles, and an interested reader might be distracted or intrigued by a ``related article'' next to the one he or she came to read. Some works are categorized into sections -- arts, sports, letters to the editor -- while others might be paired with a certain advertisement or reaction article. These examples point to the inherently interlinked nature of newspapers, and the endless potential for insightful metadata; newspapers might seem to naturally lend themselves to the digital world.

However, traditional newspapers have a major limitation: they cannot \emph{explicitly} link to other work in a structured and idiomatic way. Scholars have long relied on the footnote and bibliography to systematically track influence and dialogue, and networks of citations can be created out of them. Citation is ``as old as written language itself,''\footnote{Chakrabarti} and it is \emph{itself} a language, with its own idioms, syntaxes and exceptions. The footnote has its limitations (as discussed earlier somewhere), % WHERE?? % % QUESTION: ARE THERE ANY DH PROJECTS TO REFERENCE HERE? Mapping citation networks in older documents? %
but newspapers don't even get footnotes. So while a newspaper's layout and seriality might afford a news story more insight than an academic article, its flatness and lack of citation conventions lead to limitations in computationally gathering insight from a newspaper archive.

Digital news publishing has the potential to change the conversation through the networks it creates; its hyperlinks, its embedded media, and the media that links \emph{to} it. This can form ``hyperlink-induced communities.''\footnote{Chakrabarti}. Hyperlinks allow for a new standard of citation, reference, and context provision for news. At smaller scales, the link can even go beyond the footnote by linking in both directions, allowing readers to see who referenced the story; an old article in the New York Times, for instance, can link out to more recent related Times articles, other publishers or blogs that picked up on the story, or conversations in the Times forum or on Twitter. Linking offers great potential, not only for enlivening the reading experience, but also for creating a traceable dialogue that can improve a story's discoverability in the future.

Much of the work done on mining digital archives has focused on Natural Language Processing (NLP), or the science of converting human languages to machine language and back. This is a very fruitful avenue for research and archival enlivenment, but it is only one stone to unturn; here I might propose developing a Citation Language Processing (CLP) system.

The use of citation analysis to determine impact, weight, or ranking is an old practice, especially for the sciences. Known as bibliometry, the practice has a long history with strong conventions, which I will dive into more closely in the following chapter. The online version is sometimes known as ``webometrics.''

But even a Citation Language Processor requires a standardized Citation Language. The closest that we have come to such standards involves the Semantic Web and, specifically, the rNews initiative. But our work here is easier, because it is presuming a single publisher within one domain; the ability to change its own language, if not the language of others, enables vastly improved inlinking and recommendation from one's own archive.

\subsection{Ontologies and tags}

In technical terms, stories are usually objects in a database that have associated text, images and tags. Stories contain multitudes; they are an agglomeration of multimedia objects. Any link from one story to another must then refer to the story as a whole, rather than a salient part of that story (whether it be a certain paragraph or an interactive chart within it).

What is the atomic unit of information for news? It has traditionally been the article in a feed or stream, but Apture's Tristan Harris suggests that ``Because journalism is structured on the article, it doesn't accomodate the full extent of information we need.'' An article tends to pull paragraphs from one source, photos and charts from another. The news app Circa organizes its content around ``atoms'' of news: single facts, quotations, statistics, and images that can be reaggregated and remixed as needed. Systems like Vox and Circa aim to create a baseline repository to build upon rather than recreate from scratch every time.

This approach rethinks how we organize news items and structure stories. A ``story'' need not be a fresh new original piece of reporting every time; instead it can be a collection, ecosystem or dialogue of items. A journalist can still create, but also curate, collect, and contextualize. Thinking of a story as a collection or mash-up offers a new framework of a story as a highly linked entity, one that can start to organize itself.

As discussed (where?), organizing by link and tag has often proven a more effective form of sorting things out on the web than has organizing by overarching taxonomy or ontology. As David Weinberger, Clay Shirky and others have argued, it is part of what made Google the dominant search engine over rivals like Yahoo! and HotBot. Yahoo! began in 1994 as a hierarchical directory of useful websites. This is a natural first step for an online search engine, since computer users have grown accustomed to the tree-like document and file structure pioneered by Douglas Engelbart and others, and replicated by Berners-Lee's domains and paths on the web. It also builds relationships between categories into its structure -- parents, children, and siblings -- which readily enables features like ``More like this.''

Google's success rides on their reliance on \emph{crawling} in the weeds rather than \emph{categorizing} from on high.

Shirky shows that ``you could have a lot of links. You don't have to have just a few links, you could have a whole lot of links,'' and if you have a lot of links, ``you don't need the hierarchy anymore. There is no shelf. There is no file system. The links alone are enough.'' For Shirky, links can be formed by the tags that users create; tags are crucial in organizing the web. ``Tags are important mainly for what they leave out. By forgoing formal classification, tags enable a huge amount of user-produced organizational value, at vanishingly small cost.''

Some studies show that even at web scale, with users tagging items for personal and idiosyncratic reasons, distinct and simple patterns emerge that allow for collaborative classification.\autocite{catutto_semiotic_dynamics}.

The New York Times also sees tagging as core to their business, and the main reason that they have remained the ``paper of record'' for decades. In short, well-structured stories and tags have helped the Times remain a library, information hub and general authority on contextual information. This can be true for all publishers, both legacy and digital, and it is surely at the heart of what Vox aims to build with its card stacks. But the Times' Innovation Report sees them falling behind, as they adhere to the needs of the legacy Times Index rather than the modern affordances of digital search. The Innovation Report notes that it took seven years for the Times to start tagging stories ``September 11.'' Evan Sandhaus suggests that they are organizing their stories counter to the way people navigate news, and the Innovation Report proposes a new set of tags -- terms like ``Timeliness,'' ``Story tone,'' and ``Story threads'' (such as ``crisis in Ukraine,'' which they admit would require them to ``Better organize our archives'').

\subsection{Linked tags}

But as Stijn Debrouwere points out, even ``tags don't cut it,'' as the title of his blog post says. ``Each story could function as part of a web of knowledge around a certain topic, but it doesn't.'' Tags -- which are often inconsistent, outdated, and stale -- provide the only window into content at the metadata level. ``The whole purpose of tags is to relate one piece of content to another,'' and given the dozens of ways that one can type ``George Bush,'' they can't even do that.

Debrouwere concludes that we need ``a way of indicating how content relates to other content on our website and on other websites that is more powerful and more expressive than tags.'' The reason tags don't cut it relates to the level of \emph{intersubjectivity} that newsrooms are dealing with.

Debrouwere suggests using vocabularies; set people, places, organizations, events and themes. He also advocates for relationships over tags, which borrow from semantic web principles to add detail to a link. ``A tag on an article says ``this article has something to do with this concept or thing.'' But what exactly?'' Rather than tagging an article ``Rupert Murdoch,'' a tag has more value if it can say ``criticizes Rupert Murdoch.'' Finally, Debrouwere advocates for ``entities, not labels.'' This point is the most important; ``we don't need the arbitrary distinction between a label and the thing it labels on a website. Let's unlock the full potential of our relationships by making them relationships between things.''

In short, as Debrouwere suggests, ``we have the ability to transform those rudimentary link dumps into valuable landing pages and content hubs that collect all the content for a person, an organization or an event.'' Relationship cascades, synonyms and homonyms. Debrouwere is suggesting returning a level of structure to the open, massively linked and networked web. Why would this be useful? Wasn't the web averse to strict taxonomies and structures?

The answer is a question of scope; when deciding what to do with archives, it is crucial to remember the archive's size, shape,and scope. At a massive scale like those of Twitter or Flickr, tags work well because there are still more than enough matches to go through. At an individual scale like someone's blog, tags can work because there are only a handful of tags and one person to tag them. But publishing archives are closer to the intersubjective scale, between the personal and universal. At this scope, tags can be a challenge.

At large scales, tagging resembles a form of \emph{collaborative filtering}, which looks to the properties of users, their preferences and behaviors, in order to categorize and ultimately recommend items to others. Tagging as publishers perform it, however, is not collaborative in the same way. When a user bookmarks and tags an article or photo on the web, he or she is doing so for personal reference; when an editor tags a news story to prime it for publication, it is for the article's discoverability. In the former case, tags could be ``Articles I read last night'' rather than the standard categorical tags.

Shirky likewise discusses when ontological classification works well, suggesting that despite the success of tags on the web, a small corpus with expert catalogers should be organized around a taxonomy. But the web -- with its large corpus, lack of categories, and amateur catalogers -- is not a good fit for ontologies. News archives fall in between these extremes, but as they are increasingly digitized, atomized, and networked, they are increasingly moving towards a weblike structure.

In order to prove worthwhile, newsroom archives must afford greater value than a simple Google or LexisNexis search. While these services provide text search, a well-linked archive can include images, videos, charts, maps, statistics, quotations, comments or annotations. The stories that result can be a combination of these, referencing an ecosystem of media that already link, reference, and embed one another. Publishers can also leverage smart entity recognition and linked data tools to aid in automatic and rich tagging. While newsroom librarians are increasingly disappearing, and publishers cannot upkeep ongoing stories in their archive, why is text search the only way to search for stories?

Debrouwere envisions a tagging system where a tag can double as a full card or widget, linked in turn to other cards and widgets in a network of knowledge. Coupled with more automated, dynamic, and context-aware tagging methods, well-structured news archives could become sustainable repositories of knowledge in their own right, turning publishers into information providers and authorities on the level of libraries and information technology companies. There is value in this well beyond charging a reader per-article for archive access.

\subsection{Promising starts}

Some initiatives and organizations are taking promising steps towards linking their archives. The New York Times R\&D lab recently released a tool called Madison that aims to crowdsource insight about the ads in old Times issues. Starting with the 1950s, the tool asks users questions about the ads that they see, with the aim of adding structured metadata to otherwise difficult-to-parse texts. The team even released the underlying crowdsource platform as an open-source project, allowing others to run their own crowdsourcing endeavors. This is a promising opportunity for publishers who have the scale and user base required for a crowdsourcing project.

% Could I ask the Times how that's going? If they have similar projects?
% Add some data about how it's progressing anyway

% Vox Chorus
Vox's Chorus platform is another promising endeavor in structuring archives. Beyond the sustainable mind to data that is Vox's card stacks, Chorus helps reporters better structure their stories, from adding smart tags to media and widgets.

% Can I find out more about Chorus? Can I even see how it works?...

% Photos
Some promising endeavors are coming from small starts. Take a collection of already-related stories and build nuanced links between them. Or hone in on a single aspect of a linked collection -- for instance, geographic data -- and aim to structure that first. Projects like MapCake -- which automatically creates maps out of structured location tags -- show how effective it can be to focus efforts on structuring one media type. Sites like National Geographic are likewise well primed for beautiful photo archives, which can be watermarked and travel with articles and social media posts. It makes sense for such sites to focus on structuring their photo archives first and foremost. Radio and podcast-oriented sites could do well to take advantage of services like PopUp Archive

Finally, there are many digital libraries beginning to offer resources and services; quotes, images, and videos. These have the potential to expand the borders of a news archive. The Digital Public Library of America, JSTOR, Flickr, PopUp Archive; all of these sites have useful resources for both reporters and readers. Many also have APIs that would allow easy integration with existing content discovery systems.

% Helmond: "Circulation of content opens up the boundaries of a website...distributed across various social media platforms.""

\subsection{Next steps}

% "Ankle-deep semantics" as Chakrabarti puts it

% Tentative expansion via APIs -- DPLA, Dbpedia/Freebase, NYTimes, PopUp Archive
