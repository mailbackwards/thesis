\chapter{Networking the News}

In the previous chapters, I have outlined the ways in which archives, and critical readings of them, have expanded from a fixed and graspable entity to a suite of interconnected parts, constantly shifting and adapting to new information. The web, when seen as an archive of archives, is itself ``an active and evolving repository of knowledge,'' rather than a fixed, bordered entity or set of categories.\autocite[2]{chakrabarti_mining_2003} This chapter hones in specifically on the structure of news stories and publishing archives, and the ways online publishers and legacy news outlets are treating their digital and digitized archives in this new era of continuous reclassification.

Although news has been called ``a first rough draft of history,'' in newsrooms, the archive is traditionally known as ``the morgue'': a place where stories go to die. But new technologies and conditions have led to many recent attempts to reanimate the news archive, and there seems to be an ``archive fever'' developing amongst news publishers. Nicole Levy wondered if 2014 is ``the year of the legacy media archive'' in a story about \emph{Time} magazine's archival ``Vault.''\autocite{levy_time.com_2014} She points to \emph{The Nation}'s ``back issues,'' \emph{The New Yorker}'s open archive collections, and the \emph{New York Times}' TimesMachine and @NYTArchives Twitter account as examples of old publishers endeavoring to use their rich histories to create something new. Back archives like Harper's and the National Geographic are held up as examples of combining rich content with historical context, improving credibility and brand recognition in the process.

\emph{The Times} closely examined its own archives in their celebrated \emph{Innovation} report of 2014, suggesting that a clever use of archives could revitalize new content by seamlessly integrating with historical context. ``Our rich archive offers one of our clearest advantages over new competitors\ldots[b]ut we rarely think to mine our archive, largely because we are so focused on news and new features,'' arguing that ``we can be both a daily newsletter and a library.''\autocite[28]{_innovation_2014} The report suggests that arts and culture content, more likely to be evergreen, could be organized ``more by relevance than by publication date,'' and the topic homepages should be more like guides than wires.\autocite[29-30]{_innovation_2014} The report goes on to enumerate successful experiments with repackaging old content in collections, organized by categories and themes. They suggest allowing users to create their collections of stories-- something that readers could also do without risk to the Times brand. By creating ``no new articles, only new packaging,'' the Times can easily give new life to old stories.\autocite[34]{_innovation_2014}

In 2014 we also saw another trend towards ``explainer journalism,'' and an intense focus on context provision for readers. Vox.com, the poster child for the explainer movement, aims ``to create a site that's as good at explaining the world as it is at reporting on it.''\autocite{klein_vox_2014} Explainer journalism aims to take a step back from the immediate news event and place it in a larger phenomenon, and it reflects a deep shift in the roles and practices of online journalists; as news is increasingly broken and scooped on social media, journalists are increasingly becoming summarizers, filterers, and context providers. News has traditionally been delivered in a stream format, full of boilerplate text that is repeated across every story related to a given theme. In the archive and explainer movements, we see a pattern among some news outlets attempting to evade and reconsider the news cycle's obsession with speed and feeds, instead experimenting with new forms of what a news story can be, and how it can connect to other stories within the archive and around the web.

As many publishers emphasize the potential value of archives and context for the future of digital journalism, this moment is rich for closely examining this connection. By looking at the challenges and methods in digitizing and structuring legacy media archives, we can gain a sense of how news stories are structured on a small scale, and how a collection of them creates context on a larger scale. This lets us think closely about the structure of online content, and the ways that news publishers can continuously keep their archives relevant and context at hand.

In this chapter I will start by examining the structure of news stories to grasp at the challenges specific to news organizations when archiving. I will then outline the stages of ``linking the archive'' for news publishers, and consider current efforts in legacy news and digital preservation. Finally, I will discuss the relationship between archives and explainer journalism, examining the convergence of the roles of journalist and developer in the newsroom and the corresponding shift in the function and practices of journalists.

\section{The structure of stories}

% Hypertext as semi-structured; APIs fully structured

% Also look at Skye Doherty's foray into design of hypernarratives in journalism

Newspaper and magazine publishers prove an ideal study for examining the potentials of hypertext archives. If we treat a newspaper as a proto-hypertextual document, it becomes apparent that online news might be a natural extension of reading the newspaper. Few readers go through a newspaper sequentially, paying equal attention to every article; instead the reader jumps around from page to page, skimming some sections for its raw information while reading longer pieces more deeply. A website homepage reads like a newspaper's front page, with snippets and teasers that aim to draw the reader deeper. A given page can hold several articles, and an interested reader might be distracted or intrigued by a ``related article'' next to the one he or she came to read. Some works are categorized into sections---arts, sports, letters to the editor---while others might be paired with a certain advertisement or reaction article. These examples point to the inherently interlinked nature of newspapers, and the endless potential for insightful metadata; newspapers might seem to naturally lend themselves to the digital world.

The pre-hypertextual newspaper started as a response to a sort of historical information overload; the newspaper frontpage and summary lead paragraph, both solidified in 1870, were part of a broader trend towards ``helping readers to economize their scarce time in scanning a paper.''\autocite[254]{starr_creation_2004} Larger type, illustrations, and bolder headlines drew criticism for trying to grab attention, but they also directed and focused attention to the major stories of the day, allowing for nonlinear readings of a newspaper as a fragmented collection of stories, headlines, or leads. A newspaper's layout and seriality therefore scaffold a pseudo-hypertextual structure, one that can be computationally mined for insights. Some libraries and cultural heritage institutions are leading these endeavors, such as Europeana and Trove.\autocite{europeana,trove}

But traditional newspapers have a major limitation: they cannot \emph{explicitly} link to other work in a structured and idiomatic way. Scholars have long relied on the footnote and bibliography to systematically track influence and dialogue, and networks of citations can be created out of them. % QUESTION: ARE THERE ANY DH PROJECTS TO REFERENCE HERE? Mapping citation networks in older documents? %
This forms the basis for citation analysis, or bibliometry, a practice with a long history and strong conventions that I will dive into more closely in the following chapter. Its essential principle is that the more an item is cited, the more influential and credible it is. The online version is known as ``webometrics,'' and it applies certain new standards which online newspapers can take advantage of, both in measuring impact on the web and inside their own archives. But citation is ``as old as written language itself,'' and it is \emph{itself} a language, with its own idioms, syntaxes and exceptions.\autocite[1]{chakrabarti_mining_2003} The footnote has its limitations, only linking back to the past---but newspapers don't even get footnotes.

The journalistic affordances that the web brings can be conceptually divided into a few core features, which Mark Deuze outlines as hypertextuality, multimediality, and interactivity.\autocite{deuze_web_2003} Peter Dahlgren adds a fourth and fifth: for him, media logic is also \emph{figurational} and \emph{archival}. Discussing archivality, he asserts that ``users of cyberspace for journalism are in principle no longer so bound to the present,'' and points to hypertextuality as enabling new, more usable archives.\autocite[66]{dahlgren_media_1996} While many projects have closely mapped the role of hypertextuality in online journalism---examining newsrooms' approaches towards hyperlinking through network analysis, surveys, interviews, and newsroom ethnography---there has been less research considering hypertextuality's influence on newsrooms' archival practices.

Hyperlinks allow for a new standard of citation, reference, and context provision for news. The link can even go beyond the footnote by linking in both directions, allowing readers to see who referenced the story; an old article in \emph{The New York Times}, for instance, can link out to more recent related Times articles, other publishers or blogs that picked up on the story, or conversations in the Times forum or on Twitter. Linking offers great potential, not only for enlivening the reading experience, but for creating a traceable dialogue that can improve a story's discoverability in the future. A number of search algorithms, such as Google's PageRank and Jon Kleinberg's HITS system create ``hyperlink-induced communities,''\autocite[12]{chakrabarti_mining_2003} between websites, and the same principles can be adopted an expanded \emph{within} news websites.  

% Where to include these studies of hypertextuality in newspapers? Note that there is a large conceptual difference between inlinks and outlinks (Deuze), Tremayne, Coddington, de Maeyer, Interviews, etc.

A human editor who is tagging a story is equivalent to the archivist in the library, attempting to predict every possible way that a user might search for the story in the future, whether it's ``Sports'' or ``Breaking'' or ``Opinion''---and editors don't have the extensive training and professional expertise that comes with being a librarian or archivist. Journalists are trained to explain, contextualize, and curate rather than structure and tag. Given the impossibility of explicitly and expertly tagging in advance for every possible present and future use, as well as the arbitrariness of tagging \emph{the story} instead of its constituent parts, we can turn to entities and links as supplements to categories and tags in the newsroom archive.

\subsection{Units of news}

In technical terms, stories are usually objects in a database that have associated text, images and tags. Stories contain multitudes, and a typical story might have a variety of metadata attached to it; authors, dates, versions, categories, images, events and collections it's a part of, tags, and so on.\footnote{Figure here with typical story objects; a sample database schema} While more metadata and structure requires more investment at the outset, smart use of such metadata prepares a story for archival reuse. Some of it will be useful for linking or embedding on the website, others for use in an API or application. Stories can include other stories as part of their metadata too, either related manually (by a hyperlink or a human editor) or automatically (via similarity algorithms that analyze the words or topics in the article, the communities it reaches, and so on).

The story has long been the basic unit of news, and so it tends to have a one-to-one relationship with the URL, the basic unit of the web. One section of the \emph{Times'} \emph{Innovation} report announces that they produce ``more than 300 URLs a day,'' using URL as a sort of ``thing'' word, their default unit of work.\autocite[27]{_innovation_2014} Most publishers will assign a ``canonical URL'' to a given story, which serves as its unique identifier, and often, practically speaking, it is the only information that a researcher or search engine can feasibly obtain about a particular document on the web.\footnote{While the researcher or bot crawler could, of course, request the webpage to get the information, each request can take several seconds and some processing power, so it becomes infeasible at a larger scale.} You can be sure to find the most canonical version at the canonical URL, but the article lives in various forms across the web.

But if stories contain multitudes, then why is the article the basic unit of information for news? An article can pull paragraphs from one source, photos and charts from another. It is an ecosystem of media itself, and it can contain other stories in turn. The news app Circa organizes its content around ``atoms'' of news: single facts, quotations, statistics, and images that can be reaggregated and remixed as needed. Systems like Vox and Circa aim to create a baseline repository to build upon rather than recreate from scratch every time.

This approach rethinks how we organize news items and structure stories. A ``story'' can be a collection or dialogue of items; indeed, most stories already are. A journalist can still create, but also curate, collect, and contextualize, or allow users to do the same. All of these remixes and reuses can improve the classification and discoverability of the content in turn. Thinking of a story as a collection or mash-up offers a new framework of a story as a highly linked entity, one that can start to organize itself.

As discussed in previous chapters, organizing by link and tag has often proven a more effective form of sorting things out on the web than has organizing by overarching taxonomy or ontology. It has been the lifeblood of Google, as its PageRank algorithm made it the dominant search engine over rivals like Yahoo! and HotBot.\autocite{shirky_ontology_2005} Yahoo! began in 1994 as a hierarchical directory of useful websites. This is a natural first step for an online search engine, since computer users have grown accustomed to the tree-like document and file structure pioneered by Douglas Engelbart and others, and replicated by Berners-Lee's domains and paths on the web. It also builds relationships between categories into its structure---parents, children, and siblings---which readily enables features like ``More like this.''

But Google succeeded by crawling in the weeds rather than commanding from on high. For Google, the links sort everything out. Tim Berners-Lee proved that networks could work with many links---and in fact, if you had a lot of links, as Clay Shirky argues, ``you don't need the hierarchy anymore. There is no shelf. There is no file system. The links alone are enough.''\autocite{shirky_ontology_2005}

Shirky and David Weinberger champion the tag as a hybrid hierarchical/networked organizational structure. On one hand, tagging relies on an individual singularly classifying an object under a certain discourse. On the other hand, users are generally free to tag as many times as they want, and using whatever scheme they desire. Tags could range from ``World War II'' to ``articles I want to read.'' Studies and businesses alike have proven that at web scale, even with users tagging items for personal and idiosyncratic reasons, distinct and simple patterns emerge that allow for collaborative classification.\autocite{cattuto_semiotic_2007} Such collaborative classification systems, sometimes called ``folksonomies,'' emerge as manifestations of the ``boundary infrastructures'' proposed by Bowker and Star in \emph{Sorting Things Out}.\autocite{bowker_sorting_2000}

Tags have their limitations; if another user tags an item ``World War 2,'' the system needs to recognize that it means the same thing as ``World War II,'' and publishers employ controlled vocabularies to avoid such ambiguities. Some research has shown that the first tags on an item are likely to influence future tags in turn, resulting in a sort of ontological groupthink.\autocite{cattuto_semiotic_2007} Still, whether a Flickr tag, a Delicious bookmark, or a Twitter hashtag, these crowdsourced approaches to tagging function as links between content; it is not about the tag itself, but the \emph{connection} being made to other content. Shirky even considers the possibility that syonymous terms aren't desirable; perhaps the people who are searching for ``films'' would be better served by just seeing results tagged as ``films,'' and not ``movies'' or ``cinema.''\autocite{shirky_ontology_2005} This suggests an even more direct reliance on language to sort things out. Still, sometimes hierarchies are crucial; a user searching for movies set in Massachusetts would also want movies tagged ``Boston.''

\emph{The New York Times} sees tagging as core to its business, calling it the primary reason that the Times has remained the ``paper of record'' for decades.\autocite[41]{_innovation_2014} This title was bestowed to them largely on the back of the legacy Times Index, which has offered an annual reference version of Times stories since 1913, still published in hard copy. There is no doubt that the Times' tagging practices have helped them remain a library, information hub, and general authority on contextual information. But the \emph{Innovation} Report sees them falling behind, adhering too much to the needs of the hard copy Index. They also note crucial limitations with identifying, splitting, and lumping categories; it took seven years for the Times to start tagging stories ``September 11.'' Their team of librarians is required to shepherd about 300 new articles a day into the archive, making sure to keep them discoverable under any context. Tags can concern more than just the contents or event of a story---the Report suggests tagging stories by timeliness, story tone, and larger ``story threads''---but they admit that many of the more exciting tagging potentials would require them to ``better organize our archives.''\autocite[41-42]{_innovation_2014}

So the linking of the archive can occur not only through explicit hyperlinks, but implicit tags and entities that reside within the stories themselves. Most news articles rely on tagging to be connected to other media; if \emph{The Boston Globe} writes a story about New England Patriots coach Bill Belichick, an editor might tag the story ``football'' in order to place it in dialogue with other football stories, landing on the Globe's football topic page, and so on. But this belies a more nuanced dialogue between the words, images, and hyperlinks used within the story itself; a short story that has ``Bill Belichick'' and ``Cincinnati Bengals'' in the text is likely to be referencing a recent game or a trade, while a longer story that brings up his family members or his hometown is likely to be a biographical piece about his life and upbringing. By combining natural language processing and entity linking tools, a story can be automatically and dynamically tagged according to the myriad possible contexts that a user might search for, whether she is looking for stories about football, the Patriots, or Bill Belichick himself.\footnote{explainer on NLP and linking?}

\subsection{From topics to tags to links}

While tagging practices can be enhanced in a variety of ways, news developer, researcher, and blogger Stijn Debrouwere thinks that even ``tags don't cut it.''\autocite{debrouwere_tags_2010} As an expert in news analytics and a co-creator of link-tracking platform NewsLynx, he knows well the limitations of the web and newsrooms' content management systems. His blog series ``Information architecture for news websites'' dives into the headaches that result when journalists think of stories as blobs of text in feeds and streams, rather than structured systems of facts that carry value in their own right.\autocite{debrouwere_information_2010}

He cites a famous blog post by 2006 by Adrian Holovaty, co-creator of the Django web framework and its now-retired ``benevolent dictator for life.'' Holovaty's essay revolves around one idea: ``newspapers need to stop the story-centric worldview.''\autocite{holovaty_fundamental_2006} Each story, he notes, contains a vast amount of structure that is being thrown away with every click of the ``publish'' button. He leads through several examples, such as: \begin{itemize}
\item An obituary is about a \emph{person}, involves \emph{dates} and \emph{funeral homes}.
\item A \emph{birth} has parents, a child (or children) and a date.
\item A \emph{college graduate} has a \emph{home state}, a \emph{home town}, a \emph{degree}, a \emph{major} and \emph{graduation year}.
\item A drink special has a \emph{day of the week} and is offered at a \emph{bar}.
\item A \emph{political advertisement} has a \emph{candidate}, a \emph{state}, a \emph{political party}, multiple \emph{issues}, \emph{characters}, \emph{cues}, \emph{music} and more.\end{itemize}

\noindent Holovaty links to context everywhere above, using hyperlinks to literally highlight the information that's otherwise locked away behind stories. Of course we don't need all of this context all the time, but we may really need \emph{some} of the context \emph{sometime}, and it's easier to structure it now than to unlock it later. The better structured this information, Holovaty argues, the more serendipity can foster new features and applications. Proper story scaffolding can lead to more happy accidents of ``wouldn't it be cool if\ldots'' later. Want to map the births and deaths of a famous family, or the happy hours in a neighborhood? You might already have that information buried in your stories.

Debrouwere expands on Holovaty, summarizing his frustration with tags: ``each story could function as part of a web of knowledge around a certain topic, but it doesn't.'' Tags are our only window into content at the level of a story's metadata (which, too often, is all we have). For all their weblike strengths, they are still often inconsistent, outdated, and stale. ``The whole purpose of tags is to relate one piece of content to another,'' and given the dozens of ways that one can type ``George Bush,'' they can't even do that.

Debrouwere concludes that we need ``a way of indicating how content relates to other content on our website and on other websites that is more powerful and more expressive than tags.'' He suggests using vocabularies: set people, places, organizations, events and themes. Knowledge bases like DBpedia, OpenCalais, OpenNLP, AlchemyAPI, or the Getty Vocabularies allow for deep context at low cost, basing its tagging on ``entities, not labels.'' He also advocates for indexing relationships rather than contents, which borrows from Semantic Web principles to add detail to a link. ``A tag on an article says `this article has something to do with this concept or thing.' But what exactly?'' Rather than tagging an article ``Rupert Murdoch,'' a tag has more value if it can say ``criticizes Rupert Murdoch.'' For Debrouwere, ``we don't need the arbitrary distinction between a label and the thing it labels on a website. Let's unlock the full potential of our relationships by making them relationships between things.''

Such a scheme could benefit an end user in many ways. Topic pages, such as New York Times' over 5000 pages ranging from ``A.C. Milan'' to ``Zimbabwe,'' could be smarter, reflecting the most popular articles or most related topics. Entity- and link-oriented schemes can create cascades of relationships, synonyms, and homonyms. Journalists as well as readers would gain improved access to their organization's history, improving the research, context, and tagging of future stories. Debrouwere is suggesting a return of structure to the open web; he envisions a tagging system where a tag can double as a card or widget, linked in turn to other cards and widgets in a network of knowledge. This could be extended to events and phenomena as well as proper names and entities; some emerging systems can recognize phrases like ``Barack Obama announced his candidacy for president'' and ground it as as a unique, unambiguous newsworthy event.\autocite{nothman_grounding_2013} While this research has a long way to go, it is a promising start towards extending the ``ankle-deep semantics'' that Chakrabarti advocates.\autocite[289]{chakrabarti_mining_2003}

This return of structure does not have to be a step backwards; instead of manually and unilaterally structuring from on high, we can focus on the structure built into the stories already. This solution doesn't replace stories, or require editors to exhaustively tag every component of a piece; it just automatically supplements a story with new metadata that gives its inherent information an afterlife. Every story---whether a blog post, a map, a listicle, or an interview---has its own structures and patterns.

One example comes from the Boston Globe's March 2015 coverage of the Boston Marathon bombing trial. Data editor Laura Amico knew well that trials are far from linear stories; since they are told by lawyers, they are a series of conflicting arguments. Trials are unique in providing two stories: the chronological narrative of the trial repieces the narrative of the original event. Amico and her team knew there was more than one way to tell this story, so they decided to focus on the facts; the witnesses, exhibits, and arguments are all entered into a spreadsheet, which can generate snippets of facts and entities---designed as cards---for later review.\autocite{mullin_how_2015} Each of these cards can embed and contain other cards, or be combined to form a story. Such a framework recognizes the interlinked nature of stories and takes advantage of the structure of an event (in this case, a criminal trial).

One of the most obvious, low-hanging and underexplored structures is the hyperlink. A year after publishing his ``Information architecture'' series, Debrouwere followed up by questioning many of his own allegiances; tags still don't cut it, but maybe taxonomies don't either. He realized: ``The best content recommendations on news websites are inside of the body copy: inline links. With recommendations, you never know what you're getting. It's mystery meat. With links, a writer tells you why she's pointing at something the moment she's pointing at it.'' It is better to draw on the connections from these links than to rely on automated recommendation engines to organize content. Journalists are better at explaining and contextualizing than they are at tagging and structuring, which are a librarian's craft. Debrouwere knows that newsroom developers are building for journalists, and he ends by asserting that he wants to build ``prosthetics, not machines.''\autocite{debrouwere_taxonomies_2011}

Another under-mined source of insight lies in the plethora of ``human-generated lists,'' as Google calls them, around the web.\autocite{franks_discovering_2012} Whether collecting articles, photos, books, songs, or tweets, people obsessively collect and curate, and some are known experts at doing so. These range from Amazon wish lists to mixed-media stories on Storify. Thinking of lists as links between contents, weighted by expertise, leads to interesting potentials. The title of the list, or its other metadata, could tell us more about the context behind the link; a list of local Mexican restaurants is linked by country and location, while a list of my favorite hip-hop albums of 2014 is linked by year, quality, and musical genre. The Times' \emph{Innovation} report suggests allowing users to create lists, since it could allow for deep interactivity without risk to their brand; such a system could use readers' collective wisdom by asking users to specify the context behind their lists.

These web-native and polyhierarchical approaches to classification reflect the growing need for newsrooms to find weblike ways to organize their content; automatic and dynamic tagging, linked entites, image recognition, and tapping into the knowledge of experts and crowds are some of the solutions. But despite the news story's rigid structure and the limitations of indexing, there is no reason to believe that the article is going away as the core unit of news. Link-based platforms and services like RSS and social media feeds still rely on stable and consistent resources at given URLs. Innovations in story structure could even be at odds with the very notion of revitalizing the archive. In aiming to blend the present with the past, archive-oriented publishers are bringing past conventions back into the present. Still, some new forays into interactive, multimedia, and app-driven journalism enhance or bypass the URL and hyperlink---I will touch on these at greater length in the conclusion. My aim is not to suggest that we restructure the news story; only that we rethink how they work under the hood. Stories are not uniform resources, and they should not be uniformly tagged and categorized.

\section{Stages of digital history}

% What I don't get into: what is the AUDIENCE for this archive? Most archival research has looked at its role for researchers; but what about casual interested browsers?

In 1997, John Pavlik suggested that there were three stages of development in online versions of newspapers. Keeping in mind this early date, Pavlik observed that newspapers' first stage online was to copy the print edition to a given website (a stage known and maligned as ``shovelware''), followed by supplementing the copy with interactive features (like hyperlinks or comments), then in the third and final stage, writing copy specifically for the online version of the story.\autocite{pavlik_future_1997} Twenty years later, it is safe to say that publishers have all reached the final stage. I aim to suggest a three-stage development of my own in the state of the digital legacy news archive, referring to each stage respectively as \emph{digitizing}, \emph{atomizing}, and \emph{linking}.

The first stage for any legacy publisher is to \textbf{\emph{digitize}} the archive. This tends to consist of scanning the pages of old publications, running OCR (optical character recognition) on each page, and exposing the results to a search interface for researchers and, perhaps, interested readers. It is a crucial first step for enlivening the archive, but a physical record can often limit the digital equivalent's potentials. Digital versions of physical articles often do not leverage links, images, and mixed media to the same effect. While a digital-native version of a print article might directly cite more sources or feature an intriguing interactive, these elements remain second-class citizens to the print article, which digital versions must remain faithful to. The Times' \emph{Innovation} Report argues that by modeling their website and apps on their print structure, the Times ``ask[s] too much of readers.''\autocite[26]{_innovation_2014} So it is crucial to remember at this stage that the digital archive has different potential from its physical counterpart. As many theorists and historians remind us, too, a paper's physical appearance and content are closely linked together, so simply ``digitizing'' and newspaper changes it massively, reshaping a great deal of context.\autocite[388-389]{mussell_elemental_2014,manoff_archive_2010} Richard Abel breaks down the promises and challenges in generating archival ``big data'' in research on 1910 US cinema. Using digital newspapers led to ``a wealth of unexpected documents,'' but he notes the unreliability of completeness and searchability, and the collapse of community.\autocite{abel_pleasures_2013}

Given the print newspaper's proto-hypertextual status, it presents a unique metadata challenge for archivists. Paul Gooding, a researcher at University College London, sees digitized newspapers as ripe for analysis due to their irregular size and their seriality.\autocite{gooding_exploring_2014} In order to learn more about how people use digitized newspaper archives, Gooding analyzed user web logs from Welsh Newspapers Online, a newspaper portal maintained by the National Library of Wales, hoping to gain insight from users' behavior. He found that most researchers were not closely reading the newspapers page by page, but instead searching and browsing at a high level before diving into particular pages. He sees this behavior as an accelerated version of the way people browse through physical archives---when faced with boxes of archived newspapers, most researchers do not flip through pages, but instead skip through reams of them before delving in. So while digital newspapers do not replace the physical archive, they do mostly mimic the physical experience of diving into an archive. Still, something is lost when the physical copy becomes digital; the grain of history---the old rip, annotation, or coffee stain---is reduced to information.

It might go without saying, but it is also crucial to back up the archive. More and more evidence shows that digital content could have a shorter shelf-life than tapes, film, or other analog media.\autocite{sample_google_2015} It is important to diversify the format of the archived material as well. The Missouri School of Journalism's \emph{Columbia Missourian}, lost 15 years of stories and seven years of images in a single server crash.\autocite{sillesen_minus_2014} For Missouri's Reynolds Journalism Institute, ``News archives serve as a form of institutional knowledge allowing newer staff members to understand and convey the historical context of their stories to the readership. It is difficult to calculate the full value of news archives given the countless hours of reporting and editing they distill, not to mention the treasure they represent in terms of their community's cultural heritage.''\autocite{mccain_saving_2014} Obsolete backup formats only compound the problem. Some experts are pessimistic, suggesting that the sheer cost of maintaining the records is not worth the benefit.

In response to the server crash, the Reynolds Institute led a survey of 476 news websites, finding that found that 88--93 percent of them highly value their archives, but about a quarter of them had lost significant portions of their archive due to technical failure. Without these full archives, as Tom Warhover says, ``You can't offer up a comprehensive product to sell---your archives---if they aren't complete. You can't be sure you've really vetted a candidate for school board or city council. You can't find those historical pieces from events that now are historic and worth reporting again on anniversaries.''\autocite{mccain_saving_2014} Those publishers who have digitized their archives have already made the pledge to preserve and organize, and should be careful to preserve and protect their archive from obsolescence.

News archives form a symbiotic relationship with data-driven news apps, whether they exist for a specific event (think World Cup coverage or election results), or exist as standalone and evergreen frameworks (such as the HomicideWatch, Syria Deeply, or Timelines app). Such apps bypass the URL, but in the process they have their own challenges when saving and archiving. Scott Klein, luminary of news apps at the \emph{Times}, brings up Adrian Holovaty's ChicagoCrime.org, which Holovaty described as ``one of the original map mashups, combining crime data from the Chicago Police Department with Google Maps.''\autocite{holovaty_memory_2008} It is now defunct and unreachable in its original form; while the data survives, we have lost the presentation, and more importantly, the work, process, and context behind it.

There's no doubt that software constantly races against obsolescence, and some apps must be retired when their event passes or their function is done. But the lost process and context is lost knowledge. In March 2014, a group of NICAR conference attendees gathered at the Newseum to brainstorm the challenges and potentials of preserving news apps, suggesting more collaboration with libraries, museums, and cultural heritage institutions.\autocite{_opennews/hackdays/archive_????} Some such institutions are offering novel ways of preserving and maintaining digital work for the future. At the Cooper--Hewitt Museum, a team led by Seb Chan has been preserving the previously for-profit Planetary app as ``a living object.'' For the Cooper--Hewitt, preserving an app is more like running a zoo than a museum, where ``open sourcing the code is akin to a panda breeding program.''\autocite{chan_planetary:_2013} They'll preserve the original, but also shepherd the open-source continuation of app development, thereby protecting its offspring. While the Cooper--Hewitt is currently guarding Silicon Valley technology, overlaps and partnerships between newspapers and cultural heritage institutions could lead to similar experiments.

Some publishers have thrown up their hands altogether, relying on third-party services to organize and provide access to their own archives.\autocite{romenesko_u.s._2014} Reporters might suddenly see old stories disappeared, locked away behind services like LexisNexis. Such services provide fast and effective text search at low cost; but at what cost to an organization's brand, legal rights, and sense of history? A digital story is not just text, and increasingly, an archive includes images, videos, charts, maps, interactives, facts, statistics, quotations, comments, and annotations. Newer forms of classification can take a more holistic view of media, allowing a researcher to browse through text, image, sound, and video alike, and bypass the language limitations of search. This will become increasingly important as media evolves in a ``post-text'' web.\autocite{salmon_why_2014} Although newsroom librarians are increasingly disappearing and publishers are seeing a proliferation of new media enter their archive, the next generation of media companies cannot rely alone on text search to access their past.

The second stage is to \textbf{\emph{atomize}} the archive, or to break these scanned pages into their consituent parts. But what metadata is worth saving: the text, the subtext, the pictures? The photo or pullquote on the side? Is the image in the center of the page associated with the article on the left, the right, or both?

Newspapers are rich archival documents, because they store both ephemera and history. Journalists sometimes divide these types of news into ``stock'' and ``flow''; the constant stream of information built for \emph{right now}, versus the durable, evergreen stuff, built to stand the test of time.\autocite{sloan_stock_2010} Newspapers also have advertisements, classifieds, stock quotes, and weather diagrams. Many researchers rely on such ephemera---James Mussell calls it ``a key instrument of cultural memory''---so from the archivist's perspective, everything needs to be stored.\autocite{mussell_passing_2012} But historians might treat or navigate through ephemera differently, and each set of documents could have its own metadata or interface as a result.

A newspaper is a very complex design object with specific archival affordances; their irregular size, seriality, and great care in page placement make them ripe for unique forms of automated analysis. For some researchers, placement will be important (was an article's headline on the first page? Above or below the fold? Was there an image, or a counterpoint article next to it?). Others could be examining the newspaper itself over time, rather than the contents within (for instance, did a paper's writing style or ad placement change over the course of a decade?) Still others may be hoping to deep-dive into a particular story across various journals. In each case, we can glean information from where and when it was published on the page.

The project of atomizing the archive should take advantage of the signals built into newspapers in the first place, accumulating metadata from its size, shape, and context. An atomized archive should also provide a solid interface for viewing the original in its context. When legacy news publishers refer to a ``linked'' record in a digital archive, they are refering to this ability to see the original source page. Some publishers do not even have linked records for their entire archive, which makes context difficult to grasp for interested researchers. Even legacy publishers with atomized archives often have clunky ways of accessing them. Small usability issues add up, and lead to lower use of archives. Atomizing the archive is not helpful without a good interface.

It is telling that many of the digitization projects, begun decades ago, focused exclusively on salvaging the text. This ignores substantial information in the archive, of course, and speaks to the shortsightedness of many projects aimed at digitizing the past. Images, advertisements, maps, formatting, and related metadata were all lost, and many of them are being re-scanned by publishers, at great expense, in order to properly atomize the archive; to capture the details that they ignored years ago. Nicole Maurantonio criticizes old newspapers for ignoring the visual in favor of text, ``propelling scholars down a misguided path.''\autocite[90]{maurantonio_archiving_2014} Keith Greenwood finds that newspapers diligently archived their photographs for daily newspaper use, but did not tag items with public historical value in mind, rendering many of them useless as historical records.\autocite{greenwood_digital_2011}

Historical images are one of the greatest potential sources of engagement and revenue for news archives, and it would be relatively easy for some news archives to sell old photographs with historic value.\footnote{someone's gotta be doing this already, find them} Some metadata projects in the publishing world are aiming specifically at images, like the New York Times' \emph{Madison} project, which hopes to crowdsource insight about 1950s \emph{Times} advertisements.\footnote{Madison can be found at http://madison.nytimes.com/.} Outside the publishing sphere, Kalev Leetaru took an image-centric approach to the Internet Archive. The Internet Archive's OCR software threw out images, and Leetaru's would save whatever it threw out as an image file. He has since put 2.6 million of these Internet Archive images onto Flickr for open use. ``They have been focusing on the books as a collection of words,'' he told the BBC. ``This inverts that.''\autocite{kelion_millions_2014} Newspaper and journal images provide a richer glimpse of history, and one that might prove more engaging to digital readers than dated text. You get a sense of the visual language and associations of the time; as any visual critic or cultural studies scholar can tell you, photos and advertisements provide a revealing window into the patterns and contingencies of culture and history.\autocite{sontag_photography_1977,barthes_rhetoric_1978}

The final stage is to \textbf{\emph{link}} the archive, which when considered on a massive scale, is a quixotic endeavor along the lines of the Radiated Library or Project Xanadu. We can't predict all possible links between every possible piece of content. But linking the archive requires learning from the explicit and implicit references that already reside in the stories. This combines manual and automatic means, supplementing dedicated search in the surfacing of archival content, and using a ``push'' rather than ``pull'' method for finding archival materials. A user doesn't always know exactly what he or she wants, and a linked archive can work with a user to surface it. If the archive doesn't have the resource a user needs, could it at least point the user in the right direction? Could it interface with other knowledge bases to retrieve the answer?

The linked archive borrows from, but is distinct from the notion ``link journalism'' or ``networked journalism.'' As a term popularized by Jeff Jarvis to refer to the growing citizen journalism movement, networked journalism has also led to Jarvis's succinct motto of ``Cover what you do best, link to the rest.''\autocite{jarvis_new_2007,jarvis_networked_2006}. Building on this notion, Charlie Beckett posits that linking between sources leads to editorial diversity, connectivity and interactivity, and relevance.\autocite{beckett_editorial_2010} A networked archive turns the conversation inward; as Mark Deuze and others note, links that point inward are vastly different from those that point out, but they can adhere to the same principles of diversity, connectivity, and relevance.

It is unhelpful to have a massive, borderless archive, but linked archives can expand their borders strategically through clever use of APIs. As Anne Helmond's ``Boundaries of a website'' and the Open Knowledge Foundation's ``The News Reads Us'' project remind us, publishing websites rarely operate alone; they rely on third-party platforms and services for analytics, sharing, commenting, and recommendations.\autocite{helmond_exploring_2013,wehrmeyer_news_????} One could similarly integrate with APIs that offer archival resources from around the web. If a user is coming to a publisher's search bar instead of Google, it's because they want more context than a mere list or index of items. They want less containment and more connection. A user should be able to see response articles, comments, tweets, timelines, images and videos, from around the web (as long as these are visually separate from the main content to avoid confusion). Otherwise, users will continue to go to Google and Wikipedia for information.

A publisher's archival search interface could include results from Wikipedia, Creative Commons images from Flickr, or resources from digital libraries like Europaeana and the Digital Public Library of America---not to mention partnering with other organizations to merge archive rights, or at least indices. The linked archive is therefore intricately indexed on a small scale, but also effectively connected on a large scale, seamlessly interfacing with other archives and collections around the web.

% \section{Context in context}

% Much of the debate surrounding archives and explainers strikes at the heart of the role of journalism, both on the web and as an industry and institution. Michael Schudson frames the rise of popular newspapers in the 1890s as a pull between two journalisms, one telling ``stories'' and the other providing ``information.''\autocite{schudson} Which of these functions is the primary goal for a news outlet? This question remains critical today, with most news operating on a sort of continuum between these points. Information focuses on the facts, while stories serve an ``aesthetic'' function. But in general, a faster-paced breaking event (such as occurring over an AP wire or a Twitter feed) will result in information, while contextualized longer-form histories and interactive multimedia (like a \emph{New York Times} interactive or \emph{New Yorker} longread) skew towards stories.

% Interactive digital multimedia emerges somewhere between these scales, providing information that allows users themselves to create stories. Interactives carry the promise of deep contextual information along with narrative elements, but given the extremely time-consuming process of building an interactive piece, they tend to accompany events planned far in advance and guaranteed to drive traffic; you're more likely to see an Oscar's, World Cup, or presidential primary interactive feature than rich multimedia surrounding breaking news such as a plane crash or terrorist attack. While both breaking news and deep interactives seemingly exist to provide information, the interactives offer a  proliferation of stories to go with it.

% Such interactive elements are rich with the potential for re-use. Some of the pieces stay relatively ``evergreen'' in both content and structure, proving useful months or years later. They also incorporate collections of media that, taken together, form a network or ecosystem of resources that allow for a new view into digital archives, focused on what media you're utilizing and citing as much as what words are on the page, or what desk you're reporting from. As such, deeply linked multimedia stories can better structure an archive; but can the depth and immersion found in explainer sites scale to work with breaking news and ongoing scoops as well?

% \subsection{Paywalls}

% While many traditional media publications are recognizing the potential role of archives in their shift to digital, they have tended to silo away their archival resources behind paywalls. This is the most literal interpretation of gaining value from one's archive, and it is an understandable choice given the few avenues for them to make money on the web. But paywalls are just one of many possible ways to extract value from archives, and there's more than one way to make a paywall.

% One problem with the way that publishers deal with their paywall now is that they have few ways of measuring ``successful'' archive diving. Like many publishers, Time.com requires a paid subscription in order to access their online Vault. But say a registered user is clicking through the Time Vault. Did she sign up for the subscription just to see the old issues? Is she there on a dedicated research project, or is she just browsing?

% Archive paywalls tend to be very limiting; while publishers will occasionally lift the paywall on relevant archival stories, it is usually impossible to even preview an archived story. This prevents interested users from even testing out the interface or getting curious in the archive in the first place. As online publishers experiment with paywall methods, they should keep in mind their older stories.

% Some archives are more open than others. The New Yorker opened its archives for the summer of 2014, free of charge, as they built the paywall system. The archive is now back behind walls, but the summer experiment seemed interesting. How much were users diving into the archives? How were they browsing the archives--through search or serendipity? Did opening the archives encourage people to think and write more about the New Yorker's rich history?

% % Can I talk to the new yorker abt their experience and write about it here?

% A publisher's archive will often turn up in specific articles geared towards history. At Time.com's Vault, editor/curator Lily Rothman digs out stories and quotes from the history of Time, ranging from historical interests (``Read TIME's Original Review of \emph{The Catcher in the Rye}'') to ephemeral oddities (``13 Weirdly Morbid Vintage News Stories''). Time assistant managing editor Samuel Jacobs likened Rothman to a radio D.J., highlighting singles from the archive to entice readers to pay for the whole collection.\autocite{}

% Other historic deep dives might occur in weekly columns or ``long history'' forays by individual journalists. A Sunday Times article, for instance, might take a historic look at a particular person, neighborhood, or community. These projects have a chance to draw attention to the past through curation as well, by drawing out and resurfacing old stories, photographs and statistics.

% % Find a few examples here. Local interest stories etc. e.g. history of Whitey Bulgur, Boston's bids for the Olympics, or neighborhood deep-dives.

% These projects are a promising start, but they tend to be isolated endeavors, relegated to a single story. Sometimes willfully nostalgic, they can carry an air of ``eating your news vegetables.'' They do not bring the archive fully into dialogue with ongoing events, whether in the research or design process. Journalists don't have easy, seamless access to their publication's past knowledge and institutional memory. Topic pages suffer from lack of organization and explanation. Readers cannot easily dive into old content.

% More generally, news' perpetual focus on the \emph{new} and \emph{now} keeps news publishers in the mindset of serving as information providers rather than knowledge repositories. This results in news delivered in story format, through wires, feeds, and streams. These are one-dimensional channels and metaphors, delivering information in a straight line and single direction. But information is more understandable, sustainable and useful if it's in more than one dimension, structured as a tree, rhizome, or web. Internal news archives have the potential to provide richer, faster resources and connections than the web as a whole, or its indexers (like Google or LexisNexis). This can drive traffic back to the site in turn, and bolster the publisher's authority as an information source. Legacy news publishers are rightly proud of their long histories of stories, photos, and notes; they could do well to show it off rather than hide it away behind paywalls. For publishers, opening access to their past could contribute to solidifying their status in the future.

% \subsection{The Scoop Effect} % Dear lord find a better term for this

% The time is ripe for news and history -- content and context, feeds and archive -- to collide. News outlets have long obsessed over the ``scoop'', being the first to break a story, and indeed these breaking stories still drive a great deal of traffic. But publishers are increasingly scooped in turn; stories break immediately on social media, rather than the next morning in the newspaper. % Can I coin a neologism for this? The "break effect" or something? So that I can keep referring to it in the next couple pages without driving myself crazy
% Emily Bell argues that social media and ``these super platforms ARE the free press, taking over many of the functions of the mainstream media. Social networks are now attracting the same pressures and challenges at a much larger scale that journalism and civic media has wrestled with for years.''
% This is having a profound effect, of course, on publishers; where does the media fit in here? But it is especially affecting the research process, skills and news lifecycle for both journalists and editors/strategists.

% For journalists, it has increasingly destroyed the stereotypical image of the reporter with a notepad in city hall. The increasingly real-time nature of scooping has led to reporters scouring Twitter as much as being in the field; even communication with sources increasingly occurs via email or tweet. The increasing presence of ``explainer'' and ``data journalism'' likewise speaks to this need; reporters must to wade through massive amounts of information in fast, efficient ways in order to uncover possible news stories, which requires very strong digital research skills. These are skills that librarians have been practicing for centuries, and a well-organized and linked archive can help reporters immensely with this research process. The data journalists thus emerges as an amalgam of reporter and librarian.

% For editors and newsroom strategists, it has shifted the role of the journalist and the news publisher to explainer, data-gatherer, and context-provider. Picture a newsworthy event occurring as the epicenter, and the reporting that occurs around it as a set of concentric circles around the event. Towards the center, one might find tweets, wire reports, and quick announcements. At the edge, there are longform pieces, explainers, multimedia work and data-oriented stories that help draw immediate events into larger phenomena. While the scoop remains crucial and breaking news draws traffic, news outlets can no longer serve as raw information providers, with no context. For a publisher to stand out, it is crucial to bring ongoing stories into a larger dialogue and conversation.

% % Coddington 2014 WikiLeaks article. Also articles from Ethan's class. "Sense-making" rather than "information-gathering"
% % Also Boczkowski 2010 talks about recycling content ("News at Work" book)

% \subsection{Explainers}

% This focus is not limited to legacy media, as the rise of ``explainer journalism'' and context-based reporting emerges as the other side of this coin. \emph{The Nation}'s editor and publisher Katrina vanden Heuvel suggests that ``a clever use of archives is kind of an explainer 2.0.''\autocite{} The goal is to provide knowledge, not news.

% The concept of explaining the news is not new. A 2001 Pew Center survey of newspaper editors concluded that they wanted to be ``news explainers'' first and foremost, ahead of ``news breakers'' or ``investigative watchdogs.'' But in a 2008 article called ``National Explainer,'' Jay Rosen accused editors of not staying true to their mission: journalists are not good at explaining the news and providing context. Instead, they focus too much on incremental and episodic updates, many of which go over the head of readers who haven't been following. Rosen likens the process to pushing software updates to a computer that doesn't have the software installed.

% Rosen argues that while journalists are paid to report the news and not explain it, they should also be giving background and context to larger stories. Journalists ``dont do a very good job of talking about the beginning and what got us to this point where it became news,'' according to Alex Blumburg of \emph{This American Life}. Even the occasional explainer that gets it right ends up in the flow of the same old information; Rosen argues that explainers like David Leonhardt's credit crisis piece in the New York Times ``should have been a tool in the sidebar of every news story the Times did about the mortgage mess.'' The little ``what's this?'' link is ``not about web design. That's a whole new category in journalism that I fear we do not understand at all.''

% Rosen also points out that such explainers are helpful for other reporters as well as the public, influencing news and information flow across the pipeline. A Times explainer, for instance, can reach a reporter who is informed by it as he or she interviews local officials. Calling it a ``scaffold of understanding,'' Rosen suggests that we ``start with clueless journalists'' in the path towards providing context, and went on to create explainthis.org, for people to admit what they don't know to journalists who are ``standing by.''\autocite{rosen_2008}

% Explainthis.org, now defunct, was like a library reference desk, staffed by the public and monitored by journalists. A peer of StackOverflow and ancestor to Quora, it is organized around questions rather than topics, discussed by the public and monitored by journalists. It requires someone to be curious enough to ask the question, however. Rosen touts the ability of explainers to generate interest in a topic, but here we're already expected to be interested.

% At a South by Southwest panel in 2010 called ``Future of Context,'' Rosen outlined the reasons explanation is needed and why it wasn't taking off. He cited both design and institutional problems; the prestige and real-time excitement of breaking (rather than explaining) news, as well as the explainer format getting lost in the shuffle of other news items.\autocite{rosen_2010} Metrics like clicking, watching, and even spending time on a site are not measuring the level of understanding or knowledge gained.

% The panel opened with NPR's Matt Thompson, owner of former contextual news blog newsless.org, arguing that we need more ``systemic information, not episodic info.'' Systemic information could include lists, charts, and maps that stay valuable well after the episodic news is irrelevant. Tristan Harris of Apture says, ``my background is computer science. You never do work that you can't re-use.'' He suggested an ``object-oriented'' approach to journalism with an eye towards sustainable, continuously updating tools and widgets that keep a reader informed. News is organized around stories rather than objects, resulting in streams rather than systems. A systemic, object-oriented approach to news places the context in the center.

% The panel concluded with Harris suggesting a wiki-like approach to journalism, which a big news organization like the New York Times would have the power to sustain. When Kramer asked ``how is this more than links?'' Thompson replied ``Links can be part of it.'' So can wikis, embeds, collections, and related articles.


% ``The context should be the foundation. The systemic stuff should be what you can access first. The episodic stuff is what should be the more info. We ghettoize topics pages on our sites, by creating a topics section. When the public just finds just a random collection of links on a so-called topics page, the quest for context everywhere is set back, Thompson argues. What would a site look like if it were structured around systems instead of stories?

% Journalists may think, were doing so much and now you want to provide context!? Think like an engineer. Make it an imperative to do work you can re-use to provide context. You can use that subduction plates info graphic again and again with every story you write about earthquakes. Its redefining the notion of today value. Youre writing something TODAY thats only appending something thats already valuable. Engineers dont do work they cant re-use. Do work you can use next time.''

% Chuck Peters, CEO of the Cedar Rapids Gazette:

% ``I cant see providing that context without changing how we create information in the first instance. Any factual element (photo, incident, quote, data, etc.) can be relevant to numerous contextual narratives. So each of those elements needs to both stand on its own and be tagged with as many potential relationships as possible\ldots We usually create information today in locked-down packaged articles, which block the easy flow of the elements between and among narratives.''

% Finally, in an article entitled ``Swimming lessons for journalists,'' PBS's Amy Gahran asserts, ``todays journalists can---and probably should---consciously shift away from jobs that revolve around content creation (producing packaged `stories') and toward providing layers of journalistic insight and context on top of content created by others (including public information).''

% % Criticize explainers a bit. ``Everything you need to know about x'' sounds final; it runs counter to the very idea that knowledge is infinite, interlinked, and flexible/elastic/malleable. It might not be everything you need to know tomorrow. It might be more or less than you need to know depending on what you need. http://pando.com/2015/01/20/how-rap-genius-and-explainer-sites-are-killing-music-journalism/
% % It turns out that smart, easy-to-digest journalism doesnt lend itself so easily to the fast production cycles of Internet media, which requires writers with little specialization to quickly educate themselves on a topic using Wikipedia or other secondary sources before regurgitating it back to the public. % "It keeps the reader at arm's length as an outsider looking in"

% % Problem with constantly-updating news: it might make it feel less consequential to publish something erroneous. How do you trust something without an official publish date?
% % Explaining is different from aggregating; and it can seem final and complete, the opposite of what's intended here

% \subsection{Vox}

% At the start of 2014, Ezra Klein left his position at the head of Washington Post's Wonkblog to start Vox, a news site that aimed to make context a first-class citizen of web journalism. Vox's mission: ``to create a site that's as good at explaining the world as it is at reporting on it.''\autocite{} Vox hopes to take a step back from the immediate news event and place it in a larger phenomenon. Taking the long view on stories also gives them an eye towards sustainability; Vox's topics are built around what they call ``card stacks.''\footnote{See http://www.vox.com/cardstacks.} Cards have titles like ``Everything you need to know about marijuana legalization,'' or ``9 facts about the Eurozone crisis,'' and each card is divided into question-driven subsections like ``What is marijuana decriminalization?'' Readers can navigate sequentially, or dive from question to question, going through Vox's explanations and photos. The final option is always the same: the ``Explore'' button takes the reader back to the top of the stacks.

% Vox's card stacks house a growing and morphing repository of knowledge. They are a public archive, like a Wiki but with more authorship intact. At the end of each card, Vox offers a link to email the author/curator of the card stack. For Vox reporters, starting a stack is also a pledge to maintain it. Vox also give a summary of changes made to the card (full versioning, they say, is coming soon).

% The goal is not to replicate Wikipedia, but more like a wiki ``written by one person with a little attitude,'' as Vox co-founder Melissa Bell put it. It's obeying the rules of journalism rather than ``no original research.''  Klein has Wikipedia in his sights, suggesting in the New Yorker that ``I think it's weird that the news cedes so much ground to Wikipedia. That isn't true in other informational sectors.'' By combining incremental news with an evolving repository, Klein hopes to gain the best of both worlds: ``the card stacks add value to the news coverage. And the news coverage creates curiosity that leads people to the card stacks.'' This follows Rosen's idea that explaining the news can generate future interest in incremental updates. For Klein, ``the biggest source of waste is everything the journalist has written before today.''\autocite{nyt_vox_melding} 

% For Klein, there is a distinct need, like Rosen saw, for a website that takes a step back and explains the news. In his words, ``The more folks in the media feel like it's beneath them to answer questions like ``What is marijuana?'' or ``What is Ukraine?'' the more we don't have to compete with them.''

% Vox has accompanied other ``explainer'' and data-focused websites, like Nate Silver's FiveThirtyEight, and The New York Times' The Upshot. Soon after Vox's launch, Craig Silverman wrote ``Why Vox (and other news orgs) could use a librarian,'' suggesting that Vox had ``a huge challenge, due to the rapid decay of facts.''.\autocite{}  Some of these facts may not even be obviously newsworthy, such as if an academic research paper changes a fact in an explainer on Alzheimer's care. Who is going to keep everything up to date? ``Someone at Vox is going to need to know which card stacks to update when,'' and how to keep the explainers updated with minimal maintenance.

% % Evergreen experiment at Vox http://www.vox.com/2015/1/15/7546877/evergreen-experiment

% % How to tie data journalism in with explainer journalism? With multimedia/interactives?

% % The ``tabloidization'' of news a la Emily Bell: "The demands of web scale economics have torpedoed the local news model". Can explainers and deep archive dives help bring local interest back to news reporting? How can news services provide local info too?

% % Herbert Simon "attention economy" in 1971 -- tied to Xeroxing. His concerns then are not new (information overload, attention economy)...but has the web's default state of copying exacerbated his concerns?

% \subsection{News Libraries}

% While someone needs to maintain all of these card stacks, it may not be a librarian. There are fewer and fewer librarians in newsrooms, which places their responsibilities increasingly on the reporter instead. Amy Disch, chair of the Special Libraries Association News Division, speaks to the traditional division of skills between reporter and librarian in the newsroom: ``We can find the information in a lot less time because we know how to drill down in a database. We know good sources to go to where you can quickly find information, so we can cut a lot of time for [reporters] and leave them to do what they do best, which is interviewing and writing. I have my specialty, and they have theirs.''\autocite{}

% Most legacy newsrooms have a library, but their librarians are ``a dying breed,'' with librarians getting laid off from a variety of institutions after the recession. Over 250 news librarians lost their jobs in the U.S. from 2007 to 2010, and membership in the Special Libraries Association News Division has steadily dwindled. Some news libraries and research centers have been completely shut down, outsourced to vendors like LexisNexis.\autocite{} Not only do their reporters' research abilities suffer, they cease to be a steady provider of useful and updated information for readers.

% At a 2001 summit on news libraries, futurist Arthur Harkins suggested that in order to stay relevant, news librarians should ``leave the information management functions to automation'' and instead focus on ``the ability to put knowledge into context and to synthesize information.'' The librarians focused on solutions like structuring incoming stories, helping merge mixed media operations and create new revenue opportunities from older assets. teaching journalists the necessary research and technical skills. Finally, the librarian's task is largely to tag; to structure stories for future discoverability and reuse, by both journalists and the public. Some news libraries, like the Boston Globe, used to manually tag their stories, but no longer do.

% Today, many of these skills are expected of new journalists at the outset. Such reporters arrive armed with years of internet research skills, though some of it with Google over specialized databases. Leslie Norman, former librarian at the Wall Street Journal, suggested, ``I see the news library as it once existed as probably dying, but in many newspapers, it's evolved into something else.''

% Although news libraries are a dying breed, some libraries and cultural heritage organizations are making promising digital inroads into news. Old newspapers provide a rich archive of both historic resources and incidental ephemera like sports scores, weather reports, advertisements and small human interest stories. This gives historians a glimpse of a day, with the major phenomena of the day mixed in with everyday events.

% Most of these projects are aimed towards the serious researcher, but they also point towards ways to engage casual browsers and fans of history. The National Library of Australia's Trove collection features 370 million resources; primarily, Australian newspapers ranging from 1803 to 1954.\footnote{http://help.nla.gov.au/trove/building-with-trove/api} Their API allows programmatic access, which in turn leads to the TroveNewsBot, an irreverent Twitter bot that can search the collection and yield a personalized result. Similarly, the Digital Public Library of America's DPLA Bot and British Library's Mechanical Curator both post random resources from their collection, aiming to inject a serendipitous sense of the past into the present.

% Newspapers would do well to merge increasingly with digital cultural heritage institutions and library APIs. 

% or they argue that ``the software newsrooms have adopted in the digital age has too often reinforced a workflow built around the old medium.''

% \subsection{Promising starts}

% Some initiatives and organizations are taking promising steps towards linking their archives. The New York Times R\&D lab recently released a tool called Madison that aims to crowdsource insight about the ads in old Times issues. Starting with the 1950s, the tool asks users questions about the ads that they see, with the aim of adding structured metadata to otherwise difficult-to-parse texts. The team even released the underlying crowdsource platform as an open-source project, allowing others to run their own crowdsourcing endeavors. This is a promising opportunity for publishers who have the scale and user base required for a crowdsourcing project.m

% % Could I ask the Times how that's going? If they have similar projects?
% % Add some data about how it's progressing anyway

% % Vox Chorus
% Vox's Chorus platform is another promising endeavor in structuring archives. Beyond the sustainable mind to data that is Vox's card stacks, Chorus helps reporters better structure their stories, from adding smart tags to media and widgets.

% % Can I find out more about Chorus? Can I even see how it works?...

% % Photos
% Some promising endeavors are coming from small starts. Take a collection of already-related stories and build nuanced links between them. Or hone in on a single aspect of a linked collection -- for instance, geographic data -- and aim to structure that first. Projects like MapCake -- which automatically creates maps out of structured location tags -- show how effective it can be to focus efforts on structuring one media type. Sites like National Geographic are likewise well primed for beautiful photo archives, which can be watermarked and travel with articles and social media posts. It makes sense for such sites to focus on structuring their photo archives first and foremost. Radio and podcast-oriented sites could do well to take advantage of services like PopUp Archive

% Finally, there are many digital libraries beginning to offer resources and services; quotes, images, and videos. These have the potential to expand the borders of a news archive. The Digital Public Library of America, JSTOR, Flickr, PopUp Archive; all of these sites have useful resources for both reporters and readers. Many also have APIs that would allow easy integration with existing content discovery systems.


% Trends happen in cycles, and the news follows; the news is often repeating itself. Even something seemingly new like data journalism is a holdover of ``precision journalism,'' something that few news stories explaining the trend will point a reader to. Sometimes old documents, whether leaked or declassified, can refuel an old story and paint it in a new light. Other times, new cultural events will conjure up the old; when \emph{12 Years a Slave} was released, the New York Times unearthed a story about its namesake, which then went viral on Gawker.
