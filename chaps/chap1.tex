\section{Stage One: The URL}

Content has a rich backstory before it arrives on the web, but I am treating its \emph{birth}---the moment when an object or artifact becomes a piece of content---as the moment in which it is placed on the web. It now has its first container, and its first piece of web-native metadata: the URL. Even before it is connected to other URLs (at which point it becomes a ``link'') an end user can access it online via a string of text. As Tim Berners-Lee tells it, the Uniform Resource Locator was one of the most difficult concepts to develop and understand as he began to weave the web.\autocite{berners-lee} To this day, he sees it as the web's most foundational element, and its importance is amplified even further in the Semantic Web. The URL itself is a remediation of old standards and practices. It mimics the file folders on our home computers (an intentional decision, so it could be understood and adopted quickly), implying a hierarchical, document-based structure. Interpreted hierarchically, the URL can be seen as an address, pointing us to increasingly specific locations until we arrive at the document in question. The virtual space of the web here seems to mimic physical space in the world, suggesting that one can find a document at a certain ``path'' under a certain ``domain.'' By turning all rich multimedia into ``uniform resources,'' the URL is a homogenizing force, encoding all content as a textual address and turning it into a reference rather than an experience or narrative.

URLs are not created equal, however, and savvy web users can read a great deal of information in this set of text. A ``.org'' top-level domain (TLD), for instance, might imply a nonprofit or philanthropic institution where a ``.com'' connotes a business. A long, seemingly obfuscated URL might contain spyware or viruses. A URL that ends with ``.html'' or ``.jpg'' will probably be a specific document (or piece of content), but one that ends with ``/users?friendrequest=true'' is more likely to be telling a social media site to request friendship with another user. Indeed, at the current stage of the web's evolution, a URL is not by definition a resource; it could yield no content and simply trigger a piece of code, allowing any arbitrary action. Moreover, even documents are subject to change, and the web has no built-in way to track content's erasures and additions. In other words, the ``Uniform Resource Locator'' is not necessarily uniform, nor is it necessarily a resource. Even this vague, homogenizing definition does not hold up.\autocite{figure here with detailed annotations of a single URL}

Eszter Hargittai points to the need for technical expertise in order to properly understand a URL and the implications behind it.\autocite{hargittai} It is easier for a user with less experience with the Internet to be duped by a phony URL that installs spyware or viruses; it is also more difficult for such users to find the content that they need when navigating through links. For instance, some users do not fully understand the difference between the web and Google, or whether a link in an article or feed will take them to the same source (the same domain) or a different one entirely. The URL thus serves as a barrier to understanding and retrieving information from the web for those who have less familiarity; technical knowledge enables information retrieval, and a lack thereof leaves users in the dark and vulnerable.

Further complicating the picture, neither the URL nor the link provide any information concerning motivation or access. In a URL, sensitive documents or paywalled media sites look the same as free and open information. With the exception of the underlying protocol (``https'' versus ``http''), a URL's overall security or level of access cannot be known without visiting it. Many online publications have constructed nuanced paywalls that can often easily be ``broken'' through a simple reordering of the URL, allowing information access through technical knowledge. Again this stratifies those who can navigate the web and those who are further in the dark.

The URL's fuzzy standards and conventions complicate any overarching attempts to understand or map the web as a whole through its URLs. Many large-scale network analyses of links (discussed at greater length in Chapter 4) rely on URLs to gain insight about the content within, because it is often too technically taxing to gather richer metadata. They rely on domains to determine whether a link is an inlink or outlink---wheter it is altruistic or ``nepotistic.''\autocite{chakrabarti} This is despite the fact that the URL is only a pointer or reference, and its address can hide or gloss over complex contingencies and nuances. For instance, an organization might be linking to its parent or sister company at a different domain, or it may register a ``.org'' website despite being a for-profit operation. Some studies have noted that users tend to trust ``.org'' websites, and many studies lean on these distinctions in quantitative link analysis; however, there is no technical or legal border between these particular domains.\autocite{err find all this wherever I read it}

Country-based TLDs do often impose certain restrictions, so suffixes like ``.es'', ``.uk'' or ``.tv'' belong to Spain, the United Kingdom and Tuvalu, respectively. These countries are generally free to do what they like with their TLD, which leads to a heterogeneous set of practices that are not accounted for in higher-level quantitative analysis. Some studies use these TLDs to map international communication networks by analyzing the link flows between these TLDs; for instance, between Brazil and Germany, or between developing countries and Western hubs.\autocite{park, others} This is in part because it is relatively easy to do such analyses at a large scale, where all you need is a list of URLs and pointers. But the URL is not so easily mapped, as it contains social and conventional complexities. The ``.tv'' domain, for instance, was sold by the small Polynesian nation Tuvalu to a Verisign company; while Tuvalu maintains a 20 percent stake and receives 4 million dollars a year to lease it out, it means that such quantitative analyses are glossing over the complex commercial and financial transactions that occur within these seemingly simple URLs.\autocite{find cite for .tv sale}

URL ``shorteners'' such as those employed by the New York-based company bit.ly (whose top-level domain would otherwise suggest that it comes from Lybia) likewise add additional layers between user and content, and further obfuscate the final destination. With a URL shortener, a small and innocuous domain (such as ``bit.ly/a423e56'') can take a user to any corner of the web, whether at the highest level (think ``google.com'') or its most specific (like ``pbs.twimg.com/media/Bm6QZAGCQAADEOk.png''). Shortened URLs have the same final reference point, but they no longer mimic the spatial world or even most personal computer filesystems; we have replicated and obfuscated the URL to the extent that any sort of uniformity or direction is impossible.\autocite{helmond}

% Anne Helmond "algorithmization of the hyperlink"

Perhaps the explosion of the URL was an inevitable byproduct of the web's very structure. The web is widely distributed and highly networked; it shuns hierarchical organization schemes, which seems to go against the ``domains'' and ``paths'' of the URL itself. Indeed, both Berners-Lee and Theodor Nelson (the original coiner of the term ``hypertext'' and its first champion) explicitly highlighted the power of the link to cut across tree structures and find new, unexpected associations.\autocite{berners-lee, nelson} Where knowledge was once shaped like a tree, on the web it looks more like Deleuze and Guattari's rhizome: an infinitely ``intertwingled'' mass.\autocite{deleuze, nelson. figure here too?} One cannot make sense of it using URLs alone, but links offer a start.

\section{Stage Two: The Link}

The birth of the ``link'' occurs at a second level of containment, after an object becomes ``content'' with a URL. The link wraps the URL in an HTML element that allows it to be quickly accessed from another page. Without links, the web would just be a series of disconnected nodes; with links, the web gains edges and becomes a network. Bowker and Star suggest that links have the power to classify without any human agency or intervention, which forms the basis of this section: ``Every link in hypertext creates a category. That is, it reflects some judgment about two or more objects: they are the same, or alike, or functionally linked, or linked as part of an unfolding series.''\autocite{bowker_star} Bowker and Star are not the only ones to cede agency to the link, and many disputes and debates occur over links; even in 2002, Jill Walker asserted that ``links have value and they give power.''\autocite{walker} In many ways, the link is the battlefield for the political economy of the web, serving as a sort of digital currency and object of value exchange.

All the same, the link is a seemingly innocuous object. We usually consider it taking the form of a blue, underlined piece of text on a webpage (under the hood it is known as an anchor tag---the string ``<a href>\ldots</a>'' and everything in betweenâ€”in an HTML document). Clicking on the link turns the object into a mechanic, leading a user down a rabbit hole of subsequent destinations and redirects (all employing some dozens of standards) before landing on the target destination---back to the URL. The URL is only one attribute of the link, along with others that determine, for instance, whether to open the link in a new tab or window---so in a literal sense, the link contains the URL.

The link is forever associated with (and perhaps plagued by) the footnote. Nelson's hypertext manifesto \emph{Computer Lib/Dream Machines} praises the screen for permitting ``footnotes on footnotes on footnotes,''\autocite{nelson} and Berners-Lee's web takes the traditional citation as inspiration. Nelson belies himself by consistently contrasting hyperlinks with footnotes; in some senses, one cannot escape being a remediation of the other. But the link's readable text---its manifestation in a browser, known as the anchor text---adds another layer of semiotic containment and enrichment to the original content. The ``jumpable interconnections'' that Nelson envisions are built into the fabric of the writing rather than set aside like a footnote.

The anchor text has no innate relationship to its target, and it is only pointing to the target's address. As a result, the link can be seen as a sign. Analyzing the link's anchor text through a semiotic frame reveals a number of interesting conventions and uses, each of which bears underlying motives. The many flexible uses of the link may follow something like Charles Sanders Peirce's semiotic triad; when a link says ``click here'' as opposed simply linking the text as so, it may be forming an indexical rather than symbolic relationship to the target. When a link's text is identical to its address, like ``http://www.google.com,'' it seems to be removing this layer entirely. However, there is nothing stopping someone from putting a completely different address into the anchor text, further emphasizing the lack of relation between anchor and target, or signifier and signified. This distance is what allows a scam artist to direct an unknowing user to a phony bank website, even if the stated URL is for their real bank. It is also used for more playful and innocuous ends, such as with ``rickrolling,'' a meme where someone provides a purportedly useful link, but it actually leads to a video of Rick Astley's 1987 hit ``Never Gonna Give You Up.'' Whether playful or nefarious, both of these uses are enabled by the structure of the link, and the lack of relationship between the text and the target.

Many studies have attempted to glean insight from the link by assuming, like Bowker and Star, that links create categories. On one hand, it seems extremely liberating to sidestep the ontological dilemma of what that category is, and simply treat it as a raw signal. I see this ability as the basis for much of the revolutionary rhetoric of the web and the power of networks. On the other hand, the lack of relation between text and target seems to point to the problems with this approach; a sign is not the same thing as a signal. Studies and practices that analyze and aggregate links would do well to closely analyze the text of the link. There have been very few large-scale studies of the semiotics of linking, or the way in which the anchor text helps to gain insight into the target resource or the connection being made. One exception comes from a small 2006 study of automated blog classification, where the researchers determined that the anchor text was in fact the best signal for improving classification.  One of the researchers now studies the text of tweets to gain insight into the links they embed, once again treating the users' descriptions of links as more important than what networks are sharing it.

But for now, most studies simply take an aggregate view of link sharing, treating each connection as equal regardless of context. This has vast implications for the news media and has undoubtedly affected content creation and discourse. Anyone who shares an article inevitably, and perhaps inadvertently, raises the article's profile and algorithmic rank whether they liked it or not. Algorithms might therefore prefer controversial links rather than universally liked, substantial, or thought-provoking ones. This could create incentives for publishers to use unnecessarily inflammatory or partisan language, with the assumption that despite how users feel about the content, they will certainly click on it, and possibly share it. This is best exemplified by Rusty Foster's ``Today in Tabs'' newsletter, which popularizes the idea of ``hate-reading'' and links to some of the most infuriating articles in the news. It is not clear to algorithms whether or not someone liked an article (let alone why they liked it)â€”it is only clear that they are talking about it. This may be because there is no straightforward way for an automated system to understand the many cultural nuances behind a link.

This limitation is apparent to Berners-Lee, who has in recent years championed the Semantic Web as a way to make the web more structured and machine-readable. The Semantic Web allows for links themselves to be annotated and queried, so that, for example, we could search for ``users who disagreed with this article'' and not just ``users who linked to this article.'' This carries great promise not only for a machine-readable web but a new order of linkage and network formation. The W3C (the standards organization for the web) maintains appropriately revolutionary rhetoric around the Semantic Web, and has tried out scores of marketing terms in its efforts. It alternately envisions a ``web of data'' (rather than documents), a ``Giant Global Graph,'' and ``Web 3.0,'' a particularly telling attempt to couch the Semantic Web as the inevitable next step of forward progress. However, while linked data has been useful in smaller-scale initiatives, the Semantic Web movement is progressing very slowly. It also brings its own problems; while a web of documents is one level removed from the data itself (and therefore more difficult for machines to read), at least it keeps the source context intact. The Semantic Web also imposes its own set of ontologies, hierarchies and categorization schemes, a problem that I will return to.

Another alternative to the web's form of linkage comes from Ted Nelson, a longtime critic of the web's architecture. As the original hypertext visionary, his scheme, called Project Xanadu, floundered for decades, and was never truly built in the way that he envisioned. When critics suggested that Xanadu was the first failed web, Nelson bristled: ``HTML is precisely what we were trying to PREVENTâ€”ever-breaking links, links going outward only, quotes you can't follow to their origins, no version management, no rights management.''  Xanadu's most important feature, absent from the web, is the two-way link; when one document referenced another, the target document referred back to the original in turn. The hyperlink on the web, for all its flexibility, does not escape the trappings of the footnote in this single, very important way. Links always move backward, and given the lack of a canonical URL on the web (another of its limitations, which the URL-shortening phenomenon compounds), finding all the citations for a single document is next to impossible. Jaron Lanier believes this simple omission has profoundly affected culture and economics, which forms a cornerstone of his recent book Who Owns the Future? 

But in the absence of replacing or reconfiguring the web's current structure, the one-way, semantically meaningless link remains the web's primary organizational scheme, and the ``click'' remains the proxy for attention and engagement. Clicking on a link is not only a navigational mechanic; it is a signal of intent and interest, which influences algorithmic decisions and other readers in turn. It is also often a financial transaction between unseen actors; each link clicked and page viewed is a new ``impression,'' causing money to change hands between content distributors and advertisers. This has in turn changed the aforementioned semiotics of the link, and the meaning of its anchor text.

There has been much recent controversy surrounding the restructuring of the news headline in the hyperlinked age. Where traditional headlines might read ``The Global Fight Against Tuberculosis,'' a more recent one is more apt to say, ``It Kills 3 People a Minute, but That's Not Stopping This Group of Superheroes.''  The headline is ``click bait,'' playing to a user's innate curiosity (Atlantic writer Derek Thompson calls it the ``curiosity gap'')  without telling them the substance of the article or the actors in play (tuberculosis, the victims affected, the Global Fund, the Gates Foundation, and others). These actors and the issues they are tackling are reduced to pronouns. Here even the content becomes glossed, and a click is just as likely to signify curiosity about what the content is, rather than any genuine interest in the content itself. Machines are not likely to recognize these nuances, which results in false identification of public interest and discourse. The website Upworthy is a canonical example of click-baiting headlines, and even its organizational structure is revealing; the company creates no original content, but instead employs people to trawl the web, find content, and put a new headline on it. The team is not creating new content, but new containersâ€”and it is one of the most popular and successful media business efforts of recent years. Despite this, Upworthy has been mocked frequently, such as via the joke news site ``Clickstrbait,'' which leads users down a rabbit hole of curiosity-inducing headlines without guiding them to actual content.

Interestingly, Upworthy is one of the first websites to attempt to move beyond the simple ``pageview'' metric, heralding a new measure of success called ``attention minutes.'' These metrics will make privacy advocates cringe; by monitoring which browser tab is open, where the mouse is pointing, or how much of a video the user has watched, Upworthy hopes to understand user behavior more deeply. Upworthy's blog claims that ``this is a metric focused on real user satisfaction,'' but it is still a measure of behavior as a proxy for emotion, and the end goal (a like? a share? a donation to a worthy cause?) remains unclear. 

In all of these cases, the layers of containment could be seen as layers of signification. I stated earlier that the birth of content (the transformation of an object into content) occurs at the moment it is uploaded to the web, and accessible via a URL. Here is where it moves from essential object to sign and message, in Jean Baudrillard's terms. Looking at the web as a designed artifact with a specific, graspable structure, Baudrillard proves fruitful in emphasizing the web's political and philosophical origins and ramifications:

The semiotic revolutionâ€¦concerns virtually all possible practices. Arts and crafts, forms and techniques both plastic and graphicâ€¦which until then were singular and distinct, are synchronized, and homogenized according to the same model. Objects, forms, and materials that until then spoke their own group dialect, which only emerged from a dialectical practice or an original style, now begin to be thought of and written out in the same tongue, the rational esperanto of design. Once functionally liberated, they begin to make signs, in both sense of the phrase (and without a pun): that is, they simultaneously become  signs and communicate among themselves. Their unity is no longer that of a style or practice, it is that of a system. 

Replacing ``design'' with a word like ``information'' or ``data'' reveals the homogenizing force of the web and its ability to squash varieties of creative works (photos, videos, text, music) into data, which allows for easy exchange, commodification and reuse.

\section{Stage Three: The Feed, the Index}

Links rarely exist in isolation. For one, links contain links themselves, as I touched on in the last section. But another form that the link takes is as part of a list or sequence. Whether it is a digest (on email), a feed (on Facebook, Twitter, or RSS), a set of search results, or a list of ``related articles,'' users are almost always confronted with several choices for what to click on. In this section, I look at the ways in which links get aggregated, indexed, and fed to users, allowing for another layer of containment beyond the link. For instance, while an article might embed an image, the article itself is then embedded and contained as a search result or single item in a table. The table usually truncates the content into a headline, and perhaps an image or opening paragraph. This can allow for a higher-level view of a major topic, author, or other organizing factor, but at the expense of hiding the richness of the content within.

The aggregators, indexers, and summarizers of the web are its search engines and social media feedsâ€”in other words, the most powerful and profitable tech companies in the world. While the content creator usually has to win the attention of the distributor, the distributor in turn must always play the aggregator's game, completely powerless without it. This is evidenced by Upworthy itself, who recently found its content potentially demoted in Facebook's algorithm with no meaningful explanation, shrinking its immense traffic to half of its previous size.  Another major content distributor, the lyrics annotation website Rap Genius, recently found its pages move from the top hit on Google to its seventh page, due to changes in Google's algorithm.  These content aggregators can move around large swaths of content (millions upon millions of interlinked pieces) via slight changes in their codebases, with no obligation to inform anyone of the reasons or even that it is occurring. This is perhaps the highest level of containment, and few (if any) actors can claim to contain these sites in turn.

To be fair, Google did explain its reasoning for the Rap Genius demotion, and the dispute was telling. Rap Genius had launched a ``Blog Affiliate'' program, which clandestinely offered to tweet out any blog post in return for links back to the Rap Genius site. In other words, Rap Genius was engaging in SEO (Search Engine Optimization) spam, attempting to falsely boost its search rankings by asking bloggers to post unrelated links back to their site. This is one high-profile example of what many smaller players do every day in order to keep their businesses alive: game Google's algorithm in order to bolster their search rankings. SEO is, in effect, an entire industry built on gaming links.

This works because Google's PageRank algorithm is primarily derived from who is linking to whom. In effect, their link-based classification scheme is what made them the dominant information provider that they are today. Prior to PageRank, web crawlers and indexers like Yahoo, HotBot, and AltaVista provided a plethora of options for Internet search (even these, in all their heterogeneity, were seen at the time as a major threat to the open web). But each was based on a traditional, hierarchical classification scheme. In PageRank, Google found a way to embrace the web's disorder; where Yahoo insisted on keeping an organized system, Google relied on links to sort everything out. Clay Shirky argues that this is what allowed Google to surpass Yahoo and become the first truly ``Web 2.0'' company, asserting that on the web, ``ontology is overrated.'' 

Google famously published their initial PageRank algorithm, and once the cat was out of the bag, advertisers and spammers began to exploit it, inserting links not for their usefulness or relation to the text, but to improve their pages' search rankings. A large portion of website hacks and attacks are merely to insert hidden links on the targeted sites. In the process, Google has had to remain one step ahead of the advertisers, with the link as the battlefield, influencing the web and changing its structure in turn. But this battle has mostly been played out by machines, which are responsible for a substantial amount of the links created â€“ as well as the links browsed and followed â€“ on the web. Besides a generic, easily replaceable piece of metadata in a web request, it is in fact impossible to tell whether a website request is coming from a human or a machine. In Google's published PageRank paper, Sergey Brin and Larry Page provide a curious ``intuitive justification'' for their algorithm that seems to conflate the two:

PageRank can be thought of as a model of user behavior. We assume there is a ``random surfer'' who is given a Web page at random and keeps clicking on links, never hitting ``back'' but eventually gets bored and starts on another random page. The probability that the random surfer visits a page is its PageRank. 

This is a very strange user indeed, assumed to be easily ``bored,'' distracted, and clicking on links at random. Moreover, this was an assumed user in 1999, and the ``model of user behavior'' must undoubtedly be changing as the web's capabilities and browsing habits change. This bizarre mixture of human and nonhuman, as well as the substantial influence that links have on the information we encounter as everyday users, speaks to the usefulness of actor-network theory in framing the political economy of links and linking.

While links are shared for a variety of reasons â€“ some of them more nefarious than others â€“ the blogging and tweeting culture of ``Web 2.0'' holds to the principle of link sharing for mutual interest and benefit. If two bloggers like one another's content, they will agree to link to each other on their respective blogs. This happens on the ``blogroll,'' a list of other blogs that a blogger might recommend, usually presented as links in the blog's sidebar. Here the link functions as an act of exchange under the guise of free information sharing. Looking at it through the lens of Marcel Mauss's writings on gift exchange, however, it seems to carry more weight than this: ``Exchanges and contracts take place in the form of presents; in theory these are voluntary, in reality they are given and reciprocated obligatorily.''  This can be seen beyond the blogs of Web 2.0; users exchange links on Twitter and retweet, favorite, or like posts on various social media platforms. In each case, a link or like on a social media post is performative and transactional, with the implicit expectation of a future like in return.

Moreover, these link exchanges solidify existing networks of bloggers and content creators, perhaps galvanizing the network but at the risk of collapsing into ``filter bubbles.'' Many studies of links have traced political homophily, public debate, blogs and global flows of information; if we take these at face value and treat hyperlink usage as a proxy for importance, impact, and communication, then link-sharing can turn conversation inward, allowing searchers to see only blogs that have overtly linked to one another (blogs which, presumably, have similar views and opinions).  While the Internet may allow for a more heterogeneous group of voices to surface than in traditional media (and indeed, this is one of the ways in which the medium is widely celebrated), one must still take part in link sharing with a particular group in order to be found, leading bloggers into already-established and tightly wound networks. This phenomenon is most expertly outlined by Philip Napoli, who calls it ``massification'': in the editorial and algorithmic decisions that determine where links are placed and directed, there is a distinctive replication of old ``mass'' media patterns. 

While content creators, distributors, and aggregators are locked in this battle over links, what happens to the actual user who visits a site, application or search engine? The user is presumably after ``content,'' and unless they were provided with a direct URL, they can only access it through a series of layered containers. Moreover, the information, story, or ``piece of content'' that they may be after is replicated and embedded in different contexts and myriad places around the web. The end result, when a user goes to Google to search, is often repetition. The same piece of content appears everywhere, such as a canonical image for a popular news story, meme, or theme.

Repetition plays a strong role in Freud's definition of the uncanny. I wouldn't suggest that a user is frightened by search results, but there is a sense of unease or anxiety in finding the same content repeated ad infinitum. Google's ``search by image'' feature provides a list of ``visually similar images'' that reveal hundreds of nearly identical photos. For example, an image search for ``office meeting'' turns up the same stereotypical figures; businessmen and businesswomen in suits, seated around a table, poring over seemingly identical documents in a typical conference room. The photos are mere signifiersâ€”it seems clear that the subjects are actors, and no business is actually being done. The emptiness of the content itself, and its endless repetition, is highly unsettling. Freud's notion of the uncanny has also been applied to the context of online advertising. Often when a user visits a product page, the same product is then re-presented to them in the sidebar of a completely different siteâ€”often mere minutes later, but other times it takes days or months. This is a more direct application of the uncanny, as it does make the user feel as if they are being watched. 

\section{Stage Four: The Archive}

Content's final resting place is in the database, or archive. But all the same, it is not fair to call it ``final,'' since the context and metadata surrounding it is always subject to change. Moreover, this lifecycle is vastly oversimplified; often the content reaches a database as soon as it is accessible via a URL (for instance, with a photo that is uploaded to Flickr or Instagram). Then as the content moves around the web, affected by other creators, distributors, aggregators and indexers, it is placed in an untold number of databases, with varying structures and associated metadata. So in a sense, the four stages that I have outlined here collapse on one another, and the framing that I have offered is far too neat, simple, and narrative-driven for the distributed, infinitely networked, rhizomic web.

The database is a different form of container than the others, as it is in fact not truly of the web; it merely talks to it, interacts with it, works with it. While users increasingly treat the web as a database â€“ what we know familiarly as the ``cloud'' â€“ it is less distributed and hypertextual than that metaphor seems. There is no single database, but rather very many, housed on servers around the world. Each of them faces the same challenge: how to flatten and store the infinite possible contexts, networks, and signals that the web has created around each piece of content, into a format that allows a user to find it efficiently using any number of contexts. Perhaps a user is looking for everything stored in a specific time frame, a certain format, a dedicated category, or any combination thereof; in each case, the archive serves the role of retrieving the information needed.

As a result, the archive must anticipate any possible need from any possible user, whether they request content today or far into the future. Any signal that is left out is lost potential knowledge. So an archivist, most often associated with storing the past, also plays a crucial role in predicting and affecting the future. Jacques Derrida traces this phenomenon in Archive Fever, where he calls the archive ``a pledge, and like every pledge, a token of the future.'' 

There is no reasonable way to store every possible route through a database that a user might take; this would require infinite storage and processing power. Given the highly networked, context-focused organization of the web, it is an impossible task. Derrida highlights this challenge as well: ``the limits, the borders, and the distinctions have been shaken by an earthquake from which no classificational concept and no implementation of the archive can be sheltered. Order is no longer assured.''  Derrida thus relates the archive to a prosthesis, a built and artificial entity that mimics but does not replicate the infinitely rich sensoria of reality. Claire Waterton, citing Michael Taussig, also uses the border metaphor to describe the increasing diffusion of information: ``the border zone of representation is currently expanding, proliferating, and blurring, becoming permeated by itself.'' 

Seen in this way, the database is perhaps the only truly containing force; the previous stages are in fact expanding contexts and meanings for each piece of content, and it is only in retrospect (through the archive) that it becomes contained. But all the same, we cannot see the content except through the archive. And with the assumption that a border must be drawn through the expansive, innately borderless web, the question is where and how to draw it. Lisa Gitelman laments the way in which the archive reduces ``ideas into character strings,'' or in the case of rich multimedia, encoded, flattened and unsearchable bits.  Character strings and encoded bits are devoid of context and semantic meaning. They certainly do no justice to the richness of the original content, which points to a proliferation of associated narratives (for instance, each photograph has a photographer, a subject, a setting, a camera, and all of the processes that formed the ``becoming'' of these entities, and which we implicitly consume and consider as we look at that photograph).

My aim is not to suggest any overarching solution to the limitations of the archive; it is, in fact, this very impulse that has often set back the work of retaining knowledge and history. Bowker and Star point to the myriad efforts of ``universal classification,'' dating back to the Tower of Babel, all of which have essentially failed. Classification is an inherently epistemological, performative act that is always embedded in a certain community and always subject to change. In short, it is socially constructed. In order to fully recognize and remember this, Bowker and Star suggest the framework of ``boundary infrastructures'' to acknowledge and work with the limitations of traditional classification. Boundary infrastructures make use of boundary objects: ``those objects that both inhabit several communities of practice and satisfy the informational requirements of each of them.''  In practice, these objects (and the infrastructures that work with them) will maintain slightly different meanings in each community, but they are common enough to be recognizable to multiples. While this approach is more of a framework than a solution, it rightly discourages the drive for an overarching schema for every object and community. By recognizing that no system will ever be perfect, it instead highlights the need for a loosely linked multiplicity of them.

Likewise, I intend to propose that the web itself should not be universally schematized, and its content will never be singly and correctly categorized. In a sense, the proliferation of databases and motives for classification that the web provides allows for more ``ways in'' to the content than if the web were stored at a single endpoint. The Semantic Web is an interesting hybrid of centralized and distributed; it aims to bridge traditional taxonomy and contemporary chaos through its use of user-generated ontologies. In order for machines to understand a network, everything must be definitively categorized, but the categorization scheme itself is subject to change. Certain standards arise, but each individual or community is free to create its own form of linked data. All the same, the slow adoption of the Semantic Web may have to do with its reliance on these ``ontologies''; even if multiple ontologies can coexist, they are still trying to compromise the web's disorder.

Derrida's ``archive fever'' is both a personal and an institutional drive. Google and Facebook store user data (including user-created content) with abandon, inventing new contexts at each turn. Users bookmark, download, pin, and clip online resources, sometimes all at once. Built-in browser solutions like bookmarks and history haven't changed their structure in years, and it showsâ€”they store nothing but the URL. ``Bookmark'' is a misnomer of a remediated word, as books can't change or disappear overnight, while ``history'' implies a time machine that the web doesn't have. Personal note-taking and online ``snapshot'' tools aim to create a sort of personal, annotatable intranet for users that want to filter signal from the noise (see applications like Evernote, Pinterest and Zotero). However, each system is limited by the borders of the database, and aside from folders and tags, none provide a useful way to store meaningful associations between these documents.

In all cases, the problem of ``information overload'' is paramount, and the virtual piles of documents and content get increasingly difficult to wade through and make meaning of (unless one takes the high level, ``big data'' viewâ€”a perspective that has its own pitfalls, as danah boyd and Kate Crawford show).  But information overload is nothing fundamentally new. Ann Blair finds a complaint about a ``confusing and harmful abundance of books'' as early as 1545 (in Conrad Gesner's attempt to catalog all known books), and many other scholars have historicized information overload and management strategies (such as commonplace books, scrapbooking, ``stringing,'' and the encyclopedia).  One of the most canonical methods of organizing too much information is the card catalog, in use by libraries for more than a century; early hypertext systems, such as Xerox PARC's NoteCards and Berners-Lee's ENQUIRE, are noteworthy in their remediation of the affordances of this old tool.

However, the associations, trails, and lists sparked by the web add to the possible avenues for research; the myriad interconnections between documents may be more responsible than anything else for the seemingly unprecedented amount of information. In response to this, users store everything, in hopes of using the archive's power of containment to understand it. But containing is not understanding, and by turning rich multimedia into bits of text, containment in fact furthers the distance between the user and the real, lived experience that the content aims to capture and describe.

\section{Postscript: Erasure and Afterlife}

Content has an afterlife when it goes through a sort of reversal of the stages outlined above; it must be plucked from an archive by a search algorithm, which is in turn responding to a request by a user. Some content never does live again; for instance, nearly one-third of all reports on the World Bank's website have never once been downloaded.  This does seem to run counter to the ``pack-rat'' mentality of users and institutions proposed earlier, but it also points to the vast amounts of knowledge we are creating that may require a new format to be rendered useful. This is not to say that the knowledge contained in the World Bank's documents has been utterly forgotten (the document could be distributed by email, or presented at a conferenceâ€”the inability to track it is the crux of the problem); only that it is (literally) not helping anyone in its current structure.

Other content may in fact be useless, or worse, detrimental. Knowledge, and even facts themselves, have been rephrased, rewritten, and reversed for as long as facts have held public influence.  Some content is outright false, misleading or slanderous, and other content is simply embarrassing. Certain regrettable pieces ought to be remembered (such as a racist comment made by a powerful public figure); others are more likely best forgotten (such as a teen's suggestive selfie that gets distributed around the internet). But the question is not who determines what deserves a place in history and what should be erased; even if one deletes the content, it is not likely to disappear entirely.

The user's experience of deleted content is the broken or dead link, the ubiquitous 404 ``Not Found'' error page. While in some cases a dead link signifies a necessary removal (such as the teen's photo), in others it is a stand-in for lost knowledge. There's no doubt that content does disappear; studies have found that 30-50 percent of citations in scholarly papers and legal opinions no longer work (a phenomenon known as ``link rot''), and even when they work there's no telling how much they have changed (this is ``reference rot,'' which the Hiberlink initiative is currently researching the extent of).  There is, of course, a substantial chance that the content still lives somewhere on the web, often in multiple places; but if it is no longer at the path specified by the link, it will be much more difficult to find. To combat this, archivists and cultural heritage institutions aim to preserve the web's history for later retrieval. The Internet Archive crawls and stores as many websites as possible, while the Archive Team aims primarily to preserve discussion forums and old blogs. Unlike other groups, the Library of Congress saves websites worth saving by manually choosing them, often taking an ``aggregate the aggregators'' approach and storing large text databases. In each of these cases, groups are establishing an archive that is perhaps less financially motivated than a company's database, aiming instead to preserve the knowledge and associations within for public benefit.

\section{Conclusion}

Hypertext is built on the premise of collapsing traditional, hierarchical categorization schemes, felling the tree and digging to find the rhizome. This information structure certainly has its historical precedents; a reference book, such a dictionary or encyclopedia, is a classic example. Organized alphabetically (which is to say arbitrarily), it is always referring to other words and terms, requiring the dedicated reader to jump from one page to another, following any thread at will. Michael Zimmer connects Diderot's EncyclopÃ©die and its use of renvois to the hyperlink, noting its ability to subvert hierarchical knowledge distribution and censorship in the process.  Much of this language is echoed by Nelson, Berners-Lee, Paul Otlet and the many early champions of the web, who saw it as a democratizing force leading towards social good.

However, the web's highest-level platforms now encompass, embed, and contain all other media, a phenomenon that is difficult to see as we users browse one page at a time. While this structure affords certain advantages, there should not be a one-size-fits-all model for experiencing media, communication, and culture. The web provides no built-in way to ``zoom out'' and see overarching link structures; it does not allow curious users to trace content to its origins; and it is a disorganized mass that various actors have spent a massive amount of time (and often earned a great deal of money) sorting out. As Bowker and Star remind us, each act of sorting has consequences, and that we rely on sites like Google to do it for us, with no obligation of transparency, is a dangerous reality to live in.

The web and the archive's acts of containment, on every level, likewise have real economic consequences. There has been much lamentation of the demise of the ``creative class,'' reducing rich and multifarious works to the act of ``content creation.'' Similarly, there is much trepidation about big data companies that ingest this content and our interactions with it, making billions of dollars in order to grant us access. I would suggest that these trends are (not necessarily caused, but) enabled by the structure of the web itself. Looking to new structures and forms of classification would do well to counteract the containing, homogenizing forces of computation, and the ``big words'' (data, information, document) that come with it.



% \section{Layers of containment}

% These news events have in common that they highlight the ways in which the web acts as a \emph{containing} force on its material, turning the result into what we call \emph{content}. ``Content'' is a word that is bound to encounter derision, whether from ``content creators,'' information theorists or media scholars. In a recent talk at MIT, Henry Jenkins referenced the word's derivation from the Latin \emph{contentum}, meaning ``a thing contained.'' Doc Searls frequently criticizes the term for its ties to marketing, implying a one-way web where content is a catchall term for anything that can be packaged, sold, and consumed online.

% Another, perhaps friendlier way to describe content is as a ``link.'' Where content implies a container (containment), a link implies a connection (expansion), promising to break free from the contained information. Looking at the link favorably, if a publisher adds a hyperlink to an article, it purports to show not only erudition (the publisher has read and vetted the content within), but also altruism (the publisher is helping the content creator, and you, the user, reach one another). But here, the link surrounds the content. In effect, it is the original container, adding the first layer of \emph{context} to the content, but diluting its core in the process. In studying the
% origins of the link's structure and the web's infrastructural qualities, we find many ways in which the web's very structure, as well as the creators, indexers, and archivists that work with content,
% acts as a containing and homogenizing force on original creative works.

% \subsection{The URL}

% \subsection{The Link}

% \subsection{The Feed/The Index}

% \section{The Fields}

% The hyperlink on the web consists essentially of three a URL (the ``target'') The URL is treated as a unique identifier and as an atomic unit of information, when it's truly neither.

% \begin{description}
% \item[The URL is not unique] The same article or event listing reappears under dozens of URLs, and any attempts to find a ``canonical URL'' are expensive and inconsistent. Sometimes -- like in the case of a wire service that gets aggregated by several publishers -- there's no singular home for it.

% \item[The URL is not atomic] URLs point to multitudes of resources, or none at all. Links have text, pictures, videos, audio, other links, and annotations on all of the above. They might change depending on who's asking for them and when they're asking. They might give your computer a virus. They can (and often do) cause money to change hands between unseen actors. They can open in a new tab or window, or open your email client.
% \end{description}

% In other words, the \emph{Uniform Resource Locator} is not uniform, nor is it necessarily a resource. The rest of the link (the <a href="..."\> and everything in between) doesn't give many clues as to what's behind it, or what the significance of the connection is. As a software developer, this leaves me scrambling for any clues as to how to define the content and its relationship to other content; aside from clicking the link and visiting the resource, which is slow and expensive at scale, you can't really know what's behind a link. 
