%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{The Size and Shape of Archives}

\section{Introduction}

\subsection{The Networked Archive}

The word “archive” brings to mind a stuffy room full of old books and manuscripts, closely guarded by librarians. In the traditional archive, books can only be in one place at one time, and always next to the same exact books on the same exact shelf. The atomic unit of information tends to be the book (or manuscript), even though books themselves contain multitudes of media (text, images, maps, diagrams) and the bibliographies and indexes that offer a window into a book’s constituent parts remain limited by space and language. If your archive dive takes you beyond the books in the current room, you’ll have to leave the room.

But archives come in many forms. More recently, an archive is likely to be digitized, stored on networked servers in databases. Here the archive’s stacks and files are virtual, and can be ordered and reordered at will. Books and documents are further atomized and calculable as data. If a search goes beyond the digital archive’s scope, it may even be able to reach for information outside of it. In short, the digital affords new abilities for linking or networking the archive, allowing it to dynamically expand, contract, and change shape. In the networked archive, we can forge new connections and create more nuanced context for the information stored inside. Most of today’s digital archives and knowledge systems take advantage of some of these new linking features, but they also still inherit many of the limitations of their physical predecessors.

A networked archive is a collection that: a) treats its contents as an ecosystem of discourses rather than a brittle item to put in boxes; b) actively forms, re-forms, and presents information in more nuanced ways than traditional search; c) gracefully takes in new content and information for future reuse; and d) interfaces with any number of other archives to expand, contract, or reframe its borders. A well-networked archive places context on the same level as content, acknowledging the constantly expanding and shifting shape of research, inquiry and history, and putting the past in full dialogue with the present.

The act of networking the archive is certainly aided by digital tools, but it is not a requirement. Many indexing and note-taking systems of the Renaissance and Enlightenment allowed for the interlinking of disparate ideas, and these offer useful inspirations and foils for examining the web and its related research tools today. Information overload is not a new phenomenon, and pre-digital knowledge systems had many techniques for what Ann Blair calls the four Ss: storing, summarizing, sorting, and selecting.  Moreover, the web is only one of many digital hypertext systems, and the hyperlink – the primary object and mechanic for network formation on the web – has its own limitations that early hypertextual systems bring into full relief, inviting close analysis of the web’s archival affordances.

\subsection{Defining the Archive}

The notion of the archive has exploded even beyond its new digital meaning. Foucault uses the term to refer to “systems of statements” that consist of the “history of ideas,” the entirety of sayable things and their referents.  Foucault’s epistemological archive subsumes both the stuffy room and the digital database into itself. So is the archive literal, digital, or figurative? What size and shape does it take? Does it represent an individual’s memory, or collective history?

This shifting notion of archive varies based its shape and its scope. An archive can be personal, institutional/collective, or universal. Despite the vast difference between, say, a student’s bookshelf and the entirety of the World Wide Web, each of these aggregations of information can be figuratively and colloquially considered an archive. . Archives morph, connect with, and contain one another. Since the archive evokes all of these scopes and practices, the word, like the referent, expands and contracts in meaning.

An archive always has a border, a point at which the collection stops. It stops on both sides: the micro level (what is the smallest unit of information that it indexes—a book, an image, a single letter?) and the macro level (what information or metadata does this archive not include?). That an archive has a limit is inevitable, and useful; a limitless archive would be impossible and unhelpful, akin to Borges’ exact one-to-one map of the world.  But ideally, an archive can expand and contract, as needed, on both scales, satisfying both the casual browser and the dedicated researcher. If a researcher asks a question too specific for any one document, the archive could break down the document into its constituent parts; if a user is browsing beyond an archive’s boundaries, it might talk to other archives that have the answer. The ideal archive is elastic, polymorphous, and adaptable.

Aside from the borders of archives, there are also borders in archives. Traditional, physical archives are divided into sections, stacks and rows, each with dedicated classification schemes that keep books in their right place. Librarians and experts draw and maintain these borders, while others need to speak their language to find their way. Today’s digital archives are not so neatly or hierarchically drawn. Jacques Derrida uses the border metaphor to describe the recent diffusion of archives: “the limits, the borders, and the distinctions have been shaken by an earthquake from which no classificational concept and no implementation of the archive can be sheltered.”  Claire Waterton likewise suggests that the border zone is “currently expanding, proliferating, becoming permeated by itself.”  Reflecting the postmodern skepticism towards standard categories and hierarchies, the networked archive morphs and munges its contents into any categorization scheme that a user or collective might define.

These complications make any singular definition of archive impossible. Generally speaking, I will use the term to refer to any collection or repository of items that offers interfaces for those items’ organization and discovery, with the aim of helping people find information, structure ideas, and do research. This includes the systems surrounding collection itself—organizational, structural, and sociocultural. To put it in Lev Manovich’s terms, “data structures and algorithms are two halves of the ontology of the world according to a computer.”  I am interested in an archive’s data structures (specifically with regard to its item’s indexing, metadata, and organizational schemes), as well as its algorithms (the ways to organize, aggregate, repurpose, and present these items to the user).

For my purposes, the “archive” is similar to the concept of the “database” as considered by Manovich and others. The distinctions between these two terms have been debated extensively, and some scholars have treated traditional, pre-digital archives as databases.  I intend to reverse this anachronism, and treat databases as archives. I do this in part to hone my focus onto the collections and systems that provide access to personal, institutional, and historical records for research and inquiry. As Marlene Manoff says, “The notion of the archive is useful in theorizing the digital precisely because it carries within it both the ideal of preserving collective memory and the reality of its impossibility.”  Following Jerome McGann’s insights, I see the database as a technical instrument used for the structuring and enabling of archives; it is not the archive itself. 

Like McGann and Manoff, I also use the word to emphasize a lineage. Today’s information management tools continue to inherit many ideas and techniques from traditional archives and note-taking systems—a fact that “database” doesn’t emphasize. These systems are always evolving and built atop one another; traces of old technologies are present in current systems. In this sense, many of the applications we use today are systems for organizing and managing personal, institutional and public archives: search and social media platforms (Google, Twitter), note-taking and citation tools (Evernote, Zotero), content management systems (WordPress, Drupal), ideation and productivity software (Trello, Basecamp), media repositories, codebases, and so on. These archives are also deeply embedded within and linked to one another through APIs, further complicating the picture.

The rise of the knowledge economy has brought more and larger archives, and new computational capabilities have brought a new kind of archive with new affordances. We use these archives for both professional and personal ends; whether we read social media and blog posts, create and collaborate on workplace documents, or use data-driven methods to track our health and habits, we are interacting with archives. Jussi Parikka suggests that “we are all miniarchivists ourselves,” calling the information society an “information management society.”  Belinda Barnet considers it a “pack-rat” mentality, while Derrida succinctly and famously titles the phenomenon “archive fever.”  My use of the term encompasses traditional archives, modern databases, and the algorithms and interfaces in between—the indexes, note-taking systems, bibliographies and encyclopedias that first forayed into networked information.

\subsection{Outline of Themes}

Most histories of the proto-web begin with Vannevar Bush (and sometimes Paul Otlet before him), leading directly through hypertext pioneers Ted Nelson and Douglas Engelbart, and concluding with Tim Berners-Lee’s World Wide Web in a direct line from past to present. I will look closely at these individuals and their goals, and even use this chronological lineage as a structuring point, but I will also break apart this history by introducing other systems and figures – whether they existed long before computers or after the rise of the web – that point towards three corresponding themes. These themes recurrently surface when dealing with digital archives and information management.

The first section addresses the spatialization of memory and knowledge. Here I consider the use of visual metaphors for information and the associations between memory and physical space. The spatial and dimensional nature of knowledge is at odds with the “flattening” effect of indexes and the collapsing of dimensional space that non-hierarchical linking affords. Cycling through Ephraim Chambers’ Cyclopaedia, I will then examine Paul Otlet’s vision of the “radiated library” and his architectural inspirations.

The second section turns to the intersubjectivity of knowledge, or the relationship between personal memory and collective history. An individual’s personal archive has markedly different properties and requirements than a group’s or institution’s, which in turn is different from a massive, aggregated universal archive for the public. At the same time, some archives sit in between these scopes, and each has different purposes and practices surrounding it. Linking and categorization schemes rely on individuals making connections between information, but different individuals might not make the same connections; how does linking become a collective and collaborative endeavor, a universal language? This phenomenon is both explicated and emphasized by a contemporary example: the web’s algorithmic recommendation systems that conflate the individual and the collective as they traverse the links of the web. In this section I will examine Vannevar Bush’s memex machine, which wavered between personal study aid and collective knowledge generator.

The third and last section analyzes the encyclopedism of knowledge systems: the constant striving to expand beyond the archive’s horizon, and to achieve total comprehensiveness. Where intersubjectivity is concerned with the amount or makeup of a system’s users, this section concerns the amount and structure of the information in the archive. Many efforts to document, index, or link the world have truly attempted to map the world – every piece of information about everything – or have at least appealed to an impulse to do so. In this section I endeavor to explain this encyclopedic impulse, and suggest some of its promises and pitfalls.

\section{Spatialization}

\subsection{The Dimensions of Memory}

Memory is inherently spatial. Even when we don’t remember something, we often know where to find it. A recent study asked participants to save statements into various folders with generic names (such as FACTS, DATA, INFO, and POINTS).  Despite the unmemorable folder names, “participants recalled the places where the statements were kept better than they recalled the statements themselves.” The researchers found that “‘where’ was prioritized in memory,” providing preliminary evidence that people “are more likely to remember where to find it than to remember the details of the item.” They conclude by suggesting that we may be using Google and Wikipedia as memory extensions that then rewire our own internal memory.

But humans have relied on external memory since the origin of writing itself, and in the meantime we have developed scores of analog systems and techniques – Barnet might call them “memory machines,” John Willinsky “technologies of knowing” – to help summarize, filter, sort, and select.  Computer systems are only one piece of this longer history of tools and practices. David Weinberger’s “three orders of order” suggest this continuum, while also pointing out the rupture that the digital creates. The first order consists of things themselves, such as books in a library. The second order is a physical set of indexes, pointers, and references to the things, like a library card catalog. Finally, the third order is the digital reference, made of bits instead of atoms. 

A theme across all of these orders of order is a reliance on spatial memory (the “where to find it” in the Columbia study). Archival and classification schemes use terms like “border,” “domain,” and “kingdom” (is it a coincidence that these terms all carry connotations of politics and power struggle?). We visualize network schemes as trees and as rhizomes, represented on maps, graphs, and diagrams. It seems that proper spatial visualization of an archive might not only help us remember where something is saved, but also give a high-level understanding of the archive itself, improving browsing and serendipitous search.
The ancient practice of constructing “memory palaces” (and Giulio Camillo’s memory theater of the Renaissance) – outlined in Frances Yates’ The Art of Memory – strongly emphasizes memory’s reliance on spatial orientation and fixed dimension.  In order to construct a memory palace, the first step is to imagine a series of loci, or places, to determine the order of the facts. Only after creating space can one then create the images that represent the facts themselves. The structure that these palaces take on are up to the memorizer, but once fixed, they are rarely reordered—only added to. This completes a grander spatial metaphor that Peter Burke notices – that of the course, which a student must run, envisioning and memorizing images in places along the route towards knowledge.

This reliance on spatial memory keeps us in just two or three dimensions; it does not escape the trappings of the physical archive. If our memories rely on a fixed visual referent to know where a book is in a library, then we cannot rearrange the library’s stacks and expect to find it again. A similar concern arises with online reading and writing. Ted Nelson calls hypertext “multi-dimensional,” and Stuart Moulthrop says it aims to be “writing in a higher-dimensional space,”  but some readers still prefer paper-imitating PDFs to websites and e-books, because PDFs maintain a layer of real-world dimensional reference (as in, “I remember reading that sentence near the top of the page in the left column…”). For all of the liberating power of the digital, computers still rely on physical metaphors to be usable, and so we use digital equivalents of desktops, files, folders, and cards. The web even nods to this with its hierarchical URL structure that asks us to “navigate” down “paths” in given “domains.”

This last fact is surprising given that a common theme among hypertext’s pioneers, including Berners-Lee, is a desire to break down traditional linear and hierarchical classification schemes. A hierarchical scheme – like Linnaeus’s biological taxonomy or Dewey’s decimal classification – immediately suggests a tree view, and we can find many old examples of tree graphs in the Renaissance and Enlightenment.  On the other hand, an alphabetical scheme offers a linear view, one that “flattens” the brittle hierarchy of taxonomy, but dulls its rich network of links, trails, and associations. The linked hypertext view might be seen as a multi-dimensional graph, more nuanced and flexible but more difficult to grasp. If the first two orders are in one (linear) and two (hierarchical) dimensions, how can we bring the third order of order into a still higher dimension? And can it complement the ways that our minds visualize information?

\subsection{The Linked Encyclopedia}

Some older, pre-digital systems and practices have hybrid hierarchical/linear structures that start to suggest a network. While not the first system to incorporate links, Ephraim Chambers’ Cyclopaedia is one of the first reference works of its kind. The encyclopedia reads somewhat like a dictionary, but it expands into general knowledge and opinion as well, and it always suggests multiple views into its contents. Chambers wrote that his encyclopedia went beyond a dictionary because it was “capable of the advantages of a continued discourse.”  The word “encyclopedia” literally means “circle of learning,” calling into question the shape of such a knowledge structure. It may be organized linearly, but as a collection of words to describe words, it always strives to double back on itself and highlight its own circular logic.

The Cyclopaedia was organized alphabetically, a relatively bold form of classification in relationship to the traditional, hierarchical schemes. Most scholars seem to agree that alphabetical order was born out of sheer necessity, related to the “intellectual entropy” and “epistemological urgency” of the time.  New knowledge was simply being created too fast to systematize and order. But Michael Zimmer suggests that alphabetical order signaled the beginning of a shift to more distributed, networked, and “egalitarian” forms of knowledge organization.  For instance, religious topics would be placed alongside secular ones. Alphabetical organizational also turned the system into more of a “quick reference” guide that favored brief digests over long forays into knowledge; the practices of browsing, skimming and summarizing were continuously honed during the Renaissance and Enlightenment as scholars coped with “a confusing and harmful abundance of books” as early as 1545 (Chambers even called this complaint “as old as Solomon”). 

All the same, Chambers felt he needed an overarching scheme. In the encyclopedia’s preface, he included a diagram and listing of forty-seven categories (called Heads), complete with cross-references to the entries. In Chambers’ words, “the difficulty lay in the form and oeconomy of it; so to dispose such a multitude of materials, as not to make a confused heap of incoherent Parts, but one consistent Whole.”  In order to truly demonstrate a “continued discourse,” Chambers needed a graph, a map. Each of the Heads in the diagram contains a footnote that lists that heads’ terms (known as Common Places). 

Chambers’ use of Heads and Common Places followed Phillipp Melanchthon’s 1521 subject division into loci and capita (Peter Burke suggests that these would now be called “topics” and “headings,” less strong and physical metaphors).  Loci (“places”) bring to mind memory palaces, but also the “commonplace book”—to which Chambers was knowingly attaching himself. Many scholars used commonplace books as information management devices to store quotes, summaries, aphorisms, and so on, and these often had specialized systems for retrieval. Richard Yeo sees Chambers’ use of the term as directly appealing to the popularity of commonplace books at the time.  Ann Blair also argues that note-taking and commonplacing were far more common than the memory palaces and theaters outlined by Frances Yates, and that the two traditions made “no explicit reference to one another.”  Still they share a strong common thread: a reliance on loci as the root of knowledge retention, memory, and interconnection.

The Cyclopaedia was an ancestor to Diderot’s celebrated Encyclopédie (Diderot started by translating Chambers). Diderot’s work made further use of renvois (references) to question and subvert traditional knowledge structures and authorities—including the book’s own authority as a reference work. Michael Zimmer argues that Diderot also used renvois to hide politically controversial topics in seemingly dry and tangential entries, “guiding the reader to radical or subversive knowledge” while evading the eyes of the censors.  Zimmer directly ties the renvois to the hypertext link, suggesting that Bush, Nelson, and Berners-Lee all “intended to free users from the hegemony of fixed information organization in much the same way that renvois did for the readers of the Encyclopédie.” 

It is clear that Diderot fully recognized and built upon Chambers’ developments in linking references, but I call into question the notion that the prior “fixed” organization systems had no detractors or provisional solutions (moreover, the renvois are “fixed” themselves). Carolus Linnaeus, the author of perhaps the prototypical taxonomy, knew well that classifications are “cultural constructs reflecting human ignorance.”  Leibniz also understood its limitations; his Plan for Arranging a Library included a “miscellaneous” section, a tacit acknowledgement that the system is in some way imperfect or incomplete. Leibniz also praised his famous Note Closet, developed by Thomas Harrison, for this same ability: “A single truth can usually be put in different places, according to the various terms it contains … and different matters to which it is relevant.” 

Moreover, multiple hierarchies can coexist and offer competing schemes. Some of these schemes were already organized not as much around content as context. Peter Burke points out that Islamic classification systems were also tree-structured, but every element was organized based on its degree of separation from the Quran.  This is, crucially, an early citation-based network.

\subsection{Paul Otlet and the Radiated Library}

Along with Vannevar Bush, Paul Otlet bridges the second and third orders of order. Born in Belgium in 1868, Otlet predated Ted Nelson’s role as an obsessive encyclopedist and commonplacer. Between the ages of ages 11 and 27, he amassed 1400 pages of notes, and in his first move to Paris, he called it “the city where the world comes to take notes”  He liked to think big and in the aggregate, creating the Universal Decimal Classification and Universal Bibliographic Repertory. He also supported international politics associations like the League of Nations and the forerunner to UNESCO, going so far as to found the Union of International Assocations (which is, indeed, an international association of international associations) with his friend Henri La Fontaine in 1907.

Due in part to the destruction of much of his work in World War II, Otlet was mostly forgotten for decades in favor of his American successors. However, the rise of the web and the efforts of several scholars – particularly his biographer Boyd Rayward – have given him a new life as a prescient predictor of a networked hypertext system. As one of the originators of information science, his ideas and innovations can be broken into three themes. First, he envisioned (and even began to amass) a universal library to serve as the heart and central authority of the world’s information. Second, following his belief that books were redundant and arbitrary agglomerations that obscure the data held within (which is the object of a researcher’s true inquiry), he suggested a universal decimal classification system that built on Dewey’s system to incorporate an item’s metadata, its references and constituent parts. Its entries read less like library call numbers and more like modern databases’ structured queries. Finally, in his most striking prediction, he proposed a “radiated library” that could handle remote requests from a centralized location by screen and telephone. He envisioned the screen with multiple windows for simultaneous document consultation, audiovisual data, and finally a full automation of the document request process: “Cinema, phonographs, radio, television, these instruments taken as substitutes for the book, will in fact become the new book.”  Otlet’s concept of a “radiated library” and a “televised book” combine to suggest the networked multimedia of the web, more than 50 years before its creation.

Otlet was an encyclopedist, but also an innovator in graphical and spatial representation. He frequently used architecture as a foil, metaphor, and inspiration for bibliographic structures, calling his main work Traité de documentation a study of the “architecture of ideas.”  The first names for the Mundaneum – the universal repository Otlet and La Fontaine set out to build – were alternately “city of knowledge” and “World Palace.” In the end, the Mundaneum – like the archive itself – bridged the physical and the digital, as Otlet called it at once “an idea, an institution, a method, a material body of work, a building and a network.”  In his discussion of the architecting of knowledge, Otlet also crucially recognized that ideas are never so fixed as physical structures; as Charles van den Heuvel puts it, “For Otlet it was important to leave space for transformation and modification in response to the unforeseen and unpredictable.”  Leibniz had conceived of the “library without walls” long before, but Otlet’s radiated library went many steps further.

His resulting decimal classification and networked library is thus less bound by linear or hierarchical schemes. The architectural inspiration also may have helped him conceive of the radiated library, one that could transmit signals across space between screens, several decades before the first computers were linked together. All the same, it is hard to see Otlet’s universal library project as anything but quixotic. The perpetual collection and detailed organization of the entirety of human history in one location, all managed by 3x5 index cards, is doomed to fail. Still, Otlet’s system seems to have worked usefully for a time: the library had more than 17 million entries by 1934, handling 1500 research requests per year, all on the backbone of Otlet’s Universal Decimal Classification.  The universal repository was, of course, never completed, but it came closer to fruition than the memex or Xanadu.

\section{Intersubjectivity}

\subsection{From personal memory to collective history}

The scrapbooks, commonplace books, and card catalogs of old usually belonged to an individual. He or she might share them and collaborate with others, or collect resources for children and grandchildren, but these early systems generally reflected and mimicked the scattered mind of a single person. A scholar’s notes are likely to consist of many shorthands, mental leaps, and personal anecdotes that no one else would follow. Interestingly, most early hypertext systems focused on this individual scope, or at most on collaborative or collective research. Only Xanadu (and perhaps Otlet’s Mundaneum) had the world-encompassing scope of the web.

Jeremias Drexel stated in 1638 that there is no substitute for personal note-taking: “One’s own notes are the best notes. One page of excerpts written by your own labor will be of greater use to you than ten, even twenty or one hundred pages made by the diligence of another.”  People forge connections and organizational schemes in unique and sometimes conflicting ways. As more and more people enter a system, it will encounter more and more possible definitions and connections.

The idiosyncratic connections formed by an individual’s memory make it difficult to generalize categories. An individual’s thought process might be reminiscent of Borges’ Chinese encyclopedia, which offers a taxonomy of animals divided by absurd traits, such as “Those that belong to the emperor, embalmed ones, those that are trained, suckling pigs, mermaids, fabulous ones, stray dogs,” and “those that are included in this classification.”  These may be the trails that a mind follows, but the humor lies in calling it a taxonomy, in making the categories intersubjective and even official, objective. Borges’ categories remind us that classifications will always be compromises, between individuals and groups, or between groups and a collective whole.

Markus Krajewski’s Paper Machines: About Cards and Catalogs hinges on the difference and tension between a personal note-taking system and a universal library. We often use the same systems for organizing each (such as the card catalog or the SQL database), but they don’t turn out to be for the same uses. Krajewski says “The difference between the collective search engine and the learned box of paper slips lies in its contingency.”  Whenever we add a tag or make a connection in an archive, we are attempting to predict what will be searched for later; this is why Derrida calls the archive “a pledge, a token of the future.”  But it is easier to classify in a personal archive; we can predict our future selves better than we can predict the future.

As a result, personal note-taking tools might seem like an easier place to start with the challenge of hypertext. They are certainly technically easier, avoiding collaboration issues like version control. But an archive is almost never entirely personal. Thought may be idiosyncratic, but it follows common patterns. Users want the possibility of sharing documents, or of passing on entire collections to others. Ann Blair points out that successors would fight over notes in wills, which suggests that any time a commonplace book is begun, it has some kind of common value.  In the case of historical figures, personal notes often become a literal part of an archive, then meant for public consultation. But we treat these archives differently than those that are constructed for us. For instance, Walter Benjamin’s Arcades Project is a set of notecards, published as a sort of commonplace book that has become a prominent work to consult in its own right. Is it a book, an archive, or a database? Who is it for? What happens to individual memory as it becomes shared history?

This relationship between the personal and the collective is taking on new meaning on the web, where we expect personalized information, but rely on a massive collective of people in order to get it. Nick Seaver argues that recommendation systems “algorithmically rearticulate the relationship between individual and aggregate traits.”  The communities and demographics that form around individuals can in turn be aggregated and intersected into a single, massive whole. At each stage, memory is abstracted further and further from us.

Today’s efforts to organize the web and its sub-archives (i.e. the web applications, tools, and platforms we use every day) tend to reflect this and aim to marry the best of both worlds: the individual and the mass. Clay Shirky and David Weinberger champion the folksonomy as a solution; let individuals tag however they want, and at the right scale everything will sort itself out.  The Semantic Web is similarly structured, by letting users define their own vocabularies for both pages and links, but strictly enforcing them once made.  These approaches are certainly worth pursuing, but both still rely on fixed language rather than associative connection; tagging an item is undoubtedly an act meant to make connections between documents, but it is always mediated by language and structured according to certain systematic and linguistic conventions.

\subsection{Vannevar Bush's Memex}

Unlike Otlet’s radiated library, or Nelson’s Xanadu, Vannevar Bush’s memex was decidedly a machine designed for personal use. It did not build in weblike networked affordances. All the same, Bush suggests many intersubjective uses for the memex, adding to the confusion between personal archive and collective library.

Bush was perhaps best known as the director of U.S. military research and development during World War II, but he also made a lasting contribution to hypertext; a 1945 essay in the Atlantic called “As We May Think” conceived of the memex machine, an automated microfilm device that could store an entire library in one drawer and retrieve any item within seconds.  Perhaps most crucially, Bush conceived of new ways to connect items: through associative trails. Linda C. Smith analyzed the citation network of many hypertext articles and discovered, in Belinda Barnet’s words, that “there is a conviction, without dissent, that modern hypertext is traceable to this article.” 

Bush begins by arguing that, “The summation of human experience is being expanded at a prodigious rate,” but suggests that our methods for retrieving such experience are hindered by “the artificiality of systems of indexing.”  He points out the limitations of keeping data only in one place, and of using strict formal rules to access it: “the human mind does not work that way. It operates by association. With one item in its grasp, it snaps instantly to the next that is suggested by the association of thoughts, in accordance with some intricate web of trails carried by the cells of the brain.” His proposed solution, the memex, aims to mechanize “selection by association, rather than by indexing.” 

The memex is built for personal use; Bush’s model is “the human mind,” after all, and not “human minds” (as Barnet notes, he follows the cybernetic tradition of the time in modeling computation on human thought, along with Wiener, Shannon, Licklider, and others).  The idiosyncrasies of individual trails, and the challenges in developing a new language for a new invention, would suggest that the machine was strictly for individual use. However, Bush points immediately to its possibility for generalization as well; he envisions an example of a person sending his trail of research to a colleague “for insertion in his own memex, there to be linked into the more general trail.” 

Bush goes on to suggest that the memex will hold new forms of encyclopedias and ready-made trails, along with “a new profession of trail blazers, those who find delight in the task of establishing useful trails through the enormous mass of the common record.”  But he does not dwell on this to consider where this “common record” will live, who will own and control it, and how individuals will tie these resources to their own idiosyncratic trails. The shift from subjectivity to intersubjectivity, and then in turn from intersubjectivity to some form of objectivity, makes each act of classification – or in Bush’s case, each act of association – increasingly fraught and violent.

Bush’s work relies on the trail, a closely curated path where one document directly associates with another. Ted Nelson instead suggested “zippered lists,” which would operate like trails but without Bush’s emphasis on sequence.  In each of these cases they rely on a human curator to create the links. Bush envisions trails shared for personal, collaborative, and general use, but the connection itself remains person-to-person, intersubjective on the smallest scale. The trails and associations formed by the memex always remain deeply human, and deeply individual.

In Bush’s “Memex Revisited,” he begins to tease out the possibility of the memex forming trails for a scholar, suggesting that it could “learn from its own experience and to refine its own trails.”  Here the influence of Wiener’s cybernetics and feedback theory are clear, and it begins to point to the machine learning and automated classification that occurs today. Most intriguing is Bush’s suggestion that like the human mind, some well-worn trails would be kept in memory, reinforced and expanded, while other less-used trails would fall away. This conjures up the notion of a fluid archive, one that is constantly forming and re-forming its associations, dynamically linking the past.

But Bush’s memex is not without its limitations. John H. Weakland offered two criticisms of the memex in response to the Atlantic article. He asks “how personal associations of the general record could be generally useful,” as well as how a researcher can find things they don’t know about already.  It appears to me that the second challenge is an extension of the first: associative indexing may be more inherently fuzzy and idiosyncratic than content-based indexing systems like text search and tagging. It sacrifices fixity and consistency at the expense of individuality and nuance.

Another limitation of the memex, offered by Belinda Barnet, is that “Bush’s model of mental association was itself technological; the mind ‘snapped’ between allied items, an unconscious movement directed by the trails themselves.”  Bush himself recognized this, pointing out that the human memory system is a “three-dimensional array of cells” that can gather, re-form, and select relationships as a whole or a subset of a whole.  While later hypertext systems and the Semantic Web come closer to such a three-dimensional structure, like the memex they are often constrained to ‘snapping’ between associations.

Finally, even though Bush seems fully aware of the morphing state of collective knowledge and history, he assumed that the trails would not grow old. He envisions a father bequeathing a memex to his son, along with the myriad trails formed, as a fixed and locked document. Even Bush’s proposed adaptive memex would be modeled against the individual researcher; in machine learning terms, its “training set” would not be formed in the aggregate like modern-day recommendation systems, but rather from the unique trails formed by an individual.

\section{Encyclopedism}

\subsection{The Endless Archive}

While the last section was based on the type and scale of users of the archive, this section concerns the type and scale of information or content within the archive. There does tend to be a relationship – an archive built for everyone is more likely to collect everything – but I divide them here to highlight the tendency for content to stretch towards complete and total comprehensiveness, or what I am calling encyclopedism.

When building an archive, where do you stop? Paul Otlet wanted to index all of every book. In his notes, he insists, “I write down everything that goes through my mind, but none of it has a sequel. At the moment there is only one thing I must do! That is, to gather together my material of all kinds, and connect it with everything else I have done up till now.”  This persistent, obsessive quest for comprehensiveness is part and parcel of the archive – you either want to collect and connect everything, or everything worthwhile, within a given scope.

Once again this conjures up a Borges story: the Library of Babel contains books with every permutation and combination of every letter. Somewhere in the library sits every great work ever written, and every great work that will be written. But the vast majority of these books are useless nonsense, and no great works will be found. Borges, a librarian himself, understood well the encyclopedic impulse and the noise and madness that results. 

Encyclopedism has its roots at least in the Renaissance, as Ann Blair notes: “it is reasonable to speak of encyclopedic ambition as a central ingredient of the Renaissance obsession with accumulating information.”  Even in 1548, Conrad Gesner began compiling a “general bibliography” with the aim of indexing all known books; he ended with 10,000 works by 3,000 authors, which was surely an obsolete number even by the time he finished.  Some critics, like Jesuit scholars Francesco Sacchini and Antonio Possevino, recommended an “aggressively purged” rather than universal library, throwing out any redundant or misleading texts. Gesner disagreed, but his reasoning was telling: “No author was spurned by me, not so much because I considered them all worthy of being cataloged or remembered, but rather to satisfy the plan which I had set for myself.”  He wanted to list all the books in order to leave others to be the judge, but first and foremost, he did it because it was his plan all along.

Some of today’s technological language reflects this drive. Wikipedia’s mission is “to give freely the sum of the world’s knowledge to every single person on the planet.”  which is reminiscent of Google’s: “to organize the world's information and make it universally accessible and useful.”  The world’s knowledge, universally accessible, to every person. The goal is impossible; capturing “the sum of the world’s knowledge” is akin to Borges’s aleph – a point that contains all points – or to his one-to-one map of the world. 

All of these universal projects are destined to fail at their end goal, but the resulting collections can be useful. The book repositories and knowledge systems of today – Wikipedia, Google Books, Project Gutenberg, Amazon – may have come closer than any previous efforts to capturing the world’s knowledge, but they does so according to certain principles, conventions, demands and traditions. They also have something else in common: they must always adhere to the technical and conventional standards and limitations of the web itself. 

\subsection{Ted Nelson and Xanadu}

Ted Nelson, inventor of the term “hypertext,” is a notorious collector, commonplacer, and self-documenter. He also always thinks big; he wants to collect everything and connect everything to everything (“everything is intertwingled,” in his parlance), and only then will it all make sense. His project for doing so, called Xanadu, began work in 1960 and has inspired scores of hypertext acolytes, but after so many years of continuous development, it still has not been fully realized.

Nelson was deeply inspired by Bush’s memex, referencing him frequently in presentations and even including the entirety of “As We May Think” in his book Literary Machines. Building on Bush’s ideas, Nelson suggested “zippered lists” instead of trails, which could be linked or unliked as its creator desired, advancing beyond Bush’s “prearranged sequences.”  But his biggest development was to reintroduce the global ambition of Otlet into Bush’s associative vision: the idea of a universal, networked, collectively managed hypertext system.

The result would be, as Barnet says, “like the web, but much better.”  In Nelson’s system, there would be no 404s, no missing links, no changes to pages forever lost to history. Links would be two-way, forged in both directions – imagine visiting a page and being able to immediately consult every page that linked to the page. And rather than copying, Xanadu operates on transclusion, a sort of soft link or window between documents that would allow new items to be quickly and easily constructed from constituent parts, readily pointing back to their source.

Nelson’s idea for Xanadu might resemble Wikipedia; one of Wikipedia’s core tenets is “No Original Research: don’t create anything from scratch, just compile,” reflecting the principle of Nelson’s transclusions.  But on the web, where so much information is ripe for mash-up, remix, and reuse, the only option is to create from scratch. The links at the footer or the inside of a Wikipedia page are merely pointers and not true windows into the source documents. Nelson’s transclusions are more akin to the Windows shortcut, Mac alias, or Linux softlink. The Web’s default, on the other hand, is to copy rather than link. Jaron Lanier suggests that copying-not-linking is a vestige of the personal computer’s origins at Xerox PARC, whose employer was quite literally in the business of copying, and was inherently wary of ideas that bypassed it. 

One could look at the resulting Wikipedia, or any such aggregation of compiled knowledge, as a combination of two actions: summarizing and filtering. To summarize is to provide a shorter version of a longer text. To filter is to offer a verbatim excerpt of the text. Most knowledge systems that I am addressing here exist along a continuum between these two primary actions, and effective ones are able to elegantly balance both. Xanadu places more focus on filtering texts, while the web might lend itself better to summarizing; it is only through the web’s hyperlinks that we get a glimpse of a filtering axis. In the end, we cannot easily filter or measure content on the web, and we need to rely on search and indexing services like Google to do it for us. One blog post by the Tow Center’s NewsLynx project laments, “the inefficiency of one-way links left a hole at the center of the web for a powerful player to step in and play librarian.” 

But unlike the web, Xanadu has still not been fully realized. It has lost, while the web has experienced an unprecedented, meteoric rise. Xanadu also has its share of detractors and challengers. Most of its biographies and summaries are fairly critical, most famously a 1995 Wired article that prompted a forceful response from Nelson.  There is a level of hubris in the encyclopedic impulse that Nelson doesn’t hide. His proposed system is top-down and brittle in certain ways, including rigid security and identification systems. And his proposal for online “micropayments” per transclusion is interesting but controversial; Jaron Lanier and others have supported it, but many are skeptical, suggesting that it would stifle the sharing of knowledge and circulation of material. 

The Xanadu system is far from perfect, but its allure comes from the idea that it treats its contents with history and context in mind. The web is an ephemeral stream, and you won’t step into the same one twice; Barnet equates web surfing with “channel surfing.”  Xanadu promised to treat its contents like an archive rather than making us build archives around it. Comparing it to the web raises interesting questions: how much structure, organization, and control should we place on our networked information systems? How much is desirable, and how much is technically and economically feasible? And if we consider the archival capabilities of each, how are they building, sorting, and selecting our information?

A skeletal version of Xanadu (still without its two-way links) was finally released on the web, after more than 50 years of development, in summer 2014.  It has joined the myriad archives and knowledge systems embedded inside the web. Many of the later, “second-generation” hypertext systems were geared towards personal and institutional uses (systems like NoteCards, Guide, WE, or Apple’s HyperCard). These likewise resemble the web platforms and tools we use today (such as Trello, Evernote, or Zotero).  But these systems, like Xanadu itself, have been subsumed by the web. Hypertext systems can all interact with one another, but the encyclopedic, universal ones can only be in competition.

\section{Conclusion}

This long history of linked, indexed, and sorted archives would suggest that the current state of archives in the digital era has occurred as a result of a continuum of developments, rather than a radical leap into completely unknown territory. But in another sense, the digital does allow for a complete rupture. The “information overload” we experience today is a product of two factors, one old and one new. The accumulation of the archive is an age-old challenge that many tools, systems and practices have endeavored to solve. But the networking of the archive is a newer challenge. There has always been too much information, but now it can all be connected, quantified, broken down and aggregated as never before. As we sort through the new intertwingled mass of content and context, it will be crucial to keep in mind its long history; after all, it is what archives are fighting to preserve.

Archives’ constant battle with issues of scope and dimensionality suggest a need to recognize and limit ambitions, to start small and build up rather than starting from the whole and breaking down. The networking of the archive requires knowing your archive – who is it for? How big is it, and how big do you want it to be? What visual and dimensional language can you employ to help the user navigate?

Looking to history can also temper the conclusions we attempt to draw from archives. The web’s massive structure suggests total comprehensiveness – a true universal library – and understanding the limits of its scope as well as the limits of its context allows us to view its contents with greater nuance. This is a crucial question as our networked archives begin to network with one another, such as with linked data and APIs. These create new modes of analysis that suggest an inarguable universality: as danah boyd and Kate Crawford argue, “Big Data reframes key questions about the constitution of knowledge, the processes of research, how we should engage with information, and the nature and the categorization of reality.”  A full understanding of the structures and challenges in network- and archive-building gives us one view into what boyd and Crawford call the “models of intelligibility” and “inbuilt limitations” of big data itself.

The web has evolved since its inception to support much more complex applications, structures, and graphics. But any new developments and platforms must be grafted onto the web rather than rethinking its core structure. I have aimed to suggest how historical context and understanding of the challenges and structures of early hypertext and information management systems can help to explain the powers and limitations of the web. These knowledge systems can also provide inspiration for new solutions: web-based digital archives could aim to mimic or approximate multiple linking, transclusions, or high-level graph views, all while keeping in mind the archive’s size, shape, and scope.
